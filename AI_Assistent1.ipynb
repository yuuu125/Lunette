{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuuu125/Lunette/blob/main/AI_Assistent1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qq1cOplyCeys",
        "outputId": "146d114c-1a14-4d41-c61d-108423cfd664",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28.1\n",
            "  Downloading openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: notion-client in /usr/local/lib/python3.11/dist-packages (2.4.0)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28.1) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28.1) (3.11.15)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from notion-client) (0.27.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->notion-client) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->notion-client) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->notion-client) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->notion-client) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->notion-client) (1.3.1)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->notion-client) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28.1) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28.1) (2.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (1.20.1)\n",
            "Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.96.1\n",
            "    Uninstalling openai-1.96.1:\n",
            "      Successfully uninstalled openai-1.96.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-openai 0.0.8 requires openai<2.0.0,>=1.10.0, but you have openai 0.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "openai"
                ]
              },
              "id": "454d93aaf3a14bb39d0018bede5ad7df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-484w9eyo\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-484w9eyo\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.28.1 python-docx notion-client langdetect pydub\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg  # ç¡®ä¿å®‰è£…å¿…è¦çš„ä¾èµ–\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import openai\n",
        "import whisper\n",
        "from docx import Document\n",
        "from google.colab import files, userdata\n",
        "from notion_client import Client\n",
        "from langdetect import detect, LangDetectException\n",
        "import datetime\n",
        "import tempfile\n",
        "import torch\n",
        "from pydub import AudioSegment\n",
        "import subprocess\n",
        "\n",
        "try:\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    NOTION_TOKEN = userdata.get('NOTION_TOKEN')\n",
        "    NOTION_DB_ID = userdata.get('NOTION_DB_ID')\n",
        "\n",
        "    if not OPENAI_API_KEY:\n",
        "        raise ValueError(\"OPENAI_API_KEY not set\")\n",
        "    if not NOTION_TOKEN:\n",
        "        print(\"âš ï¸ Notion token missing - feature disabled\")\n",
        "    if not NOTION_DB_ID:\n",
        "        print(\"âš ï¸ Notion DB ID missing - feature disabled\")\n",
        "\n",
        "    openai.api_key = OPENAI_API_KEY\n",
        "    print(\"âœ… OpenAI API key set\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Key retrieval failed: {str(e)}\")\n",
        "\n",
        "def get_audio_duration(audio_path):\n",
        "    \"\"\"ä½¿ç”¨ffmpegè·å–ç²¾ç¡®éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"format=duration\",\n",
        "             \"-of\", \"default=noprint_wrappers=1:nokey=1\", audio_path],\n",
        "            capture_output=True, text=True\n",
        "        )\n",
        "        return float(result.stdout)\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ æ— æ³•è·å–ç²¾ç¡®æ—¶é•¿ï¼Œä½¿ç”¨ä¼°ç®—å€¼: {e}\")\n",
        "        # ä¼°ç®—ï¼š8KB/s æ˜¯å¸¸è§éŸ³é¢‘æ¯”ç‰¹ç‡\n",
        "        return max(30, os.path.getsize(audio_path) // 8000)\n",
        "\n",
        "def transcribe_audio(audio_path, model_size=\"base\"):\n",
        "    \"\"\"ä½¿ç”¨Whisperè½¬å½•éŸ³é¢‘æ–‡ä»¶\"\"\"\n",
        "    print(f\"ğŸ”Š Starting transcription with Whisper ({model_size} model)...\")\n",
        "\n",
        "    try:\n",
        "        # æ£€æŸ¥GPUåŠ é€Ÿ\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"ğŸ’» Using device: {device.upper()}\")\n",
        "\n",
        "        # åŠ è½½æ¨¡å‹\n",
        "        model = whisper.load_model(model_size, device=device)\n",
        "        print(f\"âœ… Loaded Whisper {model_size} model\")\n",
        "\n",
        "        # è½¬å½•éŸ³é¢‘\n",
        "        result = model.transcribe(\n",
        "            audio_path,\n",
        "            fp16=(device == \"cuda\"),\n",
        "            verbose=True,\n",
        "            task=\"transcribe\"\n",
        "        )\n",
        "\n",
        "        transcription = result[\"text\"]\n",
        "        print(f\"âœ… Transcription complete! Characters: {len(transcription)}\")\n",
        "        return transcription\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Transcription failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def test_notion_connection():\n",
        "    \"\"\"æµ‹è¯•Notionè¿æ¥æ˜¯å¦æœ‰æ•ˆ\"\"\"\n",
        "    try:\n",
        "        # åˆå§‹åŒ–Notionå®¢æˆ·ç«¯ï¼Œæ˜¾å¼é…ç½®å®¢æˆ·ç«¯ç¦ç”¨ä»£ç†\n",
        "        notion = Client(\n",
        "        auth=NOTION_TOKEN,\n",
        "        client=httpx.Client(proxies=None)  # æ˜¾å¼ç¦ç”¨ä»£ç†\n",
        "        )\n",
        "        notion.databases.retrieve(database_id=NOTION_DB_ID)\n",
        "        print(\"âœ… Notion connection verified\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Notion connection failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def clean_transcript(text):\n",
        "    \"\"\"Cleans raw transcript text\"\"\"\n",
        "    text = re.sub(r'\\d{1,2}:\\d{2}:\\d{2}', '', text)\n",
        "    text = re.sub(r'Speaker\\s*\\d+:?', '', text)\n",
        "    return re.sub(r'\\n\\s*\\n', '\\n\\n', text).strip()\n",
        "\n",
        "def segment_text(text):\n",
        "    \"\"\"Segments text into paragraphs\"\"\"\n",
        "    return [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "\n",
        "def handle_transcript_input():\n",
        "    \"\"\"Handles transcript input methods\"\"\"\n",
        "    print(\"\\n=== Handling Transcript Input ===\")\n",
        "    print(\"Choose input method:\")\n",
        "    print(\"1 - Upload text file (.txt or .docx)\")\n",
        "    print(\"2 - Paste text directly\")\n",
        "    print(\"3 - Upload audio file (transcribe with Whisper)\")\n",
        "\n",
        "    input_method = input(\"Your choice (1/2/3): \")\n",
        "    transcript_text = \"\"\n",
        "\n",
        "    # æ–‡æœ¬æ–‡ä»¶ä¸Šä¼ \n",
        "    if input_method == \"1\":\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            print(\"âš ï¸ No files uploaded, switching to paste\")\n",
        "            transcript_text = input(\"Paste meeting transcript: \")\n",
        "        else:\n",
        "            filename = list(uploaded.keys())[0]\n",
        "            print(f\"âœ… Uploaded: {filename}\")\n",
        "\n",
        "            # æ–‡æœ¬æ–‡ä»¶å¤„ç†\n",
        "            if filename.endswith('.txt'):\n",
        "                transcript_text = uploaded[filename].decode('utf-8')\n",
        "\n",
        "            # DOCXå¤„ç†\n",
        "            elif filename.endswith('.docx'):\n",
        "                with tempfile.NamedTemporaryFile(delete=False, suffix='.docx') as tmp:\n",
        "                    tmp.write(uploaded[filename])\n",
        "                    tmp_path = tmp.name\n",
        "\n",
        "                doc = Document(tmp_path)\n",
        "                transcript_text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "                os.unlink(tmp_path)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "    # æ–‡æœ¬ç²˜è´´\n",
        "    elif input_method == \"2\":\n",
        "        transcript_text = input(\"Paste meeting transcript: \")\n",
        "\n",
        "    # éŸ³é¢‘æ–‡ä»¶å¤„ç†\n",
        "    elif input_method == \"3\":\n",
        "        uploaded_audio = files.upload()\n",
        "        if not uploaded_audio:\n",
        "            print(\"âš ï¸ No audio files uploaded, switching to text paste\")\n",
        "            transcript_text = input(\"Paste meeting transcript: \")\n",
        "        else:\n",
        "            filename = list(uploaded_audio.keys())[0]\n",
        "            print(f\"âœ… Uploaded audio: {filename}\")\n",
        "\n",
        "            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶\n",
        "            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(filename)[1]) as tmp:\n",
        "                tmp.write(uploaded_audio[filename])\n",
        "                audio_path = tmp.name\n",
        "\n",
        "            print(\"\\nâš¡ Select transcription speed:\")\n",
        "            print(\"1 - Fast (tiny model, fastest, lower accuracy)\")\n",
        "            print(\"2 - Balanced (base model, recommended)\")\n",
        "            print(\"3 - High Quality (small model, slower)\")\n",
        "\n",
        "            speed_choice = input(\"Your choice (1/2/3): \") or \"2\"\n",
        "            model_map = {\"1\": \"tiny\", \"2\": \"base\", \"3\": \"small\"}\n",
        "            model_size = model_map.get(speed_choice, \"base\")\n",
        "\n",
        "            # è·å–éŸ³é¢‘æ—¶é•¿\n",
        "            try:\n",
        "                duration = get_audio_duration(audio_path)\n",
        "                print(f\"â± Audio duration: {duration//60:.0f}m {duration%60:.0f}s\")\n",
        "\n",
        "                # æ—¶é—´ä¼°ç®—\n",
        "                time_estimates = {\"tiny\": 0.3, \"base\": 0.8, \"small\": 2.0}\n",
        "                est_sec = duration * time_estimates[model_size]\n",
        "                print(f\"â³ Estimated processing time: ~{est_sec//60:.0f}m {est_sec%60:.0f}s\")\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Duration estimation failed: {e}\")\n",
        "\n",
        "            # è½¬å½•éŸ³é¢‘\n",
        "            transcript_text = transcribe_audio(audio_path, model_size)\n",
        "\n",
        "            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
        "            os.unlink(audio_path)\n",
        "\n",
        "    else:\n",
        "        print(\"âš ï¸ Invalid option, defaulting to text paste\")\n",
        "        transcript_text = input(\"Paste meeting transcript: \")\n",
        "\n",
        "    cleaned_text = clean_transcript(transcript_text)\n",
        "    segments = segment_text(cleaned_text)\n",
        "\n",
        "    print(f\"ğŸ“ Processed text: {len(segments)} segments, {len(cleaned_text)} characters\")\n",
        "    return cleaned_text, segments\n",
        "\n",
        "def analyze_with_gpt(text, language='en'):\n",
        "    \"\"\"Analyzes text with GPT API\"\"\"\n",
        "    print(\"\\n=== Analyzing with GPT ===\")\n",
        "\n",
        "    if not openai.api_key:\n",
        "        print(\"âŒ OpenAI API key missing\")\n",
        "        return {\"error\": \"OpenAI API key not set\", \"fallback_used\": True}, 0\n",
        "\n",
        "    # Language mapping\n",
        "    lang_map = {'zh': 'Chinese', 'es': 'Spanish', 'fr': 'French', 'en': 'English'}\n",
        "    lang_name = lang_map.get(language[:2], 'English')\n",
        "\n",
        "    # System prompt setup\n",
        "    system_prompt = f\"\"\"\n",
        "    You are a professional meeting analyst. Extract key information:\n",
        "    - Respond in {lang_name}\n",
        "    - Use this JSON format:\n",
        "    {{\n",
        "        \"meeting_title\": \"Meeting Title\",\n",
        "        \"participants\": [\"Attendee1\", \"Attendee2\"],\n",
        "        \"summary\": \"Meeting summary\",\n",
        "        \"action_items\": [{{\"task\": \"Task\", \"assignee\": \"Owner\"}}],\n",
        "        \"key_points\": {{\n",
        "            \"concerns\": [],\n",
        "            \"decisions\": [],\n",
        "            \"deadlines\": [],\n",
        "            \"updates\": []\n",
        "        }},\n",
        "        \"meeting_type\": \"Meeting type\",\n",
        "        \"platform\": \"Platform\",\n",
        "        \"fallback_used\": false\n",
        "    }}\n",
        "\n",
        "    Extraction rules:\n",
        "    1. meeting_title: Extract from start/end or generate\n",
        "    2. participants: Extract all attendees\n",
        "    3. Focus on meeting start/end sections\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt = f\"Meeting transcript:\\n{text[:10000]}\"\n",
        "\n",
        "    try:\n",
        "        # GPT API call\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message['content']\n",
        "        result = json.loads(content)\n",
        "        tokens_used = response.usage['total_tokens']\n",
        "\n",
        "        print(f\"âœ… GPT analysis complete! Tokens: {tokens_used}\")\n",
        "        print(f\"Meeting title: {result.get('meeting_title', 'N/A')}\")\n",
        "        print(f\"Participants: {len(result.get('participants', []))}\")\n",
        "        print(f\"Meeting type: {result.get('meeting_type', 'N/A')}\")\n",
        "        print(f\"Action items: {len(result.get('action_items', []))}\")\n",
        "\n",
        "        # Fallback for action items\n",
        "        if not result.get('action_items'):\n",
        "            result['fallback_used'] = True\n",
        "            print(\"âš ï¸ No action items detected\")\n",
        "\n",
        "        return result, tokens_used\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ GPT analysis failed: {str(e)}\")\n",
        "        return {\n",
        "            \"error\": str(e),\n",
        "            \"fallback_used\": True\n",
        "        }, 0\n",
        "\n",
        "def create_notion_entry(meeting_data):\n",
        "    \"\"\"Creates Notion database entry\"\"\"\n",
        "    if not NOTION_TOKEN or not NOTION_DB_ID:\n",
        "        print(\"âš ï¸ Notion config incomplete - skipping\")\n",
        "        return False\n",
        "\n",
        "    print(\"\\n=== Syncing to Notion ===\")\n",
        "\n",
        "    try:\n",
        "        # åˆå§‹åŒ–Notionå®¢æˆ·ç«¯ï¼Œæ˜¾å¼é…ç½®å®¢æˆ·ç«¯ç¦ç”¨ä»£ç†\n",
        "        notion = Client(\n",
        "        auth=NOTION_TOKEN,\n",
        "        client=httpx.Client(proxies=None)  # æ˜¾å¼ç¦ç”¨ä»£ç†\n",
        ")\n",
        "\n",
        "        # Prepare properties\n",
        "        properties = {\n",
        "            \"Meeting Title\": {\"title\": [{\"text\": {\"content\": meeting_data.get(\"meeting_title\", \"Untitled\")}}]},\n",
        "            \"Participant\": {\"rich_text\": [{\"text\": {\"content\": \", \".join(meeting_data.get(\"participants\", [\"Unknown\"]))}}]},\n",
        "            \"Date & Duration\": {\"date\": {\"start\": meeting_data.get(\"date\", datetime.datetime.now().isoformat())}},\n",
        "            \"Meeting Type\": {\"rich_text\": [{\"text\": {\"content\": meeting_data.get(\"meeting_type\", \"Other\")}}]},\n",
        "            \"Platform\": {\"select\": {\"name\": meeting_data.get(\"platform\", \"Unknown\")}},\n",
        "            \"Summary\": {\"rich_text\": [{\"text\": {\"content\": meeting_data.get(\"summary\", \"\")}}]},\n",
        "            \"Key Points\": {\"rich_text\": [{\"text\": {\"content\": format_key_points(meeting_data)}}]},\n",
        "            \"Action Items\": {\"rich_text\": [{\"text\": {\"content\": format_action_items(meeting_data)}}]},\n",
        "        }\n",
        "\n",
        "        # Create entry\n",
        "        new_page = notion.pages.create(\n",
        "            parent={\"database_id\": NOTION_DB_ID},\n",
        "            properties=properties\n",
        "        )\n",
        "\n",
        "        print(f\"âœ… Notion entry created! ID: {new_page['id']}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Notion sync failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def format_key_points(data):\n",
        "    \"\"\"Formats key points for Notion\"\"\"\n",
        "    points = []\n",
        "    key_points = data.get(\"key_points\", {})\n",
        "    for category, items in key_points.items():\n",
        "        if items and isinstance(items, list):\n",
        "            points.append(f\"{category.upper()}:\")\n",
        "            points.extend([f\"- {item}\" for item in items])\n",
        "    return \"\\n\".join(points)\n",
        "\n",
        "def format_action_items(data):\n",
        "    \"\"\"Formats action items for Notion\"\"\"\n",
        "    action_items = data.get(\"action_items\", [])\n",
        "    if not action_items or not isinstance(action_items, list):\n",
        "        return \"No action items\"\n",
        "\n",
        "    formatted = []\n",
        "    for item in action_items:\n",
        "        if isinstance(item, dict):\n",
        "            task = item.get('task', 'Unknown task')\n",
        "            assignee = item.get('assignee', 'Unassigned')\n",
        "            formatted.append(f\"- {task} (Owner: {assignee})\")\n",
        "        else:\n",
        "            formatted.append(f\"- {str(item)}\")\n",
        "    return \"\\n\".join(formatted)\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main workflow execution\"\"\"\n",
        "    if not openai.api_key:\n",
        "        print(\"âŒ OpenAI API key missing\")\n",
        "        return\n",
        "\n",
        "    logs = {\"steps\": [], \"errors\": []}\n",
        "\n",
        "    # Test Notion connection\n",
        "    if NOTION_TOKEN and NOTION_DB_ID:\n",
        "        if not test_notion_connection():\n",
        "            print(\"âš ï¸ Notion connection failed\")\n",
        "\n",
        "    try:\n",
        "        # Process input\n",
        "        cleaned_text, segments = handle_transcript_input()\n",
        "        logs[\"steps\"].append({\n",
        "            \"step\": \"Text input\",\n",
        "            \"segment_count\": len(segments),\n",
        "            \"status\": \"success\"\n",
        "        })\n",
        "\n",
        "        # Detect language\n",
        "        try:\n",
        "            language = detect(cleaned_text[:500]) if cleaned_text else 'en'\n",
        "        except LangDetectException:\n",
        "            language = 'en'\n",
        "        print(f\"ğŸŒ Detected language: {language}\")\n",
        "\n",
        "        # GPT analysis\n",
        "        gpt_results, tokens_used = analyze_with_gpt(cleaned_text, language)\n",
        "\n",
        "        if \"error\" in gpt_results:\n",
        "            logs[\"steps\"].append({\n",
        "                \"step\": \"GPT analysis\",\n",
        "                \"status\": \"failed\",\n",
        "                \"error\": gpt_results[\"error\"]\n",
        "            })\n",
        "            print(f\"âŒ GPT failed: {gpt_results['error']}\")\n",
        "            return\n",
        "        else:\n",
        "            logs[\"steps\"].append({\n",
        "                \"step\": \"GPT analysis\",\n",
        "                \"tokens_used\": tokens_used,\n",
        "                \"meeting_title\": gpt_results.get(\"meeting_title\"),\n",
        "                \"participants_count\": len(gpt_results.get(\"participants\", [])),\n",
        "                \"meeting_type\": gpt_results.get(\"meeting_type\"),\n",
        "                \"action_items_count\": len(gpt_results.get(\"action_items\", [])),\n",
        "                \"status\": \"success\"\n",
        "            })\n",
        "\n",
        "        # Add date and sync to Notion\n",
        "        gpt_results[\"date\"] = datetime.datetime.now().isoformat()\n",
        "        notion_success = create_notion_entry(gpt_results)\n",
        "        logs[\"steps\"].append({\n",
        "            \"step\": \"Notion sync\",\n",
        "            \"status\": \"success\" if notion_success else \"failed\"\n",
        "        })\n",
        "\n",
        "        # Save logs\n",
        "        with open(\"meeting_logs.json\", \"w\") as f:\n",
        "            json.dump(logs, f, indent=2)\n",
        "\n",
        "        print(\"\\nâœ… Process complete! Logs saved\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logs[\"errors\"].append(str(e))\n",
        "        print(f\"\\nâŒ Process error: {str(e)}\")\n",
        "        with open(\"error_log.json\", \"w\") as f:\n",
        "            json.dump(logs, f, indent=2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsNbU1oC8N9I",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# å®‰è£…ä¾èµ–\n",
        "!pip uninstall -y langchain langchain-core langchain-community langchain-openai openai notion-client\n",
        "!pip install langchain==0.2.0\n",
        "!pip install langchain-core==0.2.38\n",
        "!pip install langchain-community==0.2.0\n",
        "!pip install langchain-openai==0.1.9\n",
        "!pip install openai==1.37.0\n",
        "!pip install notion-client==2.0.0\n",
        "!pip install tqdm python-docx langdetect pydub httpx==0.27.0\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg -y\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import openai\n",
        "import whisper\n",
        "from docx import Document\n",
        "from google.colab import files, userdata\n",
        "from notion_client import Client, errors\n",
        "from langdetect import detect, LangDetectException\n",
        "import datetime\n",
        "import tempfile\n",
        "import torch\n",
        "import subprocess\n",
        "from tqdm import tqdm\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "import httpx\n",
        "\n",
        "# æ¸…é™¤ä»£ç†ç¯å¢ƒå˜é‡\n",
        "for var in ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy']:\n",
        "    if var in os.environ:\n",
        "        del os.environ[var]\n",
        "\n",
        "# åˆå§‹åŒ–Notionå®¢æˆ·ç«¯\n",
        "notion = Client(\n",
        "    auth=userdata.get('NOTION_TOKEN'),\n",
        "    client=httpx.Client(proxies=None)\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# åˆå§‹åŒ–è®¾ç½®\n",
        "# ======================\n",
        "try:\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    NOTION_TOKEN = userdata.get('NOTION_TOKEN')\n",
        "    NOTION_DB_ID = userdata.get('NOTION_DB_ID')\n",
        "    NOTION_PAGE_ID = userdata.get('NOTION_PAGE_ID')\n",
        "\n",
        "    missing_creds = []\n",
        "    if not OPENAI_API_KEY:\n",
        "        missing_creds.append(\"OPENAI_API_KEY\")\n",
        "    if not NOTION_TOKEN:\n",
        "        missing_creds.append(\"NOTION_TOKEN\")\n",
        "    if not NOTION_DB_ID:\n",
        "        missing_creds.append(\"NOTION_DB_ID\")\n",
        "    if not NOTION_PAGE_ID:\n",
        "        missing_creds.append(\"NOTION_PAGE_ID\")\n",
        "\n",
        "    if missing_creds:\n",
        "        raise ValueError(f\"ç¼ºå°‘å‡­è¯: {', '.join(missing_creds)}\")\n",
        "\n",
        "    print(\"âœ… æ‰€æœ‰å‡­è¯å·²è®¾ç½®\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ å‡­è¯è·å–å¤±è´¥: {str(e)}\")\n",
        "    print(\"\\nğŸ”§ è®¾ç½®è¯´æ˜:\")\n",
        "    print(\"1. ç‚¹å‡»å·¦ä¾§è¾¹æ çš„é’¥åŒ™å›¾æ ‡ï¼ˆColabå¯†é’¥ï¼‰\")\n",
        "    print(\"2. æ·»åŠ ä»¥ä¸‹å¯†é’¥:\")\n",
        "    print(\"   - OPENAI_API_KEY: ä½ çš„OpenAI APIå¯†é’¥\")\n",
        "    print(\"   - NOTION_TOKEN: ä½ çš„Notioné›†æˆä»¤ç‰Œ\")\n",
        "    print(\"   - NOTION_DB_ID: Notionæ•°æ®åº“ID\")\n",
        "    print(\"   - NOTION_PAGE_ID: æŠ¥å‘Šçˆ¶é¡µé¢ID\")\n",
        "    print(\"3. æ·»åŠ åé‡æ–°è¿è¡Œæ­¤å•å…ƒæ ¼\")\n",
        "    raise\n",
        "\n",
        "# ======================\n",
        "# æ—¥å¿—ç³»ç»Ÿ\n",
        "# ======================\n",
        "class MeetingLogger:\n",
        "    def __init__(self):\n",
        "        self.logs = {\n",
        "            \"start_time\": datetime.datetime.now().isoformat(),\n",
        "            \"steps\": [],\n",
        "            \"errors\": [],\n",
        "            \"metrics\": {}\n",
        "        }\n",
        "\n",
        "    def log_step(self, step_name, status, details=None, error=None):\n",
        "        entry = {\n",
        "            \"step\": step_name,\n",
        "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "            \"status\": status\n",
        "        }\n",
        "        if details:\n",
        "            entry[\"details\"] = details\n",
        "        if error:\n",
        "            entry[\"error\"] = str(error)\n",
        "        self.logs[\"steps\"].append(entry)\n",
        "\n",
        "    def log_metric(self, name, value):\n",
        "        self.logs[\"metrics\"][name] = value\n",
        "\n",
        "    def save_logs(self, filename=\"meeting_logs.json\"):\n",
        "        with open(filename, \"w\") as f:\n",
        "            json.dump(self.logs, f, indent=2)\n",
        "        return filename\n",
        "\n",
        "    def get_console_log(self):\n",
        "        log_str = f\"=== ä¼šè®®å¤„ç†æ—¥å¿— ===\\n\"\n",
        "        log_str += f\"å¼€å§‹æ—¶é—´: {self.logs['start_time']}\\n\"\n",
        "\n",
        "        for step in self.logs[\"steps\"]:\n",
        "            status_icon = \"âœ…\" if step[\"status\"] == \"success\" else \"âŒ\"\n",
        "            log_str += f\"{status_icon} [{step['timestamp']}] {step['step']}\"\n",
        "            if \"details\" in step:\n",
        "                log_str += f\" - {step['details']}\"\n",
        "            if step[\"status\"] == \"failed\":\n",
        "                log_str += f\" - é”™è¯¯: {step.get('error', 'æœªçŸ¥')}\"\n",
        "            log_str += \"\\n\"\n",
        "\n",
        "        if self.logs[\"metrics\"]:\n",
        "            log_str += \"\\n=== æŒ‡æ ‡ ===\\n\"\n",
        "            for metric, value in self.logs[\"metrics\"].items():\n",
        "                log_str += f\"- {metric}: {value}\\n\"\n",
        "\n",
        "        return log_str\n",
        "\n",
        "logger = MeetingLogger()\n",
        "\n",
        "# ======================\n",
        "# å·¥å…·å‡½æ•°ï¼šå¤„ç†åµŒå¥—ç»“æ„\n",
        "# ======================\n",
        "def flatten_key_points(key_points):\n",
        "    \"\"\"å°†key_pointsä¸­çš„åµŒå¥—ç»“æ„ï¼ˆå­—å…¸/åˆ—è¡¨ï¼‰è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œé€‚é…Notionæ ¼å¼\"\"\"\n",
        "    flattened = {}\n",
        "    for category, items in key_points.items():\n",
        "        flattened_items = []\n",
        "        for item in items:\n",
        "            # å¤„ç†å­—å…¸ç±»å‹ï¼ˆå¦‚{\"éƒ¨é—¨\": [\"é—®é¢˜1\", \"é—®é¢˜2\"]}ï¼‰\n",
        "            if isinstance(item, dict):\n",
        "                dict_strings = []\n",
        "                for k, v in item.items():\n",
        "                    # å­—å…¸çš„å€¼å¦‚æœæ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå¸¦ç¬¦å·çš„å­—ç¬¦ä¸²\n",
        "                    if isinstance(v, list):\n",
        "                        list_str = \"â€¢ \".join([str(i) for i in v])\n",
        "                        dict_strings.append(f\"{k}ï¼šâ€¢ {list_str}\")\n",
        "                    else:\n",
        "                        dict_strings.append(f\"{k}ï¼š{v}\")\n",
        "                flattened_items.append(\"ï¼› \".join(dict_strings))\n",
        "\n",
        "            # å¤„ç†åˆ—è¡¨ç±»å‹ï¼ˆå¦‚[\"é—®é¢˜1\", \"é—®é¢˜2\"]ï¼‰\n",
        "            elif isinstance(item, list):\n",
        "                list_str = \"â€¢ \".join([str(i) for i in item])\n",
        "                flattened_items.append(f\"â€¢ {list_str}\")\n",
        "\n",
        "            # å­—ç¬¦ä¸²ç›´æ¥ä¿ç•™\n",
        "            else:\n",
        "                flattened_items.append(str(item))\n",
        "        flattened[category] = flattened_items\n",
        "    return flattened\n",
        "\n",
        "# ======================\n",
        "# éŸ³é¢‘å¤„ç†\n",
        "# ======================\n",
        "def get_audio_duration(audio_path):\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"format=duration\",\n",
        "             \"-of\", \"default=noprint_wrappers=1:nokey=1\", audio_path],\n",
        "            capture_output=True, text=True\n",
        "        )\n",
        "        duration = float(result.stdout)\n",
        "        logger.log_metric(\"éŸ³é¢‘æ—¶é•¿(ç§’)\", duration)\n",
        "        return duration\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"è·å–éŸ³é¢‘æ—¶é•¿\", \"warning\", error=e)\n",
        "        return max(30, os.path.getsize(audio_path) // 8000)\n",
        "\n",
        "def transcribe_audio(audio_path, model_size=\"base\"):\n",
        "    logger.log_step(\"éŸ³é¢‘è½¬å½•\", \"started\", {\"æ¨¡å‹å¤§å°\": model_size, \"éŸ³é¢‘è·¯å¾„\": audio_path})\n",
        "\n",
        "    try:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        logger.log_step(\"ç¡¬ä»¶æ£€æŸ¥\", \"success\", {\"è®¾å¤‡\": device})\n",
        "\n",
        "        model = whisper.load_model(model_size, device=device)\n",
        "        logger.log_step(\"åŠ è½½æ¨¡å‹\", \"success\")\n",
        "\n",
        "        result = model.transcribe(\n",
        "            audio_path,\n",
        "            fp16=(device == \"cuda\"),\n",
        "            verbose=False,\n",
        "            task=\"transcribe\"\n",
        "        )\n",
        "\n",
        "        transcription = result[\"text\"]\n",
        "        detected_lang = result[\"language\"]\n",
        "        logger.log_step(\"éŸ³é¢‘è½¬å½•\", \"success\", {\n",
        "            \"å­—ç¬¦æ•°\": len(transcription),\n",
        "            \"æ£€æµ‹è¯­è¨€\": detected_lang\n",
        "        })\n",
        "\n",
        "        return transcription, detected_lang\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"éŸ³é¢‘è½¬å½•\", \"failed\", error=e)\n",
        "        raise\n",
        "\n",
        "# ======================\n",
        "# ä¼šè®®åˆ†ææ¨¡å‹ä¸å¤„ç†\n",
        "# ======================\n",
        "class MeetingAnalysis(BaseModel):\n",
        "    meeting_title: str = Field(description=\"ä¼šè®®æ ‡é¢˜\")\n",
        "    participants: list[str] = Field(description=\"å‚ä¸è€…åå•\")\n",
        "    summary: str = Field(description=\"3-5æ®µä¼šè®®æ€»ç»“\")\n",
        "    key_points: dict = Field(description=\"æŒ‰concernsã€decisionsã€updatesã€risksåˆ†ç»„çš„å…³é”®ç‚¹ï¼ˆå‡ä¸ºæ•°ç»„ï¼‰\")\n",
        "    action_items: list[dict] = Field(description=\"è¡ŒåŠ¨é¡¹åˆ—è¡¨ï¼ŒåŒ…å«taskã€assigneeã€due_date\")\n",
        "    meeting_type: str = Field(description=\"ä¼šè®®ç±»å‹\")\n",
        "    platform: str = Field(description=\"ä¼šè®®å¹³å°\")\n",
        "\n",
        "def setup_langchain_chains(language='zh'):\n",
        "    lang_map = {\n",
        "        'zh': \"ç”¨ä¸­æ–‡åˆ†æä¼šè®®è®°å½•ï¼Œè¾“å‡ºä¸¥æ ¼ç¬¦åˆJSONæ ¼å¼ï¼Œkey_pointsçš„å­å­—æ®µå‡ä¸ºæ•°ç»„ï¼ˆç”¨[]åŒ…è£¹ï¼‰\",\n",
        "        'en': \"Analyze the meeting transcript in English, output strict JSON with key_points as arrays\",\n",
        "        'fr': \"Analyser le procÃ¨s-verbal en franÃ§ais, sortie JSON stricte avec key_points en tableaux\"\n",
        "    }\n",
        "    lang_instruction = lang_map.get(language[:2], lang_map['zh'])\n",
        "\n",
        "    parser = JsonOutputParser(pydantic_object=MeetingAnalysis)\n",
        "\n",
        "    prompt_template = PromptTemplate(\n",
        "        template=\"\"\"\n",
        "        {lang_instruction}\n",
        "\n",
        "        {format_instructions}\n",
        "\n",
        "        ### ä¼šè®®è®°å½•:\n",
        "        {transcript}\n",
        "\n",
        "        è¯·ä¸¥æ ¼æŒ‰ç…§æ ¼å¼è¦æ±‚è¾“å‡ºï¼Œç¡®ä¿JSONç»“æ„æ­£ç¡®ã€‚\n",
        "        \"\"\",\n",
        "        input_variables=[\"transcript\"],\n",
        "        partial_variables={\n",
        "            \"lang_instruction\": lang_instruction,\n",
        "            \"format_instructions\": parser.get_format_instructions()\n",
        "        }\n",
        "    )\n",
        "\n",
        "    llm = ChatOpenAI(\n",
        "        openai_api_key=OPENAI_API_KEY,\n",
        "        temperature=0.3,\n",
        "        model=\"gpt-3.5-turbo\"\n",
        "    )\n",
        "\n",
        "    # ä½¿ç”¨æ–°çš„é“¾å¼ç»“æ„\n",
        "    analysis_chain = prompt_template | llm | parser\n",
        "\n",
        "    return analysis_chain\n",
        "\n",
        "def analyze_meeting(transcript, language='zh'):\n",
        "    logger.log_step(\"åˆ†æä¼šè®®\", \"started\", {\"è¯­è¨€\": language})\n",
        "    print(\"\\nå¼€å§‹åˆ†æä¼šè®®å†…å®¹...\")\n",
        "\n",
        "    try:\n",
        "        analysis_chain = setup_langchain_chains(language)\n",
        "        processed_transcript = transcript[:15000]\n",
        "        print(f\"ä½¿ç”¨çš„è½¬å½•æ–‡æœ¬é•¿åº¦: {len(processed_transcript)}å­—ç¬¦\")\n",
        "\n",
        "        parsed = analysis_chain.invoke({\"transcript\": processed_transcript})\n",
        "\n",
        "        parsed[\"language\"] = language\n",
        "        parsed[\"date\"] = datetime.datetime.now().isoformat()\n",
        "\n",
        "        if not parsed.get(\"action_items\"):\n",
        "            logger.log_step(\"æ£€æŸ¥è¡ŒåŠ¨é¡¹\", \"warning\", \"æœªæ£€æµ‹åˆ°è¡ŒåŠ¨é¡¹\")\n",
        "            print(\"âš ï¸ æœªæ£€æµ‹åˆ°è¡ŒåŠ¨é¡¹\")\n",
        "            parsed[\"fallback_used\"] = True\n",
        "        else:\n",
        "            parsed[\"fallback_used\"] = False\n",
        "\n",
        "        logger.log_step(\"åˆ†æä¼šè®®\", \"success\", {\n",
        "            \"æ ‡é¢˜\": parsed[\"meeting_title\"],\n",
        "            \"å‚ä¸è€…æ•°é‡\": len(parsed[\"participants\"]),\n",
        "            \"è¡ŒåŠ¨é¡¹æ•°é‡\": len(parsed[\"action_items\"])\n",
        "        })\n",
        "        print(f\"âœ… ä¼šè®®åˆ†æå®Œæˆ (æ ‡é¢˜: {parsed['meeting_title']})\")\n",
        "        return parsed\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"åˆ†æä¼šè®®å¤±è´¥: {str(e)}\"\n",
        "        logger.log_step(\"åˆ†æä¼šè®®\", \"failed\", error=error_msg)\n",
        "        print(f\"âŒ {error_msg}\")\n",
        "        return {\"error\": error_msg, \"fallback_used\": True}\n",
        "\n",
        "# ======================\n",
        "# NotionæŠ¥å‘Šç”Ÿæˆ\n",
        "# ======================\n",
        "def create_notion_report_page(meeting_data, transcript, logs):\n",
        "    logger.log_step(\"åˆ›å»ºNotionæŠ¥å‘Š\", \"started\")\n",
        "\n",
        "    try:\n",
        "        global notion\n",
        "\n",
        "        # éªŒè¯çˆ¶é¡µé¢\n",
        "        try:\n",
        "            parent_page = notion.pages.retrieve(NOTION_PAGE_ID)\n",
        "            page_title = parent_page.get('properties', {}).get('title', {}).get('title', [{}])[0].get('plain_text', 'æ— æ ‡é¢˜')\n",
        "            logger.log_step(\"çˆ¶é¡µé¢æ£€æŸ¥\", \"success\", {\"é¡µé¢ID\": NOTION_PAGE_ID, \"æ ‡é¢˜\": page_title})\n",
        "            print(f\"âœ… æˆåŠŸè®¿é—®çˆ¶é¡µé¢: {page_title} (ID: {NOTION_PAGE_ID[:8]}...)\")\n",
        "        except errors.APIResponseError as e:\n",
        "            if e.status == 404:\n",
        "                error_msg = f\"çˆ¶é¡µé¢ä¸å­˜åœ¨ (ID: {NOTION_PAGE_ID})ã€‚è¯·æ£€æŸ¥IDæ˜¯å¦æ­£ç¡®ã€‚\"\n",
        "            elif e.status == 403:\n",
        "                error_msg = f\"æ²¡æœ‰è®¿é—®çˆ¶é¡µé¢çš„æƒé™ (ID: {NOTION_PAGE_ID})ã€‚è¯·å°†é¡µé¢å…±äº«ç»™Notioné›†æˆã€‚\"\n",
        "            else:\n",
        "                error_msg = f\"è®¿é—®çˆ¶é¡µé¢å¤±è´¥: {str(e)}\"\n",
        "            logger.log_step(\"çˆ¶é¡µé¢æ£€æŸ¥\", \"failed\", error=error_msg)\n",
        "            print(f\"âŒ {error_msg}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            error_msg = f\"çˆ¶é¡µé¢æ£€æŸ¥å‡ºé”™: {str(e)}\"\n",
        "            logger.log_step(\"çˆ¶é¡µé¢æ£€æŸ¥\", \"failed\", error=error_msg)\n",
        "            print(f\"âŒ {error_msg}\")\n",
        "            return None\n",
        "\n",
        "        # åˆ›å»ºå­é¡µé¢\n",
        "        new_page = notion.pages.create(\n",
        "            parent={\"page_id\": NOTION_PAGE_ID},\n",
        "            properties={\n",
        "                \"title\": {\n",
        "                    \"title\": [\n",
        "                        {\n",
        "                            \"text\": {\n",
        "                                \"content\": meeting_data.get(\"meeting_title\", \"ä¼šè®®æŠ¥å‘Š\")[:200]\n",
        "                            }\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "        page_id = new_page[\"id\"]\n",
        "        logger.log_step(\"åˆ›å»ºå­é¡µé¢\", \"success\", {\"é¡µé¢ID\": page_id})\n",
        "        print(f\"âœ… å·²åˆ›å»ºå­é¡µé¢ (ID: {page_id[:8]}...)\")\n",
        "\n",
        "        # æ„å»ºæŠ¥å‘Šå†…å®¹\n",
        "        children_blocks = []\n",
        "\n",
        "        # 1. ä¼šè®®è¯¦æƒ…\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"ä¼šè®®è¯¦æƒ…\"}}]}\n",
        "        })\n",
        "\n",
        "        details_text = f\"\"\"\n",
        "        **æ—¥æœŸ**: {meeting_data.get('date', 'æœªçŸ¥')}\n",
        "        **å‚ä¸è€…**: {', '.join(meeting_data.get('participants', []))}\n",
        "        **è¯­è¨€**: {meeting_data.get('language', 'æœªçŸ¥')}\n",
        "        **å¹³å°**: {meeting_data.get('platform', 'æœªçŸ¥')}\n",
        "        **ä¼šè®®ç±»å‹**: {meeting_data.get('meeting_type', 'æœªçŸ¥')}\n",
        "        \"\"\"\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"paragraph\",\n",
        "            \"paragraph\": {\"rich_text\": [{\"text\": {\"content\": details_text.strip()}}]}\n",
        "        })\n",
        "\n",
        "        # 2. ä¼šè®®æ€»ç»“\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"æ€»ç»“\"}}]}\n",
        "        })\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"paragraph\",\n",
        "            \"paragraph\": {\"rich_text\": [{\"text\": {\"content\": meeting_data.get('summary', '')}}]}\n",
        "        })\n",
        "\n",
        "        # 3. å…³é”®ç‚¹ï¼ˆä¿®å¤åµŒå¥—ç»“æ„é—®é¢˜ï¼‰\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"å…³é”®ç‚¹\"}}]}\n",
        "        })\n",
        "\n",
        "        key_points = meeting_data.get('key_points', {})\n",
        "        key_points = flatten_key_points(key_points)\n",
        "\n",
        "        for category, items in key_points.items():\n",
        "            children_blocks.append({\n",
        "                \"object\": \"block\",\n",
        "                \"type\": \"heading_3\",\n",
        "                \"heading_3\": {\"rich_text\": [{\"text\": {\"content\": category.capitalize()}}]}\n",
        "            })\n",
        "\n",
        "            if items:\n",
        "                for item in items:\n",
        "                    children_blocks.append({\n",
        "                        \"object\": \"block\",\n",
        "                        \"type\": \"bulleted_list_item\",\n",
        "                        \"bulleted_list_item\": {\"rich_text\": [{\"text\": {\"content\": item}}]}\n",
        "                    })\n",
        "\n",
        "        # 4. è¡ŒåŠ¨é¡¹\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"è¡ŒåŠ¨é¡¹\"}}]}\n",
        "        })\n",
        "\n",
        "        table_rows = []\n",
        "        for idx, item in enumerate(meeting_data.get('action_items', [])):\n",
        "            task = item.get('task', '')\n",
        "            assignee = item.get('assignee', 'æœªåˆ†é…')\n",
        "            due_date = item.get('due_date', 'æ— ')\n",
        "\n",
        "            table_rows.append([\n",
        "                [{\"text\": {\"content\": str(idx+1)}}],\n",
        "                [{\"text\": {\"content\": task}}],\n",
        "                [{\"text\": {\"content\": assignee}}],\n",
        "                [{\"text\": {\"content\": due_date}}]\n",
        "            ])\n",
        "\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"table\",\n",
        "            \"table\": {\n",
        "                \"table_width\": 4,\n",
        "                \"has_column_header\": True,\n",
        "                \"has_row_header\": False,\n",
        "                \"children\": [\n",
        "                    {\n",
        "                        \"object\": \"block\",\n",
        "                        \"type\": \"table_row\",\n",
        "                        \"table_row\": {\n",
        "                            \"cells\": [\n",
        "                                [{\"text\": {\"content\": \"åºå·\"}}],\n",
        "                                [{\"text\": {\"content\": \"ä»»åŠ¡\"}}],\n",
        "                                [{\"text\": {\"content\": \"è´Ÿè´£äºº\"}}],\n",
        "                                [{\"text\": {\"content\": \"æˆªæ­¢æ—¥æœŸ\"}}]\n",
        "                            ]\n",
        "                        }\n",
        "                    },\n",
        "                    *[{\n",
        "                        \"object\": \"block\",\n",
        "                        \"type\": \"table_row\",\n",
        "                        \"table_row\": {\"cells\": cells}\n",
        "                    } for cells in table_rows]\n",
        "                ]\n",
        "            }\n",
        "        })\n",
        "\n",
        "        # 5. å¤„ç†æ—¥å¿—\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"å¤„ç†æ—¥å¿—\"}}]}\n",
        "        })\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"code\",\n",
        "            \"code\": {\n",
        "                \"rich_text\": [{\"text\": {\"content\": logger.get_console_log()}}],\n",
        "                \"language\": \"plain text\"\n",
        "            }\n",
        "        })\n",
        "\n",
        "        # æ·»åŠ å†…å®¹åˆ°é¡µé¢\n",
        "        notion.blocks.children.append(\n",
        "            block_id=page_id,\n",
        "            children=children_blocks\n",
        "        )\n",
        "        logger.log_step(\"æ·»åŠ å†…å®¹åˆ°é¡µé¢\", \"success\")\n",
        "        print(f\"âœ… å·²æ·»åŠ å†…å®¹åˆ°å­é¡µé¢\")\n",
        "\n",
        "\n",
        "        # å…³è”æ•°æ®åº“ï¼ˆä¿®æ”¹åï¼‰\n",
        "        if NOTION_DB_ID:\n",
        "            try:\n",
        "                db = notion.databases.retrieve(NOTION_DB_ID)\n",
        "                logger.log_step(\"æ•°æ®åº“éªŒè¯\", \"success\", {\"db_id\": NOTION_DB_ID})\n",
        "\n",
        "        # æ‰‹åŠ¨æŒ‡å®šä½ çš„å…³ç³»å±æ€§åç§°\n",
        "                relation_prop_name = \"relation\"\n",
        "\n",
        "        # éªŒè¯å±æ€§\n",
        "                if relation_prop_name not in db[\"properties\"]:\n",
        "                    raise ValueError(f\"æ•°æ®åº“ä¸­ä¸å­˜åœ¨åä¸ºã€Œ{relation_prop_name}ã€çš„å±æ€§\")\n",
        "                if db[\"properties\"][relation_prop_name][\"type\"] != \"relation\":\n",
        "                    raise ValueError(f\"å±æ€§ã€Œ{relation_prop_name}ã€ä¸æ˜¯å…³ç³»ç±»å‹\")\n",
        "\n",
        "        # å…³è”\n",
        "                notion.pages.update(\n",
        "            page_id=page_id,\n",
        "            properties={\n",
        "                relation_prop_name: {\n",
        "                    \"relation\": [{\"id\": NOTION_DB_ID}]\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "                logger.log_step(\"å…³è”æ•°æ®åº“\", \"success\", {\"ä½¿ç”¨çš„å±æ€§\": relation_prop_name})\n",
        "                print(f\"âœ… å·²é€šè¿‡å±æ€§ã€Œ{relation_prop_name}ã€å…³è”åˆ°æ•°æ®åº“\")\n",
        "            except Exception as e:\n",
        "                logger.log_step(\"å…³è”æ•°æ®åº“\", \"warning\", error=str(e))\n",
        "                print(f\"âš ï¸ å…³è”æ•°æ®åº“å¤±è´¥: {str(e)}ï¼ˆä¸å½±å“æŠ¥å‘Šç”Ÿæˆï¼‰\")\n",
        "\n",
        "        report_url = new_page.get(\"url\", \"\")\n",
        "        logger.log_step(\"ç”ŸæˆNotionæŠ¥å‘Š\", \"success\", {\"URL\": report_url})\n",
        "        return report_url\n",
        "\n",
        "    except Exception as e:\n",
        "        error_details = f\"Notion APIé”™è¯¯: {str(e)}\"\n",
        "        if hasattr(e, 'response') and hasattr(e.response, 'content'):\n",
        "            error_details += f\"\\nå“åº”: {e.response.content.decode('utf-8')}\"\n",
        "        logger.log_step(\"ç”ŸæˆNotionæŠ¥å‘Š\", \"failed\", error=error_details)\n",
        "        print(f\"âŒ Notionæ“ä½œå¤±è´¥: {error_details}\")\n",
        "        return None\n",
        "\n",
        "# ======================\n",
        "# æƒé™æµ‹è¯•å‡½æ•°\n",
        "# ======================\n",
        "def test_notion_permissions():\n",
        "    print(\"\\n=== å¼€å§‹Notionæƒé™æµ‹è¯• ===\")\n",
        "    print(f\"ä½¿ç”¨çš„çˆ¶é¡µé¢ID: {NOTION_PAGE_ID[:8]}... (å®Œæ•´: {NOTION_PAGE_ID})\")\n",
        "\n",
        "    # 1. æµ‹è¯•é›†æˆä»¤ç‰Œæœ‰æ•ˆæ€§\n",
        "    try:\n",
        "        user_info = notion.users.me()\n",
        "        print(f\"âœ… é›†æˆä»¤ç‰Œæœ‰æ•ˆ (æ‰€å±å·¥ä½œç©ºé—´: {user_info.get('workspace_name', 'æœªçŸ¥')})\")\n",
        "    except errors.UnauthorizedError:\n",
        "        print(f\"âŒ é›†æˆä»¤ç‰Œæ— æ•ˆ (NOTION_TOKENé”™è¯¯)\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ éªŒè¯é›†æˆä»¤ç‰Œæ—¶å‡ºé”™: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "    # 2. æµ‹è¯•çˆ¶é¡µé¢è®¿é—®æƒé™\n",
        "    try:\n",
        "        page = notion.pages.retrieve(NOTION_PAGE_ID)\n",
        "        page_title = page.get('properties', {}).get('title', {}).get('title', [{}])[0].get('plain_text', 'æ— æ ‡é¢˜')\n",
        "        print(f\"âœ… æˆåŠŸè®¿é—®çˆ¶é¡µé¢: {page_title}\")\n",
        "    except errors.APIResponseError as e:\n",
        "        if e.status == 404:\n",
        "            print(f\"âŒ çˆ¶é¡µé¢ä¸å­˜åœ¨ (IDé”™è¯¯æˆ–é¡µé¢å·²åˆ é™¤)\")\n",
        "            return False\n",
        "        elif e.status == 403:\n",
        "            print(f\"âŒ æ²¡æœ‰è®¿é—®æƒé™ (è¯·å°†é¡µé¢å…±äº«ç»™é›†æˆ)\")\n",
        "            return False\n",
        "        else:\n",
        "            print(f\"âŒ è®¿é—®é¡µé¢æ—¶å‡ºé”™ (çŠ¶æ€ç : {e.status}): {str(e)}\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ è®¿é—®é¡µé¢æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "    # 3. æµ‹è¯•æ•°æ®åº“è®¿é—®æƒé™\n",
        "    try:\n",
        "        if NOTION_DB_ID:\n",
        "            db = notion.databases.retrieve(NOTION_DB_ID)\n",
        "            print(f\"âœ… æˆåŠŸè®¿é—®æ•°æ®åº“: {db.get('title', [{}])[0].get('plain_text', 'æ— æ ‡é¢˜')}\")\n",
        "    except errors.APIResponseError as e:\n",
        "        print(f\"âš ï¸ æ•°æ®åº“è®¿é—®è­¦å‘Š: {str(e)}ï¼ˆä»å¯ç”ŸæˆæŠ¥å‘Šï¼Œä½†å¯èƒ½æ— æ³•å…³è”ï¼‰\")\n",
        "\n",
        "    return True\n",
        "\n",
        "# ======================\n",
        "# è¾“å…¥å¤„ç†\n",
        "# ======================\n",
        "def handle_transcript_input():\n",
        "    logger.log_step(\"å¤„ç†è¾“å…¥\", \"started\")\n",
        "\n",
        "    print(\"\\n=== è¾“å…¥æ–¹å¼é€‰æ‹© ===\")\n",
        "    print(\"1: ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ (.mp3/.wav/.m4a/.opus)\")\n",
        "    print(\"2: ä¸Šä¼ æ–‡æœ¬æ–‡ä»¶ (.txt/.docx)\")\n",
        "    print(\"3: ç›´æ¥ç²˜è´´æ–‡æœ¬\")\n",
        "\n",
        "    try:\n",
        "        choice = input(\"è¯·é€‰æ‹©è¾“å…¥æ–¹å¼ (1/2/3): \").strip() or \"1\"\n",
        "    except:\n",
        "        choice = \"1\"\n",
        "\n",
        "    if choice == \"1\":\n",
        "        # éŸ³é¢‘å¤„ç†\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            logger.log_step(\"ä¸Šä¼ éŸ³é¢‘\", \"failed\", \"æœªä¸Šä¼ ä»»ä½•æ–‡ä»¶\")\n",
        "            print(\"âš ï¸ æœªä¸Šä¼ æ–‡ä»¶ï¼Œåˆ‡æ¢åˆ°æ–‡æœ¬è¾“å…¥\")\n",
        "            return handle_transcript_input()\n",
        "\n",
        "        filename = next(iter(uploaded.keys()))\n",
        "        logger.log_step(\"ä¸Šä¼ éŸ³é¢‘\", \"success\", {\"æ–‡ä»¶å\": filename, \"å¤§å°\": len(uploaded[filename])})\n",
        "        print(f\"âœ… å·²ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶: {filename}\")\n",
        "\n",
        "        ext = os.path.splitext(filename)[1].lower()\n",
        "        supported_audio = ['.mp3', '.wav', '.m4a', '.opus']\n",
        "        if ext not in supported_audio:\n",
        "            error = f\"ä¸æ”¯æŒçš„éŸ³é¢‘æ ¼å¼: {ext} (æ”¯æŒ: {', '.join(supported_audio)})\"\n",
        "            logger.log_step(\"å¤„ç†éŸ³é¢‘\", \"failed\", error=error)\n",
        "            print(f\"âŒ {error}\")\n",
        "            raise ValueError(error)\n",
        "\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=ext) as tmp:\n",
        "            tmp.write(uploaded[filename])\n",
        "            audio_path = tmp.name\n",
        "\n",
        "        # é€‰æ‹©æ¨¡å‹\n",
        "        print(\"\\nâš¡ é€‰æ‹©è½¬å½•æ¨¡å‹:\")\n",
        "        print(\"1: å¿«é€Ÿ (tiny, ä½ç²¾åº¦)\")\n",
        "        print(\"2: å¹³è¡¡ (base, æ¨è)\")\n",
        "        print(\"3: é«˜ç²¾åº¦ (small, è¾ƒæ…¢)\")\n",
        "        try:\n",
        "            model_choice = input(\"è¯·é€‰æ‹©æ¨¡å‹ (1/2/3): \").strip() or \"2\"\n",
        "        except:\n",
        "            model_choice = \"2\"\n",
        "        model_map = {\"1\": \"tiny\", \"2\": \"base\", \"3\": \"small\"}\n",
        "        model_size = model_map.get(model_choice, \"base\")\n",
        "        print(f\"ä½¿ç”¨æ¨¡å‹: {model_size}\")\n",
        "\n",
        "        # è½¬å½•\n",
        "        duration = get_audio_duration(audio_path)\n",
        "        print(f\"éŸ³é¢‘æ—¶é•¿: {duration:.1f}ç§’ï¼Œå¼€å§‹è½¬å½•...\")\n",
        "        transcript, detected_lang = transcribe_audio(audio_path, model_size)\n",
        "        os.unlink(audio_path)\n",
        "\n",
        "        print(f\"âœ… è½¬å½•å®Œæˆ (è¯­è¨€: {detected_lang})\")\n",
        "        return transcript, detected_lang\n",
        "\n",
        "    elif choice == \"2\":\n",
        "        # æ–‡æœ¬æ–‡ä»¶\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            logger.log_step(\"ä¸Šä¼ æ–‡æœ¬\", \"failed\", \"æœªä¸Šä¼ ä»»ä½•æ–‡ä»¶\")\n",
        "            print(\"âš ï¸ æœªä¸Šä¼ æ–‡ä»¶ï¼Œåˆ‡æ¢åˆ°ç›´æ¥ç²˜è´´\")\n",
        "            return handle_transcript_input()\n",
        "\n",
        "        filename = next(iter(uploaded.keys()))\n",
        "        logger.log_step(\"ä¸Šä¼ æ–‡æœ¬\", \"success\", {\"æ–‡ä»¶å\": filename})\n",
        "        print(f\"âœ… å·²ä¸Šä¼ æ–‡ä»¶: {filename}\")\n",
        "\n",
        "        try:\n",
        "            if filename.endswith('.txt'):\n",
        "                transcript = uploaded[filename].decode('utf-8')\n",
        "            elif filename.endswith('.docx'):\n",
        "                with tempfile.NamedTemporaryFile(delete=False, suffix='.docx') as tmp:\n",
        "                    tmp.write(uploaded[filename])\n",
        "                    doc = Document(tmp.name)\n",
        "                    transcript = \"\\n\".join(p.text for p in doc.paragraphs)\n",
        "                    os.unlink(tmp.name)\n",
        "            else:\n",
        "                raise ValueError(f\"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {filename} (æ”¯æŒ: .txt, .docx)\")\n",
        "\n",
        "            # æ£€æµ‹è¯­è¨€\n",
        "            lang = detect(transcript[:500]) if transcript else 'zh'\n",
        "            logger.log_step(\"æ£€æµ‹è¯­è¨€\", \"success\", {\"è¯­è¨€\": lang})\n",
        "            print(f\"âœ… è¯»å–å®Œæˆ (æ£€æµ‹è¯­è¨€: {lang})\")\n",
        "            return transcript, lang\n",
        "        except Exception as e:\n",
        "            logger.log_step(\"å¤„ç†æ–‡æœ¬æ–‡ä»¶\", \"failed\", error=str(e))\n",
        "            print(f\"âŒ å¤„ç†æ–‡ä»¶å‡ºé”™: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    elif choice == \"3\":\n",
        "        # ç›´æ¥ç²˜è´´\n",
        "        print(\"\\nè¯·ç²˜è´´ä¼šè®®è®°å½• (ç²˜è´´åæŒ‰Enterï¼Œè¾“å…¥ç©ºè¡Œç»“æŸ):\")\n",
        "        lines = []\n",
        "        while True:\n",
        "            line = input()\n",
        "            if not line:\n",
        "                break\n",
        "            lines.append(line)\n",
        "        transcript = \"\\n\".join(lines)\n",
        "\n",
        "        if not transcript.strip():\n",
        "            logger.log_step(\"è¾“å…¥æ–‡æœ¬\", \"failed\", \"æœªè¾“å…¥ä»»ä½•å†…å®¹\")\n",
        "            print(\"âš ï¸ æœªè¾“å…¥ä»»ä½•å†…å®¹ï¼Œé‡æ–°é€‰æ‹©è¾“å…¥æ–¹å¼\")\n",
        "            return handle_transcript_input()\n",
        "\n",
        "        # æ£€æµ‹è¯­è¨€\n",
        "        try:\n",
        "            lang = detect(transcript[:500])\n",
        "            logger.log_step(\"æ£€æµ‹è¯­è¨€\", \"success\", {\"è¯­è¨€\": lang})\n",
        "            print(f\"âœ… å·²è¾“å…¥æ–‡æœ¬ (æ£€æµ‹è¯­è¨€: {lang})\")\n",
        "        except:\n",
        "            lang = 'zh'\n",
        "            logger.log_step(\"æ£€æµ‹è¯­è¨€\", \"warning\", \"ä½¿ç”¨é»˜è®¤è¯­è¨€ä¸­æ–‡\")\n",
        "            print(f\"âœ… å·²è¾“å…¥æ–‡æœ¬ (ä½¿ç”¨é»˜è®¤è¯­è¨€: ä¸­æ–‡)\")\n",
        "\n",
        "        return transcript, lang\n",
        "\n",
        "    else:\n",
        "        logger.log_step(\"é€‰æ‹©è¾“å…¥æ–¹å¼\", \"warning\", \"æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤éŸ³é¢‘è¾“å…¥\")\n",
        "        print(\"âš ï¸ æ— æ•ˆé€‰æ‹©ï¼Œé»˜è®¤ä½¿ç”¨éŸ³é¢‘è¾“å…¥\")\n",
        "        return handle_transcript_input()\n",
        "\n",
        "# ======================\n",
        "# ä¸»å‡½æ•°\n",
        "# ======================\n",
        "def main():\n",
        "    logger.log_step(\"å·¥ä½œæµç¨‹\", \"started\")\n",
        "    print(\"=== ä¼šè®®è®°å½•å¤„ç†å·¥å…· ===\")\n",
        "\n",
        "    try:\n",
        "        if not test_notion_permissions():\n",
        "            print(\"\\nâŒ æƒé™æµ‹è¯•æœªé€šè¿‡ï¼Œè¯·å…ˆè§£å†³ä¸Šè¿°é—®é¢˜\")\n",
        "            log_file = logger.save_logs(\"error_logs.json\")\n",
        "            print(f\"é”™è¯¯æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}\")\n",
        "            return\n",
        "\n",
        "        transcript, language = handle_transcript_input()\n",
        "        logger.log_metric(\"è½¬å½•æ–‡æœ¬é•¿åº¦\", len(transcript))\n",
        "\n",
        "        meeting_data = analyze_meeting(transcript, language)\n",
        "        if \"error\" in meeting_data:\n",
        "            raise RuntimeError(f\"åˆ†æå¤±è´¥: {meeting_data['error']}\")\n",
        "\n",
        "        print(\"\\nå¼€å§‹åˆ›å»ºNotionæŠ¥å‘Š...\")\n",
        "        report_url = create_notion_report_page(meeting_data, transcript, logger.logs)\n",
        "\n",
        "        if not report_url:\n",
        "            raise RuntimeError(\"åˆ›å»ºNotionæŠ¥å‘Šå¤±è´¥\")\n",
        "\n",
        "        log_file = logger.save_logs()\n",
        "        print(f\"\\nğŸ‰ å¤„ç†å®Œæˆï¼\")\n",
        "        print(f\"ğŸ“„ ä¼šè®®æŠ¥å‘ŠURL: {report_url}\")\n",
        "        print(f\"ğŸ“‹ æ—¥å¿—æ–‡ä»¶: {log_file}\")\n",
        "\n",
        "        from IPython.display import HTML\n",
        "        display(HTML(f'<a href=\"{report_url}\" target=\"_blank\">ç‚¹å‡»æ‰“å¼€NotionæŠ¥å‘Š</a>'))\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"å·¥ä½œæµç¨‹\", \"failed\", error=str(e))\n",
        "        log_file = logger.save_logs(\"error_logs.json\")\n",
        "        print(f\"\\nâŒ å¤„ç†å¤±è´¥ï¼\")\n",
        "        print(f\"é”™è¯¯è¯¦æƒ…: {str(e)}\")\n",
        "        print(f\"é”™è¯¯æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å®‰è£…ä¾èµ– - ä¿®æ”¹éƒ¨åˆ†\n",
        "!pip uninstall -y whisper\n",
        "!pip install faster-whisper\n",
        "!pip install git+https://github.com/openai/whisper.git  # ä¿ç•™åŸæ¥å£ä½†ä½¿ç”¨faster-whisperåç«¯\n",
        "!sudo apt update && sudo apt install ffmpeg -y\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import openai\n",
        "import whisper\n",
        "from docx import Document\n",
        "from google.colab import files, userdata\n",
        "from notion_client import Client, errors\n",
        "from langdetect import detect, LangDetectException\n",
        "import datetime\n",
        "import tempfile\n",
        "import torch\n",
        "import subprocess\n",
        "from tqdm import tqdm\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "import httpx\n",
        "from whisper.utils import get_writer  # å¯¼å…¥faster-whisperçš„è¾…åŠ©å·¥å…·\n",
        "# æ¸…é™¤ä»£ç†ç¯å¢ƒå˜é‡\n",
        "for var in ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy']:\n",
        "    if var in os.environ:\n",
        "        del os.environ[var]\n",
        "# åˆå§‹åŒ–Notionå®¢æˆ·ç«¯\n",
        "http_client = httpx.Client()\n",
        "http_client.proxies = None  # ç¦ç”¨ä»£ç†\n",
        "\n",
        "notion = Client(\n",
        "    auth=userdata.get('NOTION_TOKEN'),\n",
        "    client=http_client\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# åˆå§‹åŒ–è®¾ç½®\n",
        "# ======================\n",
        "try:\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    NOTION_TOKEN = userdata.get('NOTION_TOKEN')\n",
        "    NOTION_DB_ID = userdata.get('NOTION_DB_ID')\n",
        "    NOTION_PAGE_ID = userdata.get('NOTION_PAGE_ID')\n",
        "\n",
        "    missing_creds = []\n",
        "    if not OPENAI_API_KEY:\n",
        "        missing_creds.append(\"OPENAI_API_KEY\")\n",
        "    if not NOTION_TOKEN:\n",
        "        missing_creds.append(\"NOTION_TOKEN\")\n",
        "    if not NOTION_DB_ID:\n",
        "        missing_creds.append(\"NOTION_DB_ID\")\n",
        "    if not NOTION_PAGE_ID:\n",
        "        missing_creds.append(\"NOTION_PAGE_ID\")\n",
        "\n",
        "    if missing_creds:\n",
        "        raise ValueError(f\"ç¼ºå°‘å‡­è¯: {', '.join(missing_creds)}\")\n",
        "\n",
        "    print(\"âœ… æ‰€æœ‰å‡­è¯å·²è®¾ç½®\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ å‡­è¯è·å–å¤±è´¥: {str(e)}\")\n",
        "    print(\"\\nğŸ”§ è®¾ç½®è¯´æ˜:\")\n",
        "    print(\"1. ç‚¹å‡»å·¦ä¾§è¾¹æ çš„é’¥åŒ™å›¾æ ‡ï¼ˆColabå¯†é’¥ï¼‰\")\n",
        "    print(\"2. æ·»åŠ ä»¥ä¸‹å¯†é’¥:\")\n",
        "    print(\"   - OPENAI_API_KEY: ä½ çš„OpenAI APIå¯†é’¥\")\n",
        "    print(\"   - NOTION_TOKEN: ä½ çš„Notioné›†æˆä»¤ç‰Œ\")\n",
        "    print(\"   - NOTION_DB_ID: Notionæ•°æ®åº“ID\")\n",
        "    print(\"   - NOTION_PAGE_ID: æŠ¥å‘Šçˆ¶é¡µé¢ID\")\n",
        "    print(\"3. æ·»åŠ åé‡æ–°è¿è¡Œæ­¤å•å…ƒæ ¼\")\n",
        "    raise\n",
        "\n",
        "# ======================\n",
        "# æ—¥å¿—ç³»ç»Ÿ\n",
        "# ======================\n",
        "class MeetingLogger:\n",
        "    def __init__(self):\n",
        "        self.logs = {\n",
        "            \"start_time\": datetime.datetime.now().isoformat(),\n",
        "            \"steps\": [],\n",
        "            \"errors\": [],\n",
        "            \"metrics\": {}\n",
        "        }\n",
        "\n",
        "    def log_step(self, step_name, status, details=None, error=None):\n",
        "        entry = {\n",
        "            \"step\": step_name,\n",
        "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "            \"status\": status\n",
        "        }\n",
        "        if details:\n",
        "            entry[\"details\"] = details\n",
        "        if error:\n",
        "            entry[\"error\"] = str(error)\n",
        "        self.logs[\"steps\"].append(entry)\n",
        "\n",
        "    def log_metric(self, name, value):\n",
        "        self.logs[\"metrics\"][name] = value\n",
        "\n",
        "    def save_logs(self, filename=\"meeting_logs.json\"):\n",
        "        with open(filename, \"w\") as f:\n",
        "            json.dump(self.logs, f, indent=2)\n",
        "        return filename\n",
        "\n",
        "    def get_console_log(self):\n",
        "        log_str = f\"=== ä¼šè®®å¤„ç†æ—¥å¿— ===\\n\"\n",
        "        log_str += f\"å¼€å§‹æ—¶é—´: {self.logs['start_time']}\\n\"\n",
        "\n",
        "        for step in self.logs[\"steps\"]:\n",
        "            status_icon = \"âœ…\" if step[\"status\"] == \"success\" else \"âŒ\"\n",
        "            log_str += f\"{status_icon} [{step['timestamp']}] {step['step']}\"\n",
        "            if \"details\" in step:\n",
        "                log_str += f\" - {step['details']}\"\n",
        "            if step[\"status\"] == \"failed\":\n",
        "                log_str += f\" - é”™è¯¯: {step.get('error', 'æœªçŸ¥')}\"\n",
        "            log_str += \"\\n\"\n",
        "\n",
        "        if self.logs[\"metrics\"]:\n",
        "            log_str += \"\\n=== æŒ‡æ ‡ ===\\n\"\n",
        "            for metric, value in self.logs[\"metrics\"].items():\n",
        "                log_str += f\"- {metric}: {value}\\n\"\n",
        "\n",
        "        return log_str\n",
        "\n",
        "logger = MeetingLogger()\n",
        "\n",
        "# ======================\n",
        "# å·¥å…·å‡½æ•°ï¼šå¤„ç†åµŒå¥—ç»“æ„\n",
        "# ======================\n",
        "def flatten_key_points(key_points):\n",
        "    \"\"\"å°†key_pointsä¸­çš„åµŒå¥—ç»“æ„ï¼ˆå­—å…¸/åˆ—è¡¨ï¼‰è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œé€‚é…Notionæ ¼å¼\"\"\"\n",
        "    flattened = {}\n",
        "    for category, items in key_points.items():\n",
        "        flattened_items = []\n",
        "        for item in items:\n",
        "            # å¤„ç†å­—å…¸ç±»å‹ï¼ˆå¦‚{\"éƒ¨é—¨\": [\"é—®é¢˜1\", \"é—®é¢˜2\"]}ï¼‰\n",
        "            if isinstance(item, dict):\n",
        "                dict_strings = []\n",
        "                for k, v in item.items():\n",
        "                    # å­—å…¸çš„å€¼å¦‚æœæ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå¸¦ç¬¦å·çš„å­—ç¬¦ä¸²\n",
        "                    if isinstance(v, list):\n",
        "                        list_str = \"â€¢ \".join([str(i) for i in v])\n",
        "                        dict_strings.append(f\"{k}ï¼šâ€¢ {list_str}\")\n",
        "                    else:\n",
        "                        dict_strings.append(f\"{k}ï¼š{v}\")\n",
        "                flattened_items.append(\"ï¼› \".join(dict_strings))\n",
        "\n",
        "            # å¤„ç†åˆ—è¡¨ç±»å‹ï¼ˆå¦‚[\"é—®é¢˜1\", \"é—®é¢˜2\"]ï¼‰\n",
        "            elif isinstance(item, list):\n",
        "                list_str = \"â€¢ \".join([str(i) for i in item])\n",
        "                flattened_items.append(f\"â€¢ {list_str}\")\n",
        "\n",
        "            # å­—ç¬¦ä¸²ç›´æ¥ä¿ç•™\n",
        "            else:\n",
        "                flattened_items.append(str(item))\n",
        "        flattened[category] = flattened_items\n",
        "    return flattened\n",
        "\n",
        "# ======================\n",
        "# éŸ³é¢‘å¤„ç† - ä¿®æ”¹éƒ¨åˆ†\n",
        "# ======================\n",
        "def transcribe_audio(audio_path, model_size=\"base\"):\n",
        "    logger.log_step(\"éŸ³é¢‘è½¬å½•\", \"started\", {\"æ¨¡å‹å¤§å°\": model_size, \"éŸ³é¢‘è·¯å¾„\": audio_path})\n",
        "\n",
        "    try:\n",
        "        # ä½¿ç”¨faster-whisperè¿›è¡Œè½¬å½•\n",
        "        from faster_whisper import WhisperModel\n",
        "\n",
        "        # æ ¹æ®GPUå¯ç”¨æ€§é€‰æ‹©è®¡ç®—è®¾å¤‡\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n",
        "\n",
        "        logger.log_step(\"åŠ è½½faster-whisperæ¨¡å‹\", \"info\", {\n",
        "            \"è®¾å¤‡\": device,\n",
        "            \"è®¡ç®—ç±»å‹\": compute_type,\n",
        "            \"æ¨¡å‹å¤§å°\": model_size\n",
        "        })\n",
        "\n",
        "        # åŠ è½½æ¨¡å‹ - ä½¿ç”¨æœ¬åœ°ç¼“å­˜é¿å…é‡å¤ä¸‹è½½\n",
        "        model = WhisperModel(\n",
        "            model_size,\n",
        "            device=device,\n",
        "            compute_type=compute_type,\n",
        "            download_root=\"/content/models\"  # è®¾ç½®æ¨¡å‹ç¼“å­˜ç›®å½•\n",
        "        )\n",
        "\n",
        "        # æ‰§è¡Œè½¬å½•\n",
        "        logger.log_step(\"å¼€å§‹è½¬å½•\", \"processing\")\n",
        "        segments, info = model.transcribe(\n",
        "            audio_path,\n",
        "            beam_size=5,  # å¹³è¡¡é€Ÿåº¦å’Œå‡†ç¡®åº¦\n",
        "            vad_filter=True,  # å¯ç”¨è¯­éŸ³æ´»åŠ¨æ£€æµ‹\n",
        "            word_timestamps=False  # ä¸éœ€è¦å•è¯çº§æ—¶é—´æˆ³\n",
        "        )\n",
        "\n",
        "        # æ£€æµ‹è¯­è¨€\n",
        "        detected_lang = info.language\n",
        "        logger.log_step(\"è¯­è¨€æ£€æµ‹\", \"success\", {\"è¯­è¨€\": detected_lang})\n",
        "\n",
        "        # æ”¶é›†è½¬å½•æ–‡æœ¬\n",
        "        transcription = \"\"\n",
        "        segment_list = []\n",
        "\n",
        "        # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºè½¬å½•è¿‡ç¨‹\n",
        "        with tqdm(total=info.duration, unit='sec', desc=\"è½¬å½•è¿›åº¦\") as pbar:\n",
        "            for segment in segments:\n",
        "                segment_list.append(segment.text)\n",
        "                pbar.update(segment.end - pbar.n)\n",
        "\n",
        "        transcription = \" \".join(segment_list)\n",
        "\n",
        "        logger.log_step(\"éŸ³é¢‘è½¬å½•\", \"success\", {\n",
        "            \"å­—ç¬¦æ•°\": len(transcription),\n",
        "            \"æ£€æµ‹è¯­è¨€\": detected_lang,\n",
        "            \"éŸ³é¢‘æ—¶é•¿\": f\"{info.duration:.2f}ç§’\"\n",
        "        })\n",
        "\n",
        "        return transcription, detected_lang\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"éŸ³é¢‘è½¬å½•\", \"failed\", error=e)\n",
        "        # å›é€€åˆ°åŸå§‹whisper\n",
        "        logger.log_step(\"å°è¯•ä½¿ç”¨åŸå§‹whisper\", \"fallback\")\n",
        "        try:\n",
        "            model = whisper.load_model(model_size)\n",
        "            result = model.transcribe(audio_path)\n",
        "            return result[\"text\"], result[\"language\"]\n",
        "        except Exception as fallback_error:\n",
        "            logger.log_step(\"åŸå§‹whisperå›é€€å¤±è´¥\", \"failed\", error=fallback_error)\n",
        "            raise RuntimeError(f\"è½¬å½•å¤±è´¥: {str(e)} | å›é€€å¤±è´¥: {str(fallback_error)}\")\n",
        "# ======================\n",
        "# ä¼šè®®åˆ†ææ¨¡å‹ä¸å¤„ç†\n",
        "# ======================\n",
        "class MeetingAnalysis(BaseModel):\n",
        "    meeting_title: str = Field(description=\"ä¼šè®®æ ‡é¢˜\")\n",
        "    participants: list[str] = Field(description=\"å‚ä¸è€…åå•\")\n",
        "    summary: str = Field(description=\"3-5æ®µä¼šè®®æ€»ç»“\")\n",
        "    key_points: dict = Field(description=\"æŒ‰concernsã€decisionsã€updatesã€risksåˆ†ç»„çš„å…³é”®ç‚¹ï¼ˆå‡ä¸ºæ•°ç»„ï¼‰\")\n",
        "    action_items: list[dict] = Field(description=\"è¡ŒåŠ¨é¡¹åˆ—è¡¨ï¼ŒåŒ…å«taskã€assigneeã€due_date\")\n",
        "    meeting_type: str = Field(description=\"ä¼šè®®ç±»å‹\")\n",
        "    platform: str = Field(description=\"ä¼šè®®å¹³å°\")\n",
        "\n",
        "def setup_langchain_chains(language='zh'):\n",
        "    lang_map = {\n",
        "        'zh': \"ç”¨ä¸­æ–‡åˆ†æä¼šè®®è®°å½•ï¼Œè¾“å‡ºä¸¥æ ¼ç¬¦åˆJSONæ ¼å¼ï¼Œkey_pointsçš„å­å­—æ®µå‡ä¸ºæ•°ç»„ï¼ˆç”¨[]åŒ…è£¹ï¼‰\",\n",
        "        'en': \"Analyze the meeting transcript in English, output strict JSON with key_points as arrays\",\n",
        "        'fr': \"Analyser le procÃ¨s-verbal en franÃ§ais, sortie JSON stricte avec key_points en tableaux\"\n",
        "    }\n",
        "    lang_instruction = lang_map.get(language[:2], lang_map['zh'])\n",
        "\n",
        "    parser = JsonOutputParser(pydantic_object=MeetingAnalysis)\n",
        "\n",
        "    prompt_template = PromptTemplate(\n",
        "        template=\"\"\"\n",
        "        {lang_instruction}\n",
        "\n",
        "        {format_instructions}\n",
        "\n",
        "        ### ä¼šè®®è®°å½•:\n",
        "        {transcript}\n",
        "\n",
        "        è¯·ä¸¥æ ¼æŒ‰ç…§æ ¼å¼è¦æ±‚è¾“å‡ºï¼Œç¡®ä¿JSONç»“æ„æ­£ç¡®ã€‚\n",
        "        \"\"\",\n",
        "        input_variables=[\"transcript\"],\n",
        "        partial_variables={\n",
        "            \"lang_instruction\": lang_instruction,\n",
        "            \"format_instructions\": parser.get_format_instructions()\n",
        "        }\n",
        "    )\n",
        "\n",
        "    llm = ChatOpenAI(\n",
        "        openai_api_key=OPENAI_API_KEY,\n",
        "        temperature=0.3,\n",
        "        model=\"gpt-3.5-turbo\"\n",
        "    )\n",
        "\n",
        "    # ä½¿ç”¨æ–°çš„é“¾å¼ç»“æ„\n",
        "    analysis_chain = prompt_template | llm | parser\n",
        "\n",
        "    return analysis_chain\n",
        "\n",
        "def analyze_meeting(transcript, language='zh'):\n",
        "    logger.log_step(\"åˆ†æä¼šè®®\", \"started\", {\"è¯­è¨€\": language})\n",
        "    print(\"\\nå¼€å§‹åˆ†æä¼šè®®å†…å®¹...\")\n",
        "\n",
        "    try:\n",
        "        analysis_chain = setup_langchain_chains(language)\n",
        "        processed_transcript = transcript[:15000]\n",
        "        print(f\"ä½¿ç”¨çš„è½¬å½•æ–‡æœ¬é•¿åº¦: {len(processed_transcript)}å­—ç¬¦\")\n",
        "\n",
        "        parsed = analysis_chain.invoke({\"transcript\": processed_transcript})\n",
        "\n",
        "        parsed[\"language\"] = language\n",
        "        parsed[\"date\"] = datetime.datetime.now().isoformat()\n",
        "\n",
        "        if not parsed.get(\"action_items\"):\n",
        "            logger.log_step(\"æ£€æŸ¥è¡ŒåŠ¨é¡¹\", \"warning\", \"æœªæ£€æµ‹åˆ°è¡ŒåŠ¨é¡¹\")\n",
        "            print(\"âš ï¸ æœªæ£€æµ‹åˆ°è¡ŒåŠ¨é¡¹\")\n",
        "            parsed[\"fallback_used\"] = True\n",
        "        else:\n",
        "            parsed[\"fallback_used\"] = False\n",
        "\n",
        "        logger.log_step(\"åˆ†æä¼šè®®\", \"success\", {\n",
        "            \"æ ‡é¢˜\": parsed[\"meeting_title\"],\n",
        "            \"å‚ä¸è€…æ•°é‡\": len(parsed[\"participants\"]),\n",
        "            \"è¡ŒåŠ¨é¡¹æ•°é‡\": len(parsed[\"action_items\"])\n",
        "        })\n",
        "        print(f\"âœ… ä¼šè®®åˆ†æå®Œæˆ (æ ‡é¢˜: {parsed['meeting_title']})\")\n",
        "        return parsed\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"åˆ†æä¼šè®®å¤±è´¥: {str(e)}\"\n",
        "        logger.log_step(\"åˆ†æä¼šè®®\", \"failed\", error=error_msg)\n",
        "        print(f\"âŒ {error_msg}\")\n",
        "        return {\"error\": error_msg, \"fallback_used\": True}\n",
        "\n",
        "# ======================\n",
        "# NotionæŠ¥å‘Šç”Ÿæˆ\n",
        "# ======================\n",
        "def create_notion_report_page(meeting_data, transcript, logs):\n",
        "    logger.log_step(\"åˆ›å»ºNotionæŠ¥å‘Š\", \"started\")\n",
        "\n",
        "    try:\n",
        "        global notion\n",
        "\n",
        "        # éªŒè¯çˆ¶é¡µé¢\n",
        "        try:\n",
        "            parent_page = notion.pages.retrieve(NOTION_PAGE_ID)\n",
        "            page_title = parent_page.get('properties', {}).get('title', {}).get('title', [{}])[0].get('plain_text', 'æ— æ ‡é¢˜')\n",
        "            logger.log_step(\"çˆ¶é¡µé¢æ£€æŸ¥\", \"success\", {\"é¡µé¢ID\": NOTION_PAGE_ID, \"æ ‡é¢˜\": page_title})\n",
        "            print(f\"âœ… æˆåŠŸè®¿é—®çˆ¶é¡µé¢: {page_title} (ID: {NOTION_PAGE_ID[:8]}...)\")\n",
        "        except errors.APIResponseError as e:\n",
        "            if e.status == 404:\n",
        "                error_msg = f\"çˆ¶é¡µé¢ä¸å­˜åœ¨ (ID: {NOTION_PAGE_ID})ã€‚è¯·æ£€æŸ¥IDæ˜¯å¦æ­£ç¡®ã€‚\"\n",
        "            elif e.status == 403:\n",
        "                error_msg = f\"æ²¡æœ‰è®¿é—®çˆ¶é¡µé¢çš„æƒé™ (ID: {NOTION_PAGE_ID})ã€‚è¯·å°†é¡µé¢å…±äº«ç»™Notioné›†æˆã€‚\"\n",
        "            else:\n",
        "                error_msg = f\"è®¿é—®çˆ¶é¡µé¢å¤±è´¥: {str(e)}\"\n",
        "            logger.log_step(\"çˆ¶é¡µé¢æ£€æŸ¥\", \"failed\", error=error_msg)\n",
        "            print(f\"âŒ {error_msg}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            error_msg = f\"çˆ¶é¡µé¢æ£€æŸ¥å‡ºé”™: {str(e)}\"\n",
        "            logger.log_step(\"çˆ¶é¡µé¢æ£€æŸ¥\", \"failed\", error=error_msg)\n",
        "            print(f\"âŒ {error_msg}\")\n",
        "            return None\n",
        "\n",
        "        # åˆ›å»ºå­é¡µé¢\n",
        "        new_page = notion.pages.create(\n",
        "            parent={\"page_id\": NOTION_PAGE_ID},\n",
        "            properties={\n",
        "                \"title\": {\n",
        "                    \"title\": [\n",
        "                        {\n",
        "                            \"text\": {\n",
        "                                \"content\": meeting_data.get(\"meeting_title\", \"ä¼šè®®æŠ¥å‘Š\")[:200]\n",
        "                            }\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "        page_id = new_page[\"id\"]\n",
        "        logger.log_step(\"åˆ›å»ºå­é¡µé¢\", \"success\", {\"é¡µé¢ID\": page_id})\n",
        "        print(f\"âœ… å·²åˆ›å»ºå­é¡µé¢ (ID: {page_id[:8]}...)\")\n",
        "\n",
        "        # æ„å»ºæŠ¥å‘Šå†…å®¹\n",
        "        children_blocks = []\n",
        "\n",
        "        # 1. ä¼šè®®è¯¦æƒ…\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"ä¼šè®®è¯¦æƒ…\"}}]}\n",
        "        })\n",
        "\n",
        "        details_text = f\"\"\"\n",
        "        **æ—¥æœŸ**: {meeting_data.get('date', 'æœªçŸ¥')}\n",
        "        **å‚ä¸è€…**: {', '.join(meeting_data.get('participants', []))}\n",
        "        **è¯­è¨€**: {meeting_data.get('language', 'æœªçŸ¥')}\n",
        "        **å¹³å°**: {meeting_data.get('platform', 'æœªçŸ¥')}\n",
        "        **ä¼šè®®ç±»å‹**: {meeting_data.get('meeting_type', 'æœªçŸ¥')}\n",
        "        \"\"\"\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"paragraph\",\n",
        "            \"paragraph\": {\"rich_text\": [{\"text\": {\"content\": details_text.strip()}}]}\n",
        "        })\n",
        "\n",
        "        # 2. ä¼šè®®æ€»ç»“\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"æ€»ç»“\"}}]}\n",
        "        })\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"paragraph\",\n",
        "            \"paragraph\": {\"rich_text\": [{\"text\": {\"content\": meeting_data.get('summary', '')}}]}\n",
        "        })\n",
        "\n",
        "        # 3. å…³é”®ç‚¹ï¼ˆä¿®å¤åµŒå¥—ç»“æ„é—®é¢˜ï¼‰\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"å…³é”®ç‚¹\"}}]}\n",
        "        })\n",
        "\n",
        "        key_points = meeting_data.get('key_points', {})\n",
        "        key_points = flatten_key_points(key_points)\n",
        "\n",
        "        for category, items in key_points.items():\n",
        "            children_blocks.append({\n",
        "                \"object\": \"block\",\n",
        "                \"type\": \"heading_3\",\n",
        "                \"heading_3\": {\"rich_text\": [{\"text\": {\"content\": category.capitalize()}}]}\n",
        "            })\n",
        "\n",
        "            if items:\n",
        "                for item in items:\n",
        "                    children_blocks.append({\n",
        "                        \"object\": \"block\",\n",
        "                        \"type\": \"bulleted_list_item\",\n",
        "                        \"bulleted_list_item\": {\"rich_text\": [{\"text\": {\"content\": item}}]}\n",
        "                    })\n",
        "\n",
        "        # 4. è¡ŒåŠ¨é¡¹\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"è¡ŒåŠ¨é¡¹\"}}]}\n",
        "        })\n",
        "\n",
        "        table_rows = []\n",
        "        for idx, item in enumerate(meeting_data.get('action_items', [])):\n",
        "            task = item.get('task', '')\n",
        "            assignee = item.get('assignee', 'æœªåˆ†é…')\n",
        "            due_date = item.get('due_date', 'æ— ')\n",
        "\n",
        "            table_rows.append([\n",
        "                [{\"text\": {\"content\": str(idx+1)}}],\n",
        "                [{\"text\": {\"content\": task}}],\n",
        "                [{\"text\": {\"content\": assignee}}],\n",
        "                [{\"text\": {\"content\": due_date}}]\n",
        "            ])\n",
        "\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"table\",\n",
        "            \"table\": {\n",
        "                \"table_width\": 4,\n",
        "                \"has_column_header\": True,\n",
        "                \"has_row_header\": False,\n",
        "                \"children\": [\n",
        "                    {\n",
        "                        \"object\": \"block\",\n",
        "                        \"type\": \"table_row\",\n",
        "                        \"table_row\": {\n",
        "                            \"cells\": [\n",
        "                                [{\"text\": {\"content\": \"åºå·\"}}],\n",
        "                                [{\"text\": {\"content\": \"ä»»åŠ¡\"}}],\n",
        "                                [{\"text\": {\"content\": \"è´Ÿè´£äºº\"}}],\n",
        "                                [{\"text\": {\"content\": \"æˆªæ­¢æ—¥æœŸ\"}}]\n",
        "                            ]\n",
        "                        }\n",
        "                    },\n",
        "                    *[{\n",
        "                        \"object\": \"block\",\n",
        "                        \"type\": \"table_row\",\n",
        "                        \"table_row\": {\"cells\": cells}\n",
        "                    } for cells in table_rows]\n",
        "                ]\n",
        "            }\n",
        "        })\n",
        "\n",
        "        # 5. å¤„ç†æ—¥å¿—\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"å¤„ç†æ—¥å¿—\"}}]}\n",
        "        })\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"code\",\n",
        "            \"code\": {\n",
        "                \"rich_text\": [{\"text\": {\"content\": logger.get_console_log()}}],\n",
        "                \"language\": \"plain text\"\n",
        "            }\n",
        "        })\n",
        "\n",
        "        # æ·»åŠ å†…å®¹åˆ°é¡µé¢\n",
        "        notion.blocks.children.append(\n",
        "            block_id=page_id,\n",
        "            children=children_blocks\n",
        "        )\n",
        "        logger.log_step(\"æ·»åŠ å†…å®¹åˆ°é¡µé¢\", \"success\")\n",
        "        print(f\"âœ… å·²æ·»åŠ å†…å®¹åˆ°å­é¡µé¢\")\n",
        "\n",
        "\n",
        "        # å…³è”æ•°æ®åº“ï¼ˆä¿®æ”¹åï¼‰\n",
        "        if NOTION_DB_ID:\n",
        "            try:\n",
        "                db = notion.databases.retrieve(NOTION_DB_ID)\n",
        "                logger.log_step(\"æ•°æ®åº“éªŒè¯\", \"success\", {\"db_id\": NOTION_DB_ID})\n",
        "\n",
        "        # æ‰‹åŠ¨æŒ‡å®šä½ çš„å…³ç³»å±æ€§åç§°\n",
        "                relation_prop_name = \"relation\"\n",
        "\n",
        "        # éªŒè¯å±æ€§\n",
        "                if relation_prop_name not in db[\"properties\"]:\n",
        "                    raise ValueError(f\"æ•°æ®åº“ä¸­ä¸å­˜åœ¨åä¸ºã€Œ{relation_prop_name}ã€çš„å±æ€§\")\n",
        "                if db[\"properties\"][relation_prop_name][\"type\"] != \"relation\":\n",
        "                    raise ValueError(f\"å±æ€§ã€Œ{relation_prop_name}ã€ä¸æ˜¯å…³ç³»ç±»å‹\")\n",
        "\n",
        "        # å…³è”\n",
        "                notion.pages.update(\n",
        "            page_id=page_id,\n",
        "            properties={\n",
        "                relation_prop_name: {\n",
        "                    \"relation\": [{\"id\": NOTION_DB_ID}]\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "                logger.log_step(\"å…³è”æ•°æ®åº“\", \"success\", {\"ä½¿ç”¨çš„å±æ€§\": relation_prop_name})\n",
        "                print(f\"âœ… å·²é€šè¿‡å±æ€§ã€Œ{relation_prop_name}ã€å…³è”åˆ°æ•°æ®åº“\")\n",
        "            except Exception as e:\n",
        "                logger.log_step(\"å…³è”æ•°æ®åº“\", \"warning\", error=str(e))\n",
        "                print(f\"âš ï¸ å…³è”æ•°æ®åº“å¤±è´¥: {str(e)}ï¼ˆä¸å½±å“æŠ¥å‘Šç”Ÿæˆï¼‰\")\n",
        "\n",
        "        report_url = new_page.get(\"url\", \"\")\n",
        "        logger.log_step(\"ç”ŸæˆNotionæŠ¥å‘Š\", \"success\", {\"URL\": report_url})\n",
        "        return report_url\n",
        "\n",
        "    except Exception as e:\n",
        "        error_details = f\"Notion APIé”™è¯¯: {str(e)}\"\n",
        "        if hasattr(e, 'response') and hasattr(e.response, 'content'):\n",
        "            error_details += f\"\\nå“åº”: {e.response.content.decode('utf-8')}\"\n",
        "        logger.log_step(\"ç”ŸæˆNotionæŠ¥å‘Š\", \"failed\", error=error_details)\n",
        "        print(f\"âŒ Notionæ“ä½œå¤±è´¥: {error_details}\")\n",
        "        return None\n",
        "\n",
        "# ======================\n",
        "# æƒé™æµ‹è¯•å‡½æ•°\n",
        "# ======================\n",
        "def test_notion_permissions():\n",
        "    print(\"\\n=== å¼€å§‹Notionæƒé™æµ‹è¯• ===\")\n",
        "    print(f\"ä½¿ç”¨çš„çˆ¶é¡µé¢ID: {NOTION_PAGE_ID[:8]}... (å®Œæ•´: {NOTION_PAGE_ID})\")\n",
        "\n",
        "    # 1. æµ‹è¯•é›†æˆä»¤ç‰Œæœ‰æ•ˆæ€§\n",
        "    try:\n",
        "        user_info = notion.users.me()\n",
        "        print(f\"âœ… é›†æˆä»¤ç‰Œæœ‰æ•ˆ (æ‰€å±å·¥ä½œç©ºé—´: {user_info.get('workspace_name', 'æœªçŸ¥')})\")\n",
        "    except errors.UnauthorizedError:\n",
        "        print(f\"âŒ é›†æˆä»¤ç‰Œæ— æ•ˆ (NOTION_TOKENé”™è¯¯)\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ éªŒè¯é›†æˆä»¤ç‰Œæ—¶å‡ºé”™: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "    # 2. æµ‹è¯•çˆ¶é¡µé¢è®¿é—®æƒé™\n",
        "    try:\n",
        "        page = notion.pages.retrieve(NOTION_PAGE_ID)\n",
        "        page_title = page.get('properties', {}).get('title', {}).get('title', [{}])[0].get('plain_text', 'æ— æ ‡é¢˜')\n",
        "        print(f\"âœ… æˆåŠŸè®¿é—®çˆ¶é¡µé¢: {page_title}\")\n",
        "    except errors.APIResponseError as e:\n",
        "        if e.status == 404:\n",
        "            print(f\"âŒ çˆ¶é¡µé¢ä¸å­˜åœ¨ (IDé”™è¯¯æˆ–é¡µé¢å·²åˆ é™¤)\")\n",
        "            return False\n",
        "        elif e.status == 403:\n",
        "            print(f\"âŒ æ²¡æœ‰è®¿é—®æƒé™ (è¯·å°†é¡µé¢å…±äº«ç»™é›†æˆ)\")\n",
        "            return False\n",
        "        else:\n",
        "            print(f\"âŒ è®¿é—®é¡µé¢æ—¶å‡ºé”™ (çŠ¶æ€ç : {e.status}): {str(e)}\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ è®¿é—®é¡µé¢æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "    # 3. æµ‹è¯•æ•°æ®åº“è®¿é—®æƒé™\n",
        "    try:\n",
        "        if NOTION_DB_ID:\n",
        "            db = notion.databases.retrieve(NOTION_DB_ID)\n",
        "            print(f\"âœ… æˆåŠŸè®¿é—®æ•°æ®åº“: {db.get('title', [{}])[0].get('plain_text', 'æ— æ ‡é¢˜')}\")\n",
        "    except errors.APIResponseError as e:\n",
        "        print(f\"âš ï¸ æ•°æ®åº“è®¿é—®è­¦å‘Š: {str(e)}ï¼ˆä»å¯ç”ŸæˆæŠ¥å‘Šï¼Œä½†å¯èƒ½æ— æ³•å…³è”ï¼‰\")\n",
        "\n",
        "    return True\n",
        "\n",
        "# ======================\n",
        "# è¾“å…¥å¤„ç†\n",
        "# ======================\n",
        "# åœ¨ handle_transcript_input å‡½æ•°ä¸­æ·»åŠ ç¼ºå¤±çš„éŸ³é¢‘ä¸Šä¼ ä»£ç \n",
        "def handle_transcript_input():\n",
        "    logger.log_step(\"å¤„ç†è¾“å…¥\", \"started\")\n",
        "\n",
        "    print(\"\\n=== è¾“å…¥æ–¹å¼é€‰æ‹© ===\")\n",
        "    print(\"1: ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ (.mp3/.wav/.m4a/.opus)\")\n",
        "    print(\"2: ä¸Šä¼ æ–‡æœ¬æ–‡ä»¶ (.txt/.docx)\")\n",
        "    print(\"3: ç›´æ¥ç²˜è´´æ–‡æœ¬\")\n",
        "\n",
        "    try:\n",
        "        choice = input(\"è¯·é€‰æ‹©è¾“å…¥æ–¹å¼ (1/2/3): \").strip() or \"1\"\n",
        "    except:\n",
        "        choice = \"1\"\n",
        "\n",
        "    if choice == \"1\":\n",
        "        # === æ·»åŠ çš„éŸ³é¢‘ä¸Šä¼ ä»£ç å¼€å§‹ ===\n",
        "        print(\"\\nè¯·ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ (æ”¯æŒ.mp3/.wav/.m4a/.opus):\")\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            logger.log_step(\"ä¸Šä¼ éŸ³é¢‘\", \"failed\", \"æœªä¸Šä¼ ä»»ä½•æ–‡ä»¶\")\n",
        "            print(\"âš ï¸ æœªä¸Šä¼ æ–‡ä»¶ï¼Œè¯·é‡æ–°é€‰æ‹©è¾“å…¥æ–¹å¼\")\n",
        "            return handle_transcript_input()\n",
        "\n",
        "        filename = next(iter(uploaded.keys()))\n",
        "        audio_ext = os.path.splitext(filename)[1].lower()\n",
        "        if audio_ext not in ['.mp3', '.wav', '.m4a', '.opus']:\n",
        "            logger.log_step(\"ä¸Šä¼ éŸ³é¢‘\", \"failed\", f\"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {filename}\")\n",
        "            print(f\"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {filename} (è¯·ä¸Šä¼ .mp3/.wav/.m4a/.opus)\")\n",
        "            return handle_transcript_input()\n",
        "\n",
        "        # ä¿å­˜ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶\n",
        "        audio_path = f\"/tmp/{filename}\"\n",
        "        with open(audio_path, 'wb') as f:\n",
        "            f.write(uploaded[filename])\n",
        "        logger.log_step(\"ä¸Šä¼ éŸ³é¢‘\", \"success\", {\"æ–‡ä»¶å\": filename, \"è·¯å¾„\": audio_path})\n",
        "        print(f\"âœ… å·²ä¸Šä¼ éŸ³é¢‘: {filename}\")\n",
        "        # === æ·»åŠ çš„éŸ³é¢‘ä¸Šä¼ ä»£ç ç»“æŸ ===\n",
        "\n",
        "        # å¢å¼ºæ¨¡å‹é€‰æ‹©\n",
        "        print(\"\\nâš¡ é€‰æ‹©è½¬å½•æ¨¡å‹ (ä½¿ç”¨faster-whisper):\")\n",
        "        print(\"1: æé€Ÿ (tiny, æœ€å¿«ä½†ç²¾åº¦è¾ƒä½)\")\n",
        "        print(\"2: å¿«é€Ÿ (base, æ¨èæ—¥å¸¸ä½¿ç”¨)\")\n",
        "        print(\"3: å¹³è¡¡ (small, é€Ÿåº¦å’Œç²¾åº¦å¹³è¡¡)\")\n",
        "        print(\"4: é«˜ç²¾åº¦ (medium, ä¼šè®®è®°å½•æ¨è)\")\n",
        "        print(\"5: ä¸“ä¸šçº§ (large, æœ€é«˜ç²¾åº¦)\")\n",
        "\n",
        "        try:\n",
        "            model_choice = input(\"è¯·é€‰æ‹©æ¨¡å‹ (1-5): \").strip() or \"4\"\n",
        "        except:\n",
        "            model_choice = \"4\"\n",
        "\n",
        "        model_map = {\n",
        "            \"1\": \"tiny\",\n",
        "            \"2\": \"base\",\n",
        "            \"3\": \"small\",\n",
        "            \"4\": \"medium\",\n",
        "            \"5\": \"large\"\n",
        "        }\n",
        "\n",
        "        model_size = model_map.get(model_choice, \"medium\")\n",
        "        print(f\"ä½¿ç”¨æ¨¡å‹: {model_size}\")\n",
        "\n",
        "        # è·å–éŸ³é¢‘æ—¶é•¿\n",
        "        duration = get_audio_duration(audio_path)\n",
        "        print(f\"éŸ³é¢‘æ—¶é•¿: {duration:.1f}ç§’ï¼Œå¼€å§‹è½¬å½•...\")\n",
        "\n",
        "        # æ·»åŠ æ€§èƒ½æç¤º\n",
        "        if duration > 600:  # è¶…è¿‡10åˆ†é’Ÿ\n",
        "            print(\"â³ è¾ƒé•¿éŸ³é¢‘å¤„ç†ä¸­... è¯·è€å¿ƒç­‰å¾… (å¯ä½¿ç”¨Colab Proè·å¾—GPUåŠ é€Ÿ)\")\n",
        "        elif model_size in [\"medium\", \"large\"]:\n",
        "            print(\"ğŸ” ä½¿ç”¨é«˜ç²¾åº¦æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´...\")\n",
        "\n",
        "        transcript, detected_lang = transcribe_audio(audio_path, model_size)\n",
        "        os.unlink(audio_path)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶\n",
        "\n",
        "        print(f\"âœ… è½¬å½•å®Œæˆ (è¯­è¨€: {detected_lang})\")\n",
        "        return transcript, detected_lang\n",
        "\n",
        "    # ... [å…¶ä»–è¾“å…¥æ–¹å¼çš„ä»£ç ä¿æŒä¸å˜] ...\n",
        "\n",
        "    elif choice == \"2\":\n",
        "        # æ–‡æœ¬æ–‡ä»¶\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            logger.log_step(\"ä¸Šä¼ æ–‡æœ¬\", \"failed\", \"æœªä¸Šä¼ ä»»ä½•æ–‡ä»¶\")\n",
        "            print(\"âš ï¸ æœªä¸Šä¼ æ–‡ä»¶ï¼Œåˆ‡æ¢åˆ°ç›´æ¥ç²˜è´´\")\n",
        "            return handle_transcript_input()\n",
        "\n",
        "        filename = next(iter(uploaded.keys()))\n",
        "        logger.log_step(\"ä¸Šä¼ æ–‡æœ¬\", \"success\", {\"æ–‡ä»¶å\": filename})\n",
        "        print(f\"âœ… å·²ä¸Šä¼ æ–‡ä»¶: {filename}\")\n",
        "\n",
        "        try:\n",
        "            if filename.endswith('.txt'):\n",
        "                transcript = uploaded[filename].decode('utf-8')\n",
        "            elif filename.endswith('.docx'):\n",
        "                with tempfile.NamedTemporaryFile(delete=False, suffix='.docx') as tmp:\n",
        "                    tmp.write(uploaded[filename])\n",
        "                    doc = Document(tmp.name)\n",
        "                    transcript = \"\\n\".join(p.text for p in doc.paragraphs)\n",
        "                    os.unlink(tmp.name)\n",
        "            else:\n",
        "                raise ValueError(f\"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {filename} (æ”¯æŒ: .txt, .docx)\")\n",
        "\n",
        "            # æ£€æµ‹è¯­è¨€\n",
        "            lang = detect(transcript[:500]) if transcript else 'zh'\n",
        "            logger.log_step(\"æ£€æµ‹è¯­è¨€\", \"success\", {\"è¯­è¨€\": lang})\n",
        "            print(f\"âœ… è¯»å–å®Œæˆ (æ£€æµ‹è¯­è¨€: {lang})\")\n",
        "            return transcript, lang\n",
        "        except Exception as e:\n",
        "            logger.log_step(\"å¤„ç†æ–‡æœ¬æ–‡ä»¶\", \"failed\", error=str(e))\n",
        "            print(f\"âŒ å¤„ç†æ–‡ä»¶å‡ºé”™: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    elif choice == \"3\":\n",
        "        # ç›´æ¥ç²˜è´´\n",
        "        print(\"\\nè¯·ç²˜è´´ä¼šè®®è®°å½• (ç²˜è´´åæŒ‰Enterï¼Œè¾“å…¥ç©ºè¡Œç»“æŸ):\")\n",
        "        lines = []\n",
        "        while True:\n",
        "            line = input()\n",
        "            if not line:\n",
        "                break\n",
        "            lines.append(line)\n",
        "        transcript = \"\\n\".join(lines)\n",
        "\n",
        "        if not transcript.strip():\n",
        "            logger.log_step(\"è¾“å…¥æ–‡æœ¬\", \"failed\", \"æœªè¾“å…¥ä»»ä½•å†…å®¹\")\n",
        "            print(\"âš ï¸ æœªè¾“å…¥ä»»ä½•å†…å®¹ï¼Œé‡æ–°é€‰æ‹©è¾“å…¥æ–¹å¼\")\n",
        "            return handle_transcript_input()\n",
        "\n",
        "        # æ£€æµ‹è¯­è¨€\n",
        "        try:\n",
        "            lang = detect(transcript[:500])\n",
        "            logger.log_step(\"æ£€æµ‹è¯­è¨€\", \"success\", {\"è¯­è¨€\": lang})\n",
        "            print(f\"âœ… å·²è¾“å…¥æ–‡æœ¬ (æ£€æµ‹è¯­è¨€: {lang})\")\n",
        "        except:\n",
        "            lang = 'zh'\n",
        "            logger.log_step(\"æ£€æµ‹è¯­è¨€\", \"warning\", \"ä½¿ç”¨é»˜è®¤è¯­è¨€ä¸­æ–‡\")\n",
        "            print(f\"âœ… å·²è¾“å…¥æ–‡æœ¬ (ä½¿ç”¨é»˜è®¤è¯­è¨€: ä¸­æ–‡)\")\n",
        "\n",
        "        return transcript, lang\n",
        "\n",
        "    else:\n",
        "        logger.log_step(\"é€‰æ‹©è¾“å…¥æ–¹å¼\", \"warning\", \"æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤éŸ³é¢‘è¾“å…¥\")\n",
        "        print(\"âš ï¸ æ— æ•ˆé€‰æ‹©ï¼Œé»˜è®¤ä½¿ç”¨éŸ³é¢‘è¾“å…¥\")\n",
        "        return handle_transcript_input()\n",
        "\n",
        "# ======================\n",
        "# ä¸»å‡½æ•°\n",
        "# ======================\n",
        "def main():\n",
        "    logger.log_step(\"å·¥ä½œæµç¨‹\", \"started\")\n",
        "    print(\"=== ä¼šè®®è®°å½•å¤„ç†å·¥å…· ===\")\n",
        "\n",
        "    try:\n",
        "        if not test_notion_permissions():\n",
        "            print(\"\\nâŒ æƒé™æµ‹è¯•æœªé€šè¿‡ï¼Œè¯·å…ˆè§£å†³ä¸Šè¿°é—®é¢˜\")\n",
        "            log_file = logger.save_logs(\"error_logs.json\")\n",
        "            print(f\"é”™è¯¯æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}\")\n",
        "            return\n",
        "\n",
        "        transcript, language = handle_transcript_input()\n",
        "        logger.log_metric(\"è½¬å½•æ–‡æœ¬é•¿åº¦\", len(transcript))\n",
        "\n",
        "        meeting_data = analyze_meeting(transcript, language)\n",
        "        if \"error\" in meeting_data:\n",
        "            raise RuntimeError(f\"åˆ†æå¤±è´¥: {meeting_data['error']}\")\n",
        "\n",
        "        print(\"\\nå¼€å§‹åˆ›å»ºNotionæŠ¥å‘Š...\")\n",
        "        report_url = create_notion_report_page(meeting_data, transcript, logger.logs)\n",
        "\n",
        "        if not report_url:\n",
        "            raise RuntimeError(\"åˆ›å»ºNotionæŠ¥å‘Šå¤±è´¥\")\n",
        "\n",
        "        log_file = logger.save_logs()\n",
        "        print(f\"\\nğŸ‰ å¤„ç†å®Œæˆï¼\")\n",
        "        print(f\"ğŸ“„ ä¼šè®®æŠ¥å‘ŠURL: {report_url}\")\n",
        "        print(f\"ğŸ“‹ æ—¥å¿—æ–‡ä»¶: {log_file}\")\n",
        "\n",
        "        from IPython.display import HTML\n",
        "        display(HTML(f'<a href=\"{report_url}\" target=\"_blank\">ç‚¹å‡»æ‰“å¼€NotionæŠ¥å‘Š</a>'))\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"å·¥ä½œæµç¨‹\", \"failed\", error=str(e))\n",
        "        log_file = logger.save_logs(\"error_logs.json\")\n",
        "        print(f\"\\nâŒ å¤„ç†å¤±è´¥ï¼\")\n",
        "        print(f\"é”™è¯¯è¯¦æƒ…: {str(e)}\")\n",
        "        print(f\"é”™è¯¯æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8ADRLMjnOY6e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# å®‰è£…å¿…è¦ä¾èµ–ï¼ˆæŒ‡å®šå…¼å®¹ç‰ˆæœ¬ï¼‰\n",
        "!pip uninstall -y whisper\n",
        "!pip install faster-whisper==0.10.0  # é”å®šç‰ˆæœ¬ä»¥é¿å…APIå˜æ›´\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install tqdm python-docx notion-client langdetect langchain==0.1.13 langchain-openai==0.0.8 pydantic==2.5.2 httpx==0.27.0\n",
        "!sudo apt update && sudo apt install ffmpeg -y\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import torch\n",
        "import subprocess\n",
        "import datetime\n",
        "import tempfile\n",
        "from tqdm import tqdm\n",
        "from pydantic import BaseModel, Field\n",
        "import httpx\n",
        "import concurrent.futures\n",
        "from functools import partial\n",
        "\n",
        "# å¯¼å…¥ç¬¬ä¸‰æ–¹åº“\n",
        "from google.colab import files, userdata\n",
        "from notion_client import Client, errors\n",
        "from langdetect import detect, LangDetectException\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# æ¸…é™¤ä»£ç†ç¯å¢ƒå˜é‡ï¼ˆé¿å…ç½‘ç»œè¿æ¥é—®é¢˜ï¼‰\n",
        "for var in ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy']:\n",
        "    if var in os.environ:\n",
        "        del os.environ[var]\n",
        "\n",
        "# åˆå§‹åŒ–Notionå®¢æˆ·ç«¯ï¼ˆä¿®å¤httpxä»£ç†å‚æ•°é—®é¢˜ï¼‰\n",
        "http_client = httpx.Client()\n",
        "http_client.proxies = None  # ç¦ç”¨ä»£ç†\n",
        "\n",
        "notion = Client(\n",
        "    auth=userdata.get('NOTION_TOKEN'),\n",
        "    client=http_client\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# é…ç½®å‚æ•°ä¸åˆå§‹åŒ–\n",
        "# ======================\n",
        "try:\n",
        "    # ä»ç¯å¢ƒå˜é‡è·å–å¯†é’¥\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    NOTION_TOKEN = userdata.get('NOTION_TOKEN')\n",
        "    NOTION_PAGE_ID = userdata.get('NOTION_PAGE_ID')\n",
        "\n",
        "    # é•¿éŸ³é¢‘å¤„ç†å‚æ•°\n",
        "    CHUNK_DURATION = 900  # æ¯å—15åˆ†é’Ÿï¼ˆç§’ï¼‰\n",
        "    OVERLAP_DURATION = 30  # å—é—´é‡å 30ç§’\n",
        "    MAX_CONCURRENT_CHUNKS = 1  # å•çº¿ç¨‹å¤„ç†ï¼Œé¿å…æ–‡ä»¶ç«äº‰\n",
        "    # Notionå—æ•°é‡é™åˆ¶ç›¸å…³å‚æ•°\n",
        "    MAX_TRANSCRIPT_SEGMENTS = 50  # æœ€å¤šæ˜¾ç¤º50æ¡è½¬å½•æ–‡æœ¬ï¼ˆé¿å…å—æ•°é‡è¶…é™ï¼‰\n",
        "\n",
        "    # éªŒè¯å¿…è¦å¯†é’¥\n",
        "    missing_creds = []\n",
        "    if not OPENAI_API_KEY:\n",
        "        missing_creds.append(\"OPENAI_API_KEY\")\n",
        "    if not NOTION_TOKEN:\n",
        "        missing_creds.append(\"NOTION_TOKEN\")\n",
        "    if not NOTION_PAGE_ID:\n",
        "        missing_creds.append(\"NOTION_PAGE_ID\")\n",
        "\n",
        "    if missing_creds:\n",
        "        raise ValueError(f\"ç¼ºå°‘å¿…è¦å‡­è¯: {', '.join(missing_creds)}\")\n",
        "\n",
        "    print(\"âœ… æ‰€æœ‰å‡­è¯å·²é…ç½®å®Œæˆï¼Œå‡†å¤‡å¤„ç†ä¼šè®®å†…å®¹\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ åˆå§‹åŒ–å¤±è´¥: {str(e)}\")\n",
        "    print(\"\\nè®¾ç½®æŒ‡å—:\")\n",
        "    print(\"1. ç‚¹å‡»å·¦ä¾§è¾¹æ çš„é’¥åŒ™å›¾æ ‡ï¼ˆColab Secretsï¼‰\")\n",
        "    print(\"2. æ·»åŠ ä»¥ä¸‹å¯†é’¥:\")\n",
        "    print(\"   - OPENAI_API_KEY: ä½ çš„OpenAI APIå¯†é’¥\")\n",
        "    print(\"   - NOTION_TOKEN: Notioné›†æˆä»¤ç‰Œ\")\n",
        "    print(\"   - NOTION_PAGE_ID: ç”¨äºå­˜å‚¨æŠ¥å‘Šçš„Notioné¡µé¢ID\")\n",
        "    raise\n",
        "\n",
        "# ======================\n",
        "# æ—¥å¿—è®°å½•ç³»ç»Ÿ\n",
        "# ======================\n",
        "class MeetingLogger:\n",
        "    def __init__(self):\n",
        "        self.logs = {\n",
        "            \"start_time\": datetime.datetime.now().isoformat(),\n",
        "            \"steps\": [],\n",
        "            \"chunk_status\": {}\n",
        "        }\n",
        "\n",
        "    def log_step(self, step_name, status, details=None, error=None):\n",
        "        \"\"\"è®°å½•å¤„ç†æ­¥éª¤\"\"\"\n",
        "        entry = {\n",
        "            \"step\": step_name,\n",
        "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "            \"status\": status\n",
        "        }\n",
        "        if details:\n",
        "            entry[\"details\"] = details\n",
        "        if error:\n",
        "            entry[\"error\"] = str(error)\n",
        "        self.logs[\"steps\"].append(entry)\n",
        "\n",
        "    def log_chunk(self, chunk_id, status, error=None):\n",
        "        \"\"\"è®°å½•åˆ†å—å¤„ç†çŠ¶æ€\"\"\"\n",
        "        self.logs[\"chunk_status\"][chunk_id] = {\n",
        "            \"status\": status,\n",
        "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "            \"error\": str(error) if error else None\n",
        "        }\n",
        "\n",
        "    def get_completed_chunks(self):\n",
        "        \"\"\"è·å–æˆåŠŸçš„åˆ†å—ID\"\"\"\n",
        "        return [k for k, v in self.logs[\"chunk_status\"].items() if v[\"status\"] == \"success\"]\n",
        "\n",
        "    def get_failed_chunks(self):\n",
        "        \"\"\"è·å–å¤±è´¥çš„åˆ†å—ID\"\"\"\n",
        "        return [k for k, v in self.logs[\"chunk_status\"].items() if v[\"status\"] == \"failed\"]\n",
        "\n",
        "    def save_logs(self, filename=\"meeting_logs.json\"):\n",
        "        \"\"\"ä¿å­˜æ—¥å¿—åˆ°æ–‡ä»¶\"\"\"\n",
        "        with open(filename, \"w\") as f:\n",
        "            json.dump(self.logs, f, indent=2)\n",
        "        return filename\n",
        "\n",
        "# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ\n",
        "logger = MeetingLogger()\n",
        "\n",
        "# ======================\n",
        "# éŸ³é¢‘å¤„ç†å·¥å…·\n",
        "# ======================\n",
        "def get_audio_duration(audio_path):\n",
        "    \"\"\"è·å–éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"ffmpeg\", \"-i\", audio_path],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            text=True\n",
        "        )\n",
        "        output = result.stdout\n",
        "\n",
        "        duration_match = re.search(r\"Duration: (\\d+:\\d+:\\d+\\.\\d+)\", output)\n",
        "        if not duration_match:\n",
        "            return 0.0\n",
        "\n",
        "        duration_str = duration_match.group(1)\n",
        "        h, m, s = duration_str.split(':')\n",
        "        return float(h) * 3600 + float(m) * 60 + float(s)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"è·å–éŸ³é¢‘æ—¶é•¿\", \"warning\", error=str(e))\n",
        "        return 0.0\n",
        "\n",
        "def split_audio_into_chunks(audio_path):\n",
        "    \"\"\"å°†é•¿éŸ³é¢‘åˆ†å‰²ä¸ºå¸¦é‡å çš„å—\"\"\"\n",
        "    logger.log_step(\"éŸ³é¢‘åˆ†å—\", \"started\")\n",
        "\n",
        "    try:\n",
        "        total_duration = get_audio_duration(audio_path)\n",
        "        if total_duration <= 0:\n",
        "            raise ValueError(\"æ— æ³•è·å–æœ‰æ•ˆéŸ³é¢‘æ—¶é•¿ï¼Œå¯èƒ½æ–‡ä»¶æŸå\")\n",
        "\n",
        "        # è®¡ç®—åˆ†å—æ•°é‡\n",
        "        num_chunks = max(1, int((total_duration + CHUNK_DURATION - OVERLAP_DURATION) //\n",
        "                              (CHUNK_DURATION - OVERLAP_DURATION)))\n",
        "        logger.log_step(\"è®¡ç®—åˆ†å—æ•°é‡\", \"success\", {\"æ€»æ—¶é•¿(åˆ†é’Ÿ)\": f\"{total_duration/60:.1f}\", \"åˆ†å—æ•°\": num_chunks})\n",
        "        print(f\"ğŸ“Š éŸ³é¢‘å°†åˆ†å‰²ä¸º {num_chunks} å—ï¼ˆæ¯å—15åˆ†é’Ÿï¼Œé‡å 30ç§’ï¼‰\")\n",
        "\n",
        "        # åˆ›å»ºä¸´æ—¶ç›®å½•å­˜å‚¨åˆ†å—\n",
        "        chunk_dir = tempfile.mkdtemp()\n",
        "        chunks = []\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_time = i * (CHUNK_DURATION - OVERLAP_DURATION)\n",
        "            end_time = min(start_time + CHUNK_DURATION, total_duration)\n",
        "\n",
        "            # æ ¼å¼åŒ–ä¸ºffmpegæ—¶é—´æ ¼å¼\n",
        "            start_str = str(datetime.timedelta(seconds=start_time))\n",
        "            duration_str = str(datetime.timedelta(seconds=end_time - start_time))\n",
        "\n",
        "            chunk_path = f\"{chunk_dir}/chunk_{i:03d}.wav\"\n",
        "\n",
        "            # ä½¿ç”¨ffmpegåˆ‡å‰²éŸ³é¢‘\n",
        "            subprocess.run(\n",
        "                [\n",
        "                    \"ffmpeg\", \"-y\",  # è¦†ç›–ç°æœ‰æ–‡ä»¶\n",
        "                    \"-i\", audio_path,\n",
        "                    \"-ss\", start_str,  # èµ·å§‹æ—¶é—´\n",
        "                    \"-t\", duration_str,  # æŒç»­æ—¶é—´\n",
        "                    \"-ar\", \"16000\",  # ç»Ÿä¸€é‡‡æ ·ç‡\n",
        "                    \"-ac\", \"1\",  # å•å£°é“\n",
        "                    \"-acodec\", \"pcm_s16le\",  # æ— æŸç¼–ç \n",
        "                    chunk_path\n",
        "                ],\n",
        "                check=True,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE\n",
        "            )\n",
        "\n",
        "            # éªŒè¯åˆ†å—æ–‡ä»¶\n",
        "            if not os.path.exists(chunk_path) or os.path.getsize(chunk_path) < 1024:\n",
        "                raise RuntimeError(f\"åˆ†å— {i} ç”Ÿæˆå¤±è´¥ï¼Œæ–‡ä»¶å¤§å°å¼‚å¸¸\")\n",
        "\n",
        "            chunks.append({\n",
        "                \"id\": i,\n",
        "                \"path\": chunk_path,\n",
        "                \"start_time\": start_time,\n",
        "                \"end_time\": end_time,\n",
        "                \"dir\": chunk_dir  # ä¿ç•™ç›®å½•å¼•ç”¨ï¼Œé¿å…æå‰åˆ é™¤\n",
        "            })\n",
        "\n",
        "        logger.log_step(\"éŸ³é¢‘åˆ†å—\", \"success\")\n",
        "        return chunks\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"éŸ³é¢‘åˆ†å—\", \"failed\", error=str(e))\n",
        "        raise RuntimeError(f\"éŸ³é¢‘åˆ†å—å¤±è´¥: {str(e)}\")\n",
        "\n",
        "# ======================\n",
        "# è¯­éŸ³è½¬å½•ï¼ˆæ ¸å¿ƒä¿®å¤éƒ¨åˆ†ï¼‰\n",
        "# ======================\n",
        "# å…¨å±€æ¨¡å‹å˜é‡ï¼ˆç¡®ä¿å•ä¾‹ï¼‰\n",
        "global_model = None\n",
        "\n",
        "def transcribe_chunk(chunk):\n",
        "    \"\"\"è½¬å½•å•ä¸ªéŸ³é¢‘å—ï¼ˆä¿®å¤Segmentå±æ€§é—®é¢˜ï¼‰\"\"\"\n",
        "    chunk_id = chunk[\"id\"]\n",
        "    chunk_path = chunk[\"path\"]\n",
        "    start_time = chunk[\"start_time\"]\n",
        "\n",
        "    try:\n",
        "        from faster_whisper import WhisperModel\n",
        "        global global_model\n",
        "\n",
        "        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰\n",
        "        if global_model is None:\n",
        "            if not torch.cuda.is_available():\n",
        "                raise RuntimeError(\"è¯·åˆ‡æ¢åˆ°GPUç¯å¢ƒï¼ˆRuntime > Change runtime typeï¼‰\")\n",
        "\n",
        "            print(f\"ğŸ”§ åŠ è½½ {chunk['model_size']} æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œéœ€ä¸‹è½½çº¦1.5GBï¼‰...\")\n",
        "            global_model = WhisperModel(\n",
        "                chunk[\"model_size\"],\n",
        "                device=\"cuda\",\n",
        "                compute_type=\"float16\",\n",
        "                download_root=\"/content/models\"\n",
        "            )\n",
        "            print(f\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ\")\n",
        "\n",
        "        # éªŒè¯æ–‡ä»¶å¯è®¿é—®\n",
        "        if not os.path.exists(chunk_path):\n",
        "            raise FileNotFoundError(f\"åˆ†å—æ–‡ä»¶ä¸å­˜åœ¨: {chunk_path}\")\n",
        "\n",
        "        # æ‰§è¡Œè½¬å½•ï¼ˆä¸ä¾èµ–confidenceå±æ€§ï¼‰\n",
        "        segments, info = global_model.transcribe(\n",
        "            chunk_path,\n",
        "            beam_size=2,\n",
        "            vad_filter=True,  # å¯ç”¨è¯­éŸ³æ´»åŠ¨æ£€æµ‹\n",
        "            vad_parameters=dict(min_silence_duration_ms=300),\n",
        "            language=None  # è‡ªåŠ¨æ£€æµ‹è¯­è¨€\n",
        "        )\n",
        "\n",
        "        # å¤„ç†è½¬å½•ç»“æœï¼ˆç§»é™¤confidenceå±æ€§ï¼‰\n",
        "        timestamped_segments = []\n",
        "        for seg in segments:\n",
        "            timestamped_segments.append({\n",
        "                \"text\": seg.text.strip(),\n",
        "                \"start\": seg.start + start_time,  # è½¬æ¢ä¸ºå…¨å±€æ—¶é—´\n",
        "                \"end\": seg.end + start_time\n",
        "                # ç§»é™¤confidenceå±æ€§ï¼Œå…¼å®¹æ–°ç‰ˆæœ¬API\n",
        "            })\n",
        "\n",
        "        logger.log_chunk(chunk_id, \"success\")\n",
        "        return {\n",
        "            \"chunk_id\": chunk_id,\n",
        "            \"segments\": timestamped_segments,\n",
        "            \"language\": info.language\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_chunk(chunk_id, \"failed\", error=str(e))\n",
        "        print(f\"âš ï¸ åˆ†å— {chunk_id} è½¬å½•å¤±è´¥: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def transcribe_long_audio(audio_path, model_size=\"small.en\"):\n",
        "    \"\"\"è½¬å½•é•¿éŸ³é¢‘ï¼ˆåˆ†å—å¤„ç†ï¼‰\"\"\"\n",
        "    logger.log_step(\"éŸ³é¢‘è½¬å½•\", \"started\", {\"æ¨¡å‹\": model_size})\n",
        "\n",
        "    try:\n",
        "        global global_model\n",
        "        global_model = None  # é‡ç½®æ¨¡å‹\n",
        "\n",
        "        # åˆ†å‰²éŸ³é¢‘ä¸ºå—\n",
        "        chunks = split_audio_into_chunks(audio_path)\n",
        "        num_chunks = len(chunks)\n",
        "\n",
        "        # ä¸ºæ¯ä¸ªåˆ†å—æ·»åŠ æ¨¡å‹å‚æ•°\n",
        "        for chunk in chunks:\n",
        "            chunk[\"model_size\"] = model_size\n",
        "\n",
        "        # è½¬å½•æ‰€æœ‰åˆ†å—ï¼ˆå•çº¿ç¨‹é¿å…æ–‡ä»¶ç«äº‰ï¼‰\n",
        "        print(f\"ğŸš€ å¼€å§‹è½¬å½• {num_chunks} ä¸ªåˆ†å—...\")\n",
        "        results = []\n",
        "        for chunk in tqdm(chunks, desc=\"è½¬å½•è¿›åº¦\"):\n",
        "            result = transcribe_chunk(chunk)\n",
        "            if result:\n",
        "                results.append(result)\n",
        "\n",
        "        # é‡è¯•å¤±è´¥çš„åˆ†å—\n",
        "        failed_ids = logger.get_failed_chunks()\n",
        "        if failed_ids:\n",
        "            print(f\"ğŸ”„ é‡è¯• {len(failed_ids)} ä¸ªå¤±è´¥åˆ†å—...\")\n",
        "            for chunk in chunks:\n",
        "                if chunk[\"id\"] in failed_ids:\n",
        "                    result = transcribe_chunk(chunk)\n",
        "                    if result:\n",
        "                        results.append(result)\n",
        "\n",
        "        # æŒ‰åˆ†å—IDæ’åºå¹¶åˆå¹¶ç»“æœ\n",
        "        results.sort(key=lambda x: x[\"chunk_id\"])\n",
        "        all_segments = []\n",
        "        for res in results:\n",
        "            all_segments.extend(res[\"segments\"])\n",
        "\n",
        "        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆæœ€åæ¸…ç†ï¼Œé¿å…è¯»å–å¤±è´¥ï¼‰\n",
        "        unique_dirs = list({chunk[\"dir\"] for chunk in chunks})\n",
        "        for dir_path in unique_dirs:\n",
        "            if os.path.exists(dir_path):\n",
        "                for f in os.listdir(dir_path):\n",
        "                    os.unlink(os.path.join(dir_path, f))\n",
        "                os.rmdir(dir_path)\n",
        "\n",
        "        # æ£€æµ‹è¯­è¨€\n",
        "        language = results[0][\"language\"] if results else \"en\"\n",
        "\n",
        "        # éªŒè¯è½¬å½•ç»“æœ\n",
        "        if not all_segments:\n",
        "            raise RuntimeError(\"æœªè·å–åˆ°æœ‰æ•ˆè½¬å½•å†…å®¹ï¼Œè¯·æ£€æŸ¥éŸ³é¢‘è´¨é‡\")\n",
        "\n",
        "        logger.log_step(\"éŸ³é¢‘è½¬å½•\", \"success\", {\"æ€»ç‰‡æ®µæ•°\": len(all_segments)})\n",
        "        print(f\"âœ… è½¬å½•å®Œæˆï¼ˆ{len(all_segments)}ä¸ªç‰‡æ®µï¼Œæ£€æµ‹è¯­è¨€: {language}ï¼‰\")\n",
        "        return \" \".join([s[\"text\"] for s in all_segments]), all_segments, language\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"éŸ³é¢‘è½¬å½•\", \"failed\", error=str(e))\n",
        "        raise RuntimeError(f\"è½¬å½•è¿‡ç¨‹å¤±è´¥: {str(e)}\")\n",
        "\n",
        "# ======================\n",
        "# ä¼šè®®å†…å®¹åˆ†æï¼ˆä¿®å¤dictå±æ€§é”™è¯¯ï¼‰\n",
        "# ======================\n",
        "# åˆ†æç»“æœæ•°æ®æ¨¡å‹\n",
        "class ChunkAnalysis(BaseModel):\n",
        "    summary: str = Field(description=\"è¯¥ç‰‡æ®µçš„æ€»ç»“ï¼ˆ100-200å­—ï¼‰\")\n",
        "    key_points: list[str] = Field(description=\"è¯¥ç‰‡æ®µçš„å…³é”®ç‚¹åˆ—è¡¨\")\n",
        "    action_items: list[dict] = Field(description=\"è¡ŒåŠ¨é¡¹åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«taskã€assigneeã€due_date\")\n",
        "    topics: list[str] = Field(description=\"è®¨è®ºçš„è¯é¢˜åˆ—è¡¨\")\n",
        "\n",
        "class FullMeetingAnalysis(BaseModel):\n",
        "    meeting_title: str = Field(description=\"ä¼šè®®æ ‡é¢˜\")\n",
        "    participants: list[str] = Field(description=\"å‚ä¸è€…åå•\")\n",
        "    summary: str = Field(description=\"3-5æ®µå®Œæ•´ä¼šè®®æ€»ç»“\")\n",
        "    key_points: dict = Field(description=\"æŒ‰è¯é¢˜åˆ†ç»„çš„å…¨å±€å…³é”®ç‚¹\")\n",
        "    action_items: list[dict] = Field(description=\"æ±‡æ€»çš„è¡ŒåŠ¨é¡¹\")\n",
        "    meeting_type: str = Field(description=\"ä¼šè®®ç±»å‹\")\n",
        "    topics_flow: list[str] = Field(description=\"ä¼šè®®è¯é¢˜æµè½¬é¡ºåº\")\n",
        "\n",
        "def get_chunk_analysis_chain(language):\n",
        "    \"\"\"åˆ›å»ºåˆ†å—åˆ†æé“¾ï¼ˆæ˜¾å¼ä¼ é€’APIå¯†é’¥ï¼‰\"\"\"\n",
        "    parser = JsonOutputParser(pydantic_object=ChunkAnalysis)\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"åˆ†æä»¥ä¸‹ä¼šè®®ç‰‡æ®µï¼Œæå–å…³é”®ä¿¡æ¯ï¼ˆç”¨{language}ï¼‰ï¼š\\n{format_instructions}\\nä¼šè®®ç‰‡æ®µï¼š{transcript}\",\n",
        "        input_variables=[\"transcript\"],\n",
        "        partial_variables={\n",
        "            \"language\": \"ä¸­æ–‡\" if language.startswith('zh') else \"English\",\n",
        "            \"format_instructions\": parser.get_format_instructions()\n",
        "        }\n",
        "    )\n",
        "    # å…³é”®ä¿®å¤ï¼šæ˜¾å¼ä¼ å…¥APIå¯†é’¥\n",
        "    return prompt | ChatOpenAI(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        temperature=0.3,\n",
        "        openai_api_key=OPENAI_API_KEY  # ç›´æ¥ä½¿ç”¨å…¨å±€å˜é‡ä¸­çš„å¯†é’¥\n",
        "    ) | parser\n",
        "\n",
        "def get_full_analysis_chain(language):\n",
        "    \"\"\"åˆ›å»ºå…¨å±€åˆ†æé“¾ï¼ˆæ˜¾å¼ä¼ é€’APIå¯†é’¥ï¼‰\"\"\"\n",
        "    parser = JsonOutputParser(pydantic_object=FullMeetingAnalysis)\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"åŸºäºä»¥ä¸‹å„ç‰‡æ®µåˆ†æï¼Œç”Ÿæˆå®Œæ•´ä¼šè®®æŠ¥å‘Šï¼ˆç”¨{language}ï¼‰ï¼š\\n{format_instructions}\\nç‰‡æ®µåˆ†æï¼š{chunk_analyses}\",\n",
        "        input_variables=[\"chunk_analyses\"],\n",
        "        partial_variables={\n",
        "            \"language\": \"ä¸­æ–‡\" if language.startswith('zh') else \"English\",\n",
        "            \"format_instructions\": parser.get_format_instructions()\n",
        "        }\n",
        "    )\n",
        "    # å…³é”®ä¿®å¤ï¼šæ˜¾å¼ä¼ å…¥APIå¯†é’¥\n",
        "    return prompt | ChatOpenAI(\n",
        "        model=\"gpt-4\",\n",
        "        temperature=0.3,\n",
        "        openai_api_key=OPENAI_API_KEY  # ç›´æ¥ä½¿ç”¨å…¨å±€å˜é‡ä¸­çš„å¯†é’¥\n",
        "    ) | parser\n",
        "\n",
        "def analyze_meeting(transcript_segments, language='en'):\n",
        "    \"\"\"åˆ†æä¼šè®®å†…å®¹ï¼ˆåˆ†å—åˆ†æ+å…¨å±€æ•´åˆï¼‰\"\"\"\n",
        "    logger.log_step(\"ä¼šè®®åˆ†æ\", \"started\")\n",
        "    print(\"\\nå¼€å§‹åˆ†æä¼šè®®å†…å®¹...\")\n",
        "\n",
        "    try:\n",
        "        # æŒ‰æ—¶é—´åˆ†å‰²ä¸ºåˆ†æå—ï¼ˆ45åˆ†é’Ÿ/å—ï¼‰\n",
        "        ANALYSIS_CHUNK_DURATION = 2700\n",
        "        analysis_chunks = []\n",
        "        current_chunk = []\n",
        "\n",
        "        for seg in transcript_segments:\n",
        "            if not current_chunk:\n",
        "                current_chunk.append(seg)\n",
        "            else:\n",
        "                if seg[\"end\"] - current_chunk[0][\"start\"] <= ANALYSIS_CHUNK_DURATION:\n",
        "                    current_chunk.append(seg)\n",
        "                else:\n",
        "                    analysis_chunks.append(current_chunk)\n",
        "                    current_chunk = [seg]\n",
        "        if current_chunk:\n",
        "            analysis_chunks.append(current_chunk)\n",
        "\n",
        "        print(f\"ğŸ“ å°†ä¼šè®®å†…å®¹åˆ†ä¸º {len(analysis_chunks)} ä¸ªåˆ†æå—\")\n",
        "\n",
        "        # åˆ†å—åˆ†æ\n",
        "        chunk_analyses = []\n",
        "        chunk_chain = get_chunk_analysis_chain(language)\n",
        "\n",
        "        for i, chunk in enumerate(tqdm(analysis_chunks, desc=\"åˆ†æè¿›åº¦\")):\n",
        "            # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„å—æ–‡æœ¬\n",
        "            chunk_text = \"\\n\".join([\n",
        "                f\"[{str(datetime.timedelta(seconds=int(seg['start'])))}] {seg['text']}\"\n",
        "                for seg in chunk\n",
        "            ])\n",
        "\n",
        "            try:\n",
        "                analysis = chunk_chain.invoke({\"transcript\": chunk_text[:12000]})  # é™åˆ¶é•¿åº¦\n",
        "                chunk_analyses.append({\n",
        "                    \"chunk_id\": i,\n",
        "                    \"start_time\": chunk[0][\"start\"],\n",
        "                    \"end_time\": chunk[-1][\"end\"],\n",
        "                    \"analysis\": analysis\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ åˆ†æå— {i} å¤±è´¥: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        if not chunk_analyses:\n",
        "            raise RuntimeError(\"æ‰€æœ‰åˆ†æå—å¤„ç†å¤±è´¥ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š\")\n",
        "\n",
        "        # å…¨å±€æ•´åˆåˆ†æ - ä¿®å¤ï¼šç§»é™¤.dict()è°ƒç”¨ï¼Œå› ä¸ºåˆ†æç»“æœå·²ç»æ˜¯å­—å…¸\n",
        "        full_chain = get_full_analysis_chain(language)\n",
        "        full_analysis = full_chain.invoke({\n",
        "            \"chunk_analyses\": json.dumps([\n",
        "                {\n",
        "                    \"æ—¶é—´æ®µ\": f\"{str(datetime.timedelta(seconds=int(c['start_time'])))} - {str(datetime.timedelta(seconds=int(c['end_time'])))}\",\n",
        "                    \"åˆ†æ\": c[\"analysis\"]  # å…³é”®ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨å­—å…¸å¯¹è±¡\n",
        "                } for c in chunk_analyses\n",
        "            ], ensure_ascii=False)\n",
        "        })\n",
        "\n",
        "        # æ·»åŠ å…ƒæ•°æ® - ä¿®å¤ï¼šå¦‚æœfull_analysisæ˜¯Pydanticæ¨¡å‹ï¼Œå…ˆè½¬å­—å…¸\n",
        "        if hasattr(full_analysis, 'dict'):\n",
        "            full_analysis = full_analysis.dict()\n",
        "\n",
        "        full_analysis[\"language\"] = language\n",
        "        full_analysis[\"date\"] = datetime.datetime.now().isoformat()\n",
        "        full_analysis[\"total_duration\"] = f\"{(transcript_segments[-1]['end'] - transcript_segments[0]['start'])/3600:.2f}å°æ—¶\"\n",
        "\n",
        "        logger.log_step(\"ä¼šè®®åˆ†æ\", \"success\")\n",
        "        print(f\"âœ… ä¼šè®®åˆ†æå®Œæˆï¼ˆ{len(full_analysis['topics_flow'])}ä¸ªè¯é¢˜ï¼Œ{len(full_analysis['action_items'])}ä¸ªè¡ŒåŠ¨é¡¹ï¼‰\")\n",
        "        return full_analysis\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"ä¼šè®®åˆ†æå¤±è´¥: {str(e)}\"\n",
        "        logger.log_step(\"ä¼šè®®åˆ†æ\", \"failed\", error=error_msg)\n",
        "        print(f\"âŒ {error_msg}\")\n",
        "        return {\"error\": error_msg}\n",
        "\n",
        "# ======================\n",
        "# NotionæŠ¥å‘Šç”Ÿæˆï¼ˆä¿®å¤å—æ•°é‡è¶…é™é—®é¢˜ï¼‰\n",
        "# ======================\n",
        "def create_notion_report(meeting_data, transcript_segments):\n",
        "    \"\"\"åœ¨Notionä¸­åˆ›å»ºä¼šè®®æŠ¥å‘Šï¼ˆæ§åˆ¶å—æ•°é‡â‰¤100ï¼‰\"\"\"\n",
        "    logger.log_step(\"åˆ›å»ºNotionæŠ¥å‘Š\", \"started\")\n",
        "\n",
        "    try:\n",
        "        # éªŒè¯çˆ¶é¡µé¢æ˜¯å¦å­˜åœ¨\n",
        "        try:\n",
        "            parent_page = notion.pages.retrieve(NOTION_PAGE_ID)\n",
        "            parent_title = parent_page.get('properties', {}).get('title', {}).get('title', [{}])[0].get('plain_text', 'æ— æ ‡é¢˜é¡µé¢')\n",
        "            print(f\"âœ… æˆåŠŸè®¿é—®çˆ¶é¡µé¢: {parent_title}\")\n",
        "        except errors.APIResponseError as e:\n",
        "            raise RuntimeError(f\"Notionçˆ¶é¡µé¢è®¿é—®å¤±è´¥: {str(e)}\")\n",
        "\n",
        "        # åˆ›å»ºæ–°æŠ¥å‘Šé¡µé¢\n",
        "        new_page = notion.pages.create(\n",
        "            parent={\"page_id\": NOTION_PAGE_ID},\n",
        "            properties={\n",
        "                \"title\": {\n",
        "                    \"title\": [{\"text\": {\"content\": meeting_data.get(\"meeting_title\", \"ä¼šè®®æŠ¥å‘Š\")[:200]}}]\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "        page_id = new_page[\"id\"]\n",
        "\n",
        "        # æ„å»ºæŠ¥å‘Šå†…å®¹å—ï¼ˆæ§åˆ¶æ€»æ•°é‡â‰¤100ï¼‰\n",
        "        children_blocks = []\n",
        "\n",
        "        # 1. ä¼šè®®æ¦‚è§ˆï¼ˆçº¦2ä¸ªå—ï¼‰\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"ä¼šè®®æ¦‚è§ˆ\"}}]}\n",
        "        })\n",
        "        overview_text = f\"\"\"\n",
        "        æ ‡é¢˜: {meeting_data.get('meeting_title', 'æœªå‘½åä¼šè®®')}\n",
        "        æ—¥æœŸ: {meeting_data.get('date', datetime.datetime.now().strftime('%Y-%m-%d'))}\n",
        "        æ€»æ—¶é•¿: {meeting_data.get('total_duration', 'æœªçŸ¥')}\n",
        "        å‚ä¸è€…: {', '.join(meeting_data.get('participants', [])) or 'æœªè¯†åˆ«'}\n",
        "        è¯­è¨€: {meeting_data.get('language', 'æœªçŸ¥')}\n",
        "        \"\"\"\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"paragraph\",\n",
        "            \"paragraph\": {\"rich_text\": [{\"text\": {\"content\": overview_text.strip()}}]}\n",
        "        })\n",
        "\n",
        "        # 2. è¯é¢˜æµè½¬ï¼ˆè¯é¢˜æ•°é‡+1ä¸ªå—ï¼‰\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"è¯é¢˜æµè½¬é¡ºåº\"}}]}\n",
        "        })\n",
        "        # é™åˆ¶è¯é¢˜æ•°é‡ï¼ˆé¿å…å—è¿‡å¤šï¼‰\n",
        "        max_topics = 20  # æœ€å¤šæ˜¾ç¤º20ä¸ªè¯é¢˜\n",
        "        for topic in meeting_data.get('topics_flow', [])[:max_topics]:\n",
        "            children_blocks.append({\n",
        "                \"object\": \"block\",\n",
        "                \"type\": \"numbered_list_item\",\n",
        "                \"numbered_list_item\": {\"rich_text\": [{\"text\": {\"content\": topic}}]}\n",
        "            })\n",
        "\n",
        "        # 3. ä¼šè®®æ€»ç»“ï¼ˆæ€»ç»“æ®µè½æ•°+1ä¸ªå—ï¼‰\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"ä¼šè®®æ€»ç»“\"}}]}\n",
        "        })\n",
        "        # é™åˆ¶æ€»ç»“æ®µè½æ•°\n",
        "        max_summary_paras = 5  # æœ€å¤š5æ®µæ€»ç»“\n",
        "        for para in meeting_data.get('summary', '').split('\\n\\n')[:max_summary_paras]:\n",
        "            if para.strip():\n",
        "                children_blocks.append({\n",
        "                    \"object\": \"block\",\n",
        "                    \"type\": \"paragraph\",\n",
        "                    \"paragraph\": {\"rich_text\": [{\"text\": {\"content\": para.strip()}}]}\n",
        "                })\n",
        "\n",
        "        # 4. å…³é”®è¦ç‚¹ï¼ˆè¯é¢˜æ•°+è¦ç‚¹æ•°+1ä¸ªå—ï¼‰\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"å…³é”®è¦ç‚¹\"}}]}\n",
        "        })\n",
        "        key_points = meeting_data.get('key_points', {})\n",
        "        max_key_topics = 10  # æœ€å¤š10ä¸ªå…³é”®è¯é¢˜\n",
        "        for topic, points in list(key_points.items())[:max_key_topics]:\n",
        "            children_blocks.append({\n",
        "                \"object\": \"block\",\n",
        "                \"type\": \"heading_3\",\n",
        "                \"heading_3\": {\"rich_text\": [{\"text\": {\"content\": topic}}]}\n",
        "            })\n",
        "            # é™åˆ¶æ¯ä¸ªè¯é¢˜çš„è¦ç‚¹æ•°é‡\n",
        "            max_points_per_topic = 5  # æ¯ä¸ªè¯é¢˜æœ€å¤š5ä¸ªè¦ç‚¹\n",
        "            for point in points[:max_points_per_topic]:\n",
        "                children_blocks.append({\n",
        "                    \"object\": \"block\",\n",
        "                    \"type\": \"bulleted_list_item\",\n",
        "                    \"bulleted_list_item\": {\"rich_text\": [{\"text\": {\"content\": point}}]}\n",
        "                })\n",
        "\n",
        "        # 5. è¡ŒåŠ¨é¡¹è¡¨æ ¼ï¼ˆè¡ŒåŠ¨é¡¹æ•°+2ä¸ªå—ï¼‰\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"è¡ŒåŠ¨é¡¹\"}}]}\n",
        "        })\n",
        "        table_rows = []\n",
        "        max_actions = 20  # æœ€å¤šæ˜¾ç¤º20ä¸ªè¡ŒåŠ¨é¡¹\n",
        "        for idx, item in enumerate(meeting_data.get('action_items', [])[:max_actions]):\n",
        "            table_rows.append([\n",
        "                [{\"text\": {\"content\": str(idx+1)}}],\n",
        "                [{\"text\": {\"content\": item.get('task', '')}}],\n",
        "                [{\"text\": {\"content\": item.get('assignee', 'æœªåˆ†é…')}}],\n",
        "                [{\"text\": {\"content\": item.get('due_date', 'æ— ')}}]\n",
        "            ])\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"table\",\n",
        "            \"table\": {\n",
        "                \"table_width\": 4,\n",
        "                \"has_column_header\": True,\n",
        "                \"children\": [\n",
        "                    {\n",
        "                        \"object\": \"block\",\n",
        "                        \"type\": \"table_row\",\n",
        "                        \"table_row\": {\n",
        "                            \"cells\": [\n",
        "                                [{\"text\": {\"content\": \"åºå·\"}}],\n",
        "                                [{\"text\": {\"content\": \"ä»»åŠ¡\"}}],\n",
        "                                [{\"text\": {\"content\": \"è´Ÿè´£äºº\"}}],\n",
        "                                [{\"text\": {\"content\": \"æˆªæ­¢æ—¥æœŸ\"}}]\n",
        "                            ]\n",
        "                        }\n",
        "                    },\n",
        "                    *[{\n",
        "                        \"object\": \"block\",\n",
        "                        \"type\": \"table_row\",\n",
        "                        \"table_row\": {\"cells\": cells}\n",
        "                    } for cells in table_rows]\n",
        "                ]\n",
        "            }\n",
        "        })\n",
        "\n",
        "        # 6. è½¬å½•æ–‡æœ¬ï¼ˆé™åˆ¶ä¸ºMAX_TRANSCRIPT_SEGMENTSæ¡ï¼Œé¿å…å—è¶…é™ï¼‰\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": f\"ä¼šè®®è½¬å½•æ–‡æœ¬ï¼ˆèŠ‚é€‰ï¼Œå…±{len(transcript_segments)}æ¡ï¼‰\"}}]}\n",
        "        })\n",
        "        # åªæ˜¾ç¤ºå‰MAX_TRANSCRIPT_SEGMENTSæ¡è½¬å½•æ–‡æœ¬\n",
        "        for seg in transcript_segments[:MAX_TRANSCRIPT_SEGMENTS]:\n",
        "            time_str = str(datetime.timedelta(seconds=int(seg[\"start\"])))\n",
        "            children_blocks.append({\n",
        "                \"object\": \"block\",\n",
        "                \"type\": \"paragraph\",\n",
        "                \"paragraph\": {\"rich_text\": [{\"text\": {\"content\": f\"[{time_str}] {seg['text']}\"}}]}\n",
        "            })\n",
        "\n",
        "        # æ£€æŸ¥æ€»å—æ•°ï¼Œç¡®ä¿ä¸è¶…è¿‡100\n",
        "        if len(children_blocks) > 100:\n",
        "            # ç´§æ€¥æˆªæ–­ï¼ˆä¿ç•™æ ¸å¿ƒå†…å®¹ï¼‰\n",
        "            children_blocks = children_blocks[:100]\n",
        "            print(f\"âš ï¸ è­¦å‘Šï¼šå†…å®¹å—æ•°é‡è¶…é™ï¼Œå·²æˆªæ–­ä¸º100ä¸ªå—\")\n",
        "\n",
        "        # å°†å†…å®¹æ·»åŠ åˆ°é¡µé¢\n",
        "        notion.blocks.children.append(block_id=page_id, children=children_blocks)\n",
        "        report_url = new_page.get(\"url\", \"\")\n",
        "\n",
        "        logger.log_step(\"åˆ›å»ºNotionæŠ¥å‘Š\", \"success\", {\"æŠ¥å‘ŠURL\": report_url, \"æ€»å—æ•°\": len(children_blocks)})\n",
        "        print(f\"âœ… NotionæŠ¥å‘Šå·²ç”Ÿæˆï¼ˆæ€»å—æ•°: {len(children_blocks)}ï¼‰\")\n",
        "        return report_url\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"åˆ›å»ºNotionæŠ¥å‘Š\", \"failed\", error=str(e))\n",
        "        print(f\"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# ======================\n",
        "# è¾“å…¥å¤„ç†\n",
        "# ======================\n",
        "def handle_input():\n",
        "    \"\"\"å¤„ç†ç”¨æˆ·è¾“å…¥ï¼ˆéŸ³é¢‘æˆ–æ–‡æœ¬ï¼‰\"\"\"\n",
        "    print(\"\\n=== è¯·é€‰æ‹©è¾“å…¥æ–¹å¼ ===\")\n",
        "    print(\"1: ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ (.mp3/.wav/.m4a/.opus)\")\n",
        "    print(\"2: ä¸Šä¼ å¸¦æ—¶é—´æˆ³çš„è½¬å½•æ–‡æœ¬ (.txt)\")\n",
        "\n",
        "    try:\n",
        "        choice = input(\"è¯·é€‰æ‹© (1/2): \").strip() or \"1\"\n",
        "    except:\n",
        "        choice = \"1\"\n",
        "\n",
        "    if choice == \"1\":\n",
        "        # å¤„ç†éŸ³é¢‘è¾“å…¥\n",
        "        print(\"\\nè¯·ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ï¼ˆæ”¯æŒ2-3å°æ—¶ä¼šè®®å½•éŸ³ï¼‰:\")\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            print(\"âš ï¸ æœªæ£€æµ‹åˆ°ä¸Šä¼ æ–‡ä»¶ï¼Œè¯·é‡è¯•\")\n",
        "            return handle_input()\n",
        "\n",
        "        filename = next(iter(uploaded.keys()))\n",
        "        audio_ext = os.path.splitext(filename)[1].lower()\n",
        "        supported_ext = ['.mp3', '.wav', '.m4a', '.opus']\n",
        "\n",
        "        if audio_ext not in supported_ext:\n",
        "            print(f\"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼ˆæ”¯æŒ: {', '.join(supported_ext)}ï¼‰\")\n",
        "            return handle_input()\n",
        "\n",
        "        # ä¿å­˜éŸ³é¢‘æ–‡ä»¶\n",
        "        audio_path = f\"/tmp/{filename}\"\n",
        "        with open(audio_path, 'wb') as f:\n",
        "            f.write(uploaded[filename])\n",
        "\n",
        "        # æ£€æŸ¥éŸ³é¢‘æ—¶é•¿\n",
        "        duration = get_audio_duration(audio_path)\n",
        "        if duration < 3600:  # å°äº1å°æ—¶\n",
        "            print(f\"âš ï¸ æ£€æµ‹åˆ°éŸ³é¢‘æ—¶é•¿è¾ƒçŸ­ï¼ˆ{duration/60:.1f}åˆ†é’Ÿï¼‰\")\n",
        "            if input(\"æ˜¯å¦ç»§ç»­ä½¿ç”¨é•¿ä¼šè®®æ¨¡å¼å¤„ç†? (y/n): \").strip().lower() != 'y':\n",
        "                os.unlink(audio_path)\n",
        "                print(\"å·²åˆ‡æ¢åˆ°æ™®é€šæ¨¡å¼\")\n",
        "                return handle_input()\n",
        "\n",
        "        print(f\"ğŸµ éŸ³é¢‘ä¿¡æ¯: {filename}ï¼ˆ{duration/60:.1f}åˆ†é’Ÿï¼‰\")\n",
        "\n",
        "        # é€‰æ‹©è½¬å½•æ¨¡å‹\n",
        "        print(\"\\nâš¡ è¯·é€‰æ‹©è½¬å½•æ¨¡å‹:\")\n",
        "        print(\"1: base.en - å¿«é€Ÿæ¨¡å¼ï¼ˆé€‚åˆæ¸…æ™°è¯­éŸ³ï¼‰\")\n",
        "        print(\"2: small.en - å¹³è¡¡æ¨¡å¼ï¼ˆæ¨èï¼Œé€Ÿåº¦ä¸ç²¾åº¦å…¼é¡¾ï¼‰\")\n",
        "        print(\"3: medium.en - é«˜ç²¾åº¦æ¨¡å¼ï¼ˆé€‚åˆå¤æ‚ä¼šè®®ï¼‰\")\n",
        "\n",
        "        model_choice = input(\"è¯·é€‰æ‹© (1-3ï¼Œé»˜è®¤2): \").strip() or \"2\"\n",
        "        model_map = {\"1\": \"base.en\", \"2\": \"small.en\", \"3\": \"medium.en\"}\n",
        "        model_size = model_map.get(model_choice, \"small.en\")\n",
        "        print(f\"å°†ä½¿ç”¨ {model_size} æ¨¡å‹è¿›è¡Œè½¬å½•\")\n",
        "\n",
        "        # æ‰§è¡Œè½¬å½•\n",
        "        try:\n",
        "            transcript, segments, language = transcribe_long_audio(audio_path, model_size)\n",
        "            return transcript, segments, language\n",
        "        except Exception as e:\n",
        "            if os.path.exists(audio_path):\n",
        "                os.unlink(audio_path)\n",
        "            raise\n",
        "\n",
        "    elif choice == \"2\":\n",
        "        # å¤„ç†æ–‡æœ¬è¾“å…¥\n",
        "        print(\"\\nè¯·ä¸Šä¼ å¸¦æ—¶é—´æˆ³çš„è½¬å½•æ–‡æœ¬ï¼ˆæ ¼å¼ç¤ºä¾‹: [00:05:10] å‘è¨€äºº: ...ï¼‰:\")\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            print(\"âš ï¸ æœªæ£€æµ‹åˆ°ä¸Šä¼ æ–‡ä»¶ï¼Œè¯·é‡è¯•\")\n",
        "            return handle_input()\n",
        "\n",
        "        filename = next(iter(uploaded.keys()))\n",
        "        if not filename.endswith('.txt'):\n",
        "            print(\"âŒ ä»…æ”¯æŒ.txtæ ¼å¼çš„æ–‡æœ¬æ–‡ä»¶\")\n",
        "            return handle_input()\n",
        "\n",
        "        # è§£ææ–‡æœ¬\n",
        "        try:\n",
        "            transcript_text = uploaded[filename].decode('utf-8')\n",
        "            segments = []\n",
        "            time_pattern = re.compile(r'\\[(\\d+:\\d+:\\d+)\\]')  # åŒ¹é…[HH:MM:SS]\n",
        "\n",
        "            for line in transcript_text.split('\\n'):\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                match = time_pattern.search(line)\n",
        "                if match:\n",
        "                    time_str = match.group(1)\n",
        "                    text = time_pattern.sub('', line).strip()\n",
        "                    # è½¬æ¢æ—¶é—´ä¸ºç§’\n",
        "                    h, m, s = map(int, time_str.split(':'))\n",
        "                    start_time = h * 3600 + m * 60 + s\n",
        "                    segments.append({\n",
        "                        \"text\": text,\n",
        "                        \"start\": start_time,\n",
        "                        \"end\": start_time + 30  # ä¼°ç®—ç»“æŸæ—¶é—´\n",
        "                    })\n",
        "\n",
        "            if not segments:\n",
        "                raise ValueError(\"æœªæ£€æµ‹åˆ°æœ‰æ•ˆæ—¶é—´æˆ³ï¼Œè¯·æ£€æŸ¥æ–‡æœ¬æ ¼å¼\")\n",
        "\n",
        "            # æ£€æµ‹è¯­è¨€\n",
        "            language = detect(transcript_text[:500]) if transcript_text else 'en'\n",
        "            print(f\"âœ… å·²è§£æè½¬å½•æ–‡æœ¬ï¼ˆ{len(segments)}ä¸ªç‰‡æ®µï¼Œæ£€æµ‹è¯­è¨€: {language}ï¼‰\")\n",
        "            return transcript_text, segments, language\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ æ–‡æœ¬è§£æå¤±è´¥: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    else:\n",
        "        print(\"âš ï¸ æ— æ•ˆé€‰æ‹©ï¼Œé»˜è®¤ä½¿ç”¨éŸ³é¢‘è¾“å…¥\")\n",
        "        return handle_input()\n",
        "\n",
        "# ======================\n",
        "# ä¸»å‡½æ•°\n",
        "# ======================\n",
        "def main():\n",
        "    \"\"\"ä¸»å‡½æ•°ï¼šåè°ƒä¼šè®®å¤„ç†æµç¨‹\"\"\"\n",
        "    print(\"=== ä¼šè®®åˆ†æå·¥å…· ===\")\n",
        "    logger.log_step(\"ä¼šè®®å¤„ç†æµç¨‹\", \"started\")\n",
        "\n",
        "    try:\n",
        "        # éªŒè¯GPUç¯å¢ƒ\n",
        "        if not torch.cuda.is_available():\n",
        "            raise RuntimeError(\"è¯·åˆ‡æ¢åˆ°GPUç¯å¢ƒï¼ˆRuntime > Change runtime type > é€‰æ‹©GPUï¼‰\")\n",
        "        print(f\"âœ… æ£€æµ‹åˆ°GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "        # å¤„ç†è¾“å…¥\n",
        "        transcript, segments, language = handle_input()\n",
        "        if not transcript or not segments:\n",
        "            raise RuntimeError(\"æœªè·å–åˆ°æœ‰æ•ˆä¼šè®®å†…å®¹\")\n",
        "\n",
        "        # åˆ†æä¼šè®®\n",
        "        meeting_data = analyze_meeting(segments, language)\n",
        "        if \"error\" in meeting_data:\n",
        "            raise RuntimeError(meeting_data[\"error\"])\n",
        "\n",
        "        # ç”ŸæˆNotionæŠ¥å‘Š\n",
        "        report_url = create_notion_report(meeting_data, segments)\n",
        "        if not report_url:\n",
        "            raise RuntimeError(\"æ— æ³•ç”ŸæˆNotionæŠ¥å‘Š\")\n",
        "\n",
        "        # ä¿å­˜æ—¥å¿—å¹¶è¾“å‡ºç»“æœ\n",
        "        log_file = logger.save_logs()\n",
        "        print(f\"\\nğŸ‰ ä¼šè®®å¤„ç†å®Œæˆï¼\")\n",
        "        print(f\"ğŸ“„ ä¼šè®®æŠ¥å‘Š: {report_url}\")\n",
        "        print(f\"ğŸ“‹ å¤„ç†æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}\")\n",
        "\n",
        "        # æ˜¾ç¤ºæŠ¥å‘Šé“¾æ¥\n",
        "        from IPython.display import HTML\n",
        "        display(HTML(f'<a href=\"{report_url}\" target=\"_blank\">ç‚¹å‡»æŸ¥çœ‹Notionä¼šè®®æŠ¥å‘Š</a>'))\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.save_logs(\"meeting_error_logs.json\")\n",
        "        print(f\"\\nâŒ å¤„ç†å¤±è´¥: {str(e)}\")\n",
        "        print(\"é”™è¯¯è¯¦æƒ…å·²ä¿å­˜åˆ° meeting_error_logs.json\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "NNnUaMEXajrA",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# å®‰è£…å¿…è¦ä¾èµ–ï¼ˆæŒ‡å®šå…¼å®¹ç‰ˆæœ¬ï¼‰\n",
        "!pip uninstall -y whisper\n",
        "!pip install faster-whisper==0.10.0  # é”å®šç‰ˆæœ¬ä»¥é¿å…APIå˜æ›´\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install tqdm python-docx notion-client langdetect langchain==0.1.13 langchain-openai==0.0.8 pydantic==2.5.2 httpx==0.27.0\n",
        "!sudo apt update && sudo apt install ffmpeg -y\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import torch\n",
        "import subprocess\n",
        "import datetime\n",
        "import tempfile\n",
        "from tqdm import tqdm\n",
        "from pydantic import BaseModel, Field\n",
        "import httpx\n",
        "import concurrent.futures\n",
        "from functools import partial\n",
        "\n",
        "# å¯¼å…¥ç¬¬ä¸‰æ–¹åº“\n",
        "from google.colab import files, userdata\n",
        "from notion_client import Client, errors\n",
        "from langdetect import detect, LangDetectException\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# æ¸…é™¤ä»£ç†ç¯å¢ƒå˜é‡ï¼ˆé¿å…ç½‘ç»œè¿æ¥é—®é¢˜ï¼‰\n",
        "for var in ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy']:\n",
        "    if var in os.environ:\n",
        "        del os.environ[var]\n",
        "\n",
        "# åˆå§‹åŒ–Notionå®¢æˆ·ç«¯ï¼ˆä¿®å¤httpxä»£ç†å‚æ•°é—®é¢˜ï¼‰\n",
        "http_client = httpx.Client()\n",
        "http_client.proxies = None  # ç¦ç”¨ä»£ç†\n",
        "\n",
        "notion = Client(\n",
        "    auth=userdata.get('NOTION_TOKEN'),\n",
        "    client=http_client\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# é…ç½®å‚æ•°ä¸åˆå§‹åŒ–\n",
        "# ======================\n",
        "try:\n",
        "    # ä»ç¯å¢ƒå˜é‡è·å–å¯†é’¥\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    NOTION_TOKEN = userdata.get('NOTION_TOKEN')\n",
        "    NOTION_PAGE_ID = userdata.get('NOTION_PAGE_ID')\n",
        "    NOTION_DB_ID = userdata.get('NOTION_DB_ID')  # æ•°æ®åº“ID\n",
        "\n",
        "    # é•¿éŸ³é¢‘å¤„ç†å‚æ•°\n",
        "    CHUNK_DURATION = 900  # æ¯å—15åˆ†é’Ÿï¼ˆç§’ï¼‰\n",
        "    OVERLAP_DURATION = 30  # å—é—´é‡å 30ç§’\n",
        "    MAX_CONCURRENT_CHUNKS = 1  # å•çº¿ç¨‹å¤„ç†ï¼Œé¿å…æ–‡ä»¶ç«äº‰\n",
        "    # Notionå—æ•°é‡é™åˆ¶ç›¸å…³å‚æ•°\n",
        "    MAX_TRANSCRIPT_SEGMENTS = 50  # æœ€å¤šæ˜¾ç¤º50æ¡è½¬å½•æ–‡æœ¬\n",
        "    NOTION_RICH_TEXT_LIMIT = 1950  # Notion rich_textå­—æ®µæœ€å¤§é•¿åº¦ï¼ˆç•™50å­—ç¬¦ä½™é‡ï¼‰\n",
        "\n",
        "    # éªŒè¯å¿…è¦å¯†é’¥\n",
        "    missing_creds = []\n",
        "    if not OPENAI_API_KEY:\n",
        "        missing_creds.append(\"OPENAI_API_KEY\")\n",
        "    if not NOTION_TOKEN:\n",
        "        missing_creds.append(\"NOTION_TOKEN\")\n",
        "    if not NOTION_PAGE_ID:\n",
        "        missing_creds.append(\"NOTION_PAGE_ID\")\n",
        "    if not NOTION_DB_ID:\n",
        "        missing_creds.append(\"NOTION_DB_ID\")\n",
        "\n",
        "    if missing_creds:\n",
        "        raise ValueError(f\"ç¼ºå°‘å¿…è¦å‡­è¯: {', '.join(missing_creds)}\")\n",
        "\n",
        "    print(\"âœ… æ‰€æœ‰å‡­è¯å·²é…ç½®å®Œæˆï¼Œå‡†å¤‡å¤„ç†ä¼šè®®å†…å®¹\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ åˆå§‹åŒ–å¤±è´¥: {str(e)}\")\n",
        "    print(\"\\nè®¾ç½®æŒ‡å—:\")\n",
        "    print(\"1. ç‚¹å‡»å·¦ä¾§è¾¹æ çš„é’¥åŒ™å›¾æ ‡ï¼ˆColab Secretsï¼‰\")\n",
        "    print(\"2. æ·»åŠ ä»¥ä¸‹å¯†é’¥:\")\n",
        "    print(\"   - OPENAI_API_KEY: ä½ çš„OpenAI APIå¯†é’¥\")\n",
        "    print(\"   - NOTION_TOKEN: Notioné›†æˆä»¤ç‰Œ\")\n",
        "    print(\"   - NOTION_PAGE_ID: ä¼šè®®é›†æˆé¡µID\")\n",
        "    print(\"   - NOTION_DB_ID: ç›®æ ‡æ•°æ®åº“ID\")\n",
        "    raise\n",
        "\n",
        "# ======================\n",
        "# æ—¥å¿—è®°å½•ç³»ç»Ÿ\n",
        "# ======================\n",
        "class MeetingLogger:\n",
        "    def __init__(self):\n",
        "        self.logs = {\n",
        "            \"start_time\": datetime.datetime.now().isoformat(),\n",
        "            \"steps\": [],\n",
        "            \"chunk_status\": {}\n",
        "        }\n",
        "\n",
        "    def log_step(self, step_name, status, details=None, error=None):\n",
        "        entry = {\n",
        "            \"step\": step_name,\n",
        "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "            \"status\": status\n",
        "        }\n",
        "        if details:\n",
        "            entry[\"details\"] = details\n",
        "        if error:\n",
        "            entry[\"error\"] = str(error)\n",
        "        self.logs[\"steps\"].append(entry)\n",
        "\n",
        "    def log_chunk(self, chunk_id, status, error=None):\n",
        "        self.logs[\"chunk_status\"][chunk_id] = {\n",
        "            \"status\": status,\n",
        "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "            \"error\": str(error) if error else None\n",
        "        }\n",
        "\n",
        "    def get_completed_chunks(self):\n",
        "        return [k for k, v in self.logs[\"chunk_status\"].items() if v[\"status\"] == \"success\"]\n",
        "\n",
        "    def get_failed_chunks(self):\n",
        "        return [k for k, v in self.logs[\"chunk_status\"].items() if v[\"status\"] == \"failed\"]\n",
        "\n",
        "    def save_logs(self, filename=\"meeting_logs.json\"):\n",
        "        with open(filename, \"w\") as f:\n",
        "            json.dump(self.logs, f, indent=2)\n",
        "        return filename\n",
        "\n",
        "logger = MeetingLogger()\n",
        "\n",
        "# ======================\n",
        "# éŸ³é¢‘å¤„ç†å·¥å…·\n",
        "# ======================\n",
        "def get_audio_duration(audio_path):\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"ffmpeg\", \"-i\", audio_path],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            text=True\n",
        "        )\n",
        "        output = result.stdout\n",
        "\n",
        "        duration_match = re.search(r\"Duration: (\\d+:\\d+:\\d+\\.\\d+)\", output)\n",
        "        if not duration_match:\n",
        "            return 0.0\n",
        "\n",
        "        duration_str = duration_match.group(1)\n",
        "        h, m, s = duration_str.split(':')\n",
        "        return float(h) * 3600 + float(m) * 60 + float(s)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"è·å–éŸ³é¢‘æ—¶é•¿\", \"warning\", error=str(e))\n",
        "        return 0.0\n",
        "\n",
        "def split_audio_into_chunks(audio_path):\n",
        "    logger.log_step(\"éŸ³é¢‘åˆ†å—\", \"started\")\n",
        "\n",
        "    try:\n",
        "        total_duration = get_audio_duration(audio_path)\n",
        "        if total_duration <= 0:\n",
        "            raise ValueError(\"æ— æ³•è·å–æœ‰æ•ˆéŸ³é¢‘æ—¶é•¿ï¼Œå¯èƒ½æ–‡ä»¶æŸå\")\n",
        "\n",
        "        num_chunks = max(1, int((total_duration + CHUNK_DURATION - OVERLAP_DURATION) //\n",
        "                              (CHUNK_DURATION - OVERLAP_DURATION)))\n",
        "        logger.log_step(\"è®¡ç®—åˆ†å—æ•°é‡\", \"success\", {\"æ€»æ—¶é•¿(åˆ†é’Ÿ)\": f\"{total_duration/60:.1f}\", \"åˆ†å—æ•°\": num_chunks})\n",
        "        print(f\"ğŸ“Š éŸ³é¢‘å°†åˆ†å‰²ä¸º {num_chunks} å—ï¼ˆæ¯å—15åˆ†é’Ÿï¼Œé‡å 30ç§’ï¼‰\")\n",
        "\n",
        "        chunk_dir = tempfile.mkdtemp()\n",
        "        chunks = []\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_time = i * (CHUNK_DURATION - OVERLAP_DURATION)\n",
        "            end_time = min(start_time + CHUNK_DURATION, total_duration)\n",
        "\n",
        "            start_str = str(datetime.timedelta(seconds=start_time))\n",
        "            duration_str = str(datetime.timedelta(seconds=end_time - start_time))\n",
        "\n",
        "            chunk_path = f\"{chunk_dir}/chunk_{i:03d}.wav\"\n",
        "\n",
        "            subprocess.run(\n",
        "                [\n",
        "                    \"ffmpeg\", \"-y\",\n",
        "                    \"-i\", audio_path,\n",
        "                    \"-ss\", start_str,\n",
        "                    \"-t\", duration_str,\n",
        "                    \"-ar\", \"16000\",\n",
        "                    \"-ac\", \"1\",\n",
        "                    \"-acodec\", \"pcm_s16le\",\n",
        "                    chunk_path\n",
        "                ],\n",
        "                check=True,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE\n",
        "            )\n",
        "\n",
        "            if not os.path.exists(chunk_path) or os.path.getsize(chunk_path) < 1024:\n",
        "                raise RuntimeError(f\"åˆ†å— {i} ç”Ÿæˆå¤±è´¥ï¼Œæ–‡ä»¶å¤§å°å¼‚å¸¸\")\n",
        "\n",
        "            chunks.append({\n",
        "                \"id\": i,\n",
        "                \"path\": chunk_path,\n",
        "                \"start_time\": start_time,\n",
        "                \"end_time\": end_time,\n",
        "                \"dir\": chunk_dir\n",
        "            })\n",
        "\n",
        "        logger.log_step(\"éŸ³é¢‘åˆ†å—\", \"success\")\n",
        "        return chunks\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"éŸ³é¢‘åˆ†å—\", \"failed\", error=str(e))\n",
        "        raise RuntimeError(f\"éŸ³é¢‘åˆ†å—å¤±è´¥: {str(e)}\")\n",
        "\n",
        "# ======================\n",
        "# è¯­éŸ³è½¬å½•\n",
        "# ======================\n",
        "global_model = None\n",
        "\n",
        "def transcribe_chunk(chunk):\n",
        "    chunk_id = chunk[\"id\"]\n",
        "    chunk_path = chunk[\"path\"]\n",
        "    start_time = chunk[\"start_time\"]\n",
        "\n",
        "    try:\n",
        "        from faster_whisper import WhisperModel\n",
        "        global global_model\n",
        "\n",
        "        if global_model is None:\n",
        "            if not torch.cuda.is_available():\n",
        "                raise RuntimeError(\"è¯·åˆ‡æ¢åˆ°GPUç¯å¢ƒï¼ˆRuntime > Change runtime typeï¼‰\")\n",
        "\n",
        "            print(f\"ğŸ”§ åŠ è½½ {chunk['model_size']} æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œéœ€ä¸‹è½½çº¦1.5GBï¼‰...\")\n",
        "            global_model = WhisperModel(\n",
        "                chunk[\"model_size\"],\n",
        "                device=\"cuda\",\n",
        "                compute_type=\"float16\",\n",
        "                download_root=\"/content/models\"\n",
        "            )\n",
        "            print(f\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ\")\n",
        "\n",
        "        if not os.path.exists(chunk_path):\n",
        "            raise FileNotFoundError(f\"åˆ†å—æ–‡ä»¶ä¸å­˜åœ¨: {chunk_path}\")\n",
        "\n",
        "        segments, info = global_model.transcribe(\n",
        "            chunk_path,\n",
        "            beam_size=2,\n",
        "            vad_filter=True,\n",
        "            vad_parameters=dict(min_silence_duration_ms=300),\n",
        "            language=None  # è‡ªåŠ¨æ£€æµ‹è¯­è¨€\n",
        "        )\n",
        "\n",
        "        timestamped_segments = []\n",
        "        for seg in segments:\n",
        "            timestamped_segments.append({\n",
        "                \"text\": seg.text.strip(),\n",
        "                \"start\": seg.start + start_time,\n",
        "                \"end\": seg.end + start_time,\n",
        "                \"language\": info.language  # è®°å½•å½“å‰å—çš„è¯­è¨€\n",
        "            })\n",
        "\n",
        "        logger.log_chunk(chunk_id, \"success\")\n",
        "        return {\n",
        "            \"chunk_id\": chunk_id,\n",
        "            \"segments\": timestamped_segments,\n",
        "            \"language\": info.language\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_chunk(chunk_id, \"failed\", error=str(e))\n",
        "        print(f\"âš ï¸ åˆ†å— {chunk_id} è½¬å½•å¤±è´¥: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def transcribe_long_audio(audio_path, model_size=\"small.en\"):\n",
        "    logger.log_step(\"éŸ³é¢‘è½¬å½•\", \"started\", {\"æ¨¡å‹\": model_size})\n",
        "\n",
        "    try:\n",
        "        global global_model\n",
        "        global_model = None\n",
        "\n",
        "        chunks = split_audio_into_chunks(audio_path)\n",
        "        num_chunks = len(chunks)\n",
        "\n",
        "        for chunk in chunks:\n",
        "            chunk[\"model_size\"] = model_size\n",
        "\n",
        "        print(f\"ğŸš€ å¼€å§‹è½¬å½• {num_chunks} ä¸ªåˆ†å—...\")\n",
        "        results = []\n",
        "        for chunk in tqdm(chunks, desc=\"è½¬å½•è¿›åº¦\"):\n",
        "            result = transcribe_chunk(chunk)\n",
        "            if result:\n",
        "                results.append(result)\n",
        "\n",
        "        failed_ids = logger.get_failed_chunks()\n",
        "        if failed_ids:\n",
        "            print(f\"ğŸ”„ é‡è¯• {len(failed_ids)} ä¸ªå¤±è´¥åˆ†å—...\")\n",
        "            for chunk in chunks:\n",
        "                if chunk[\"id\"] in failed_ids:\n",
        "                    result = transcribe_chunk(chunk)\n",
        "                    if result:\n",
        "                        results.append(result)\n",
        "\n",
        "        results.sort(key=lambda x: x[\"chunk_id\"])\n",
        "        all_segments = []\n",
        "        for res in results:\n",
        "            all_segments.extend(res[\"segments\"])\n",
        "\n",
        "        unique_dirs = list({chunk[\"dir\"] for chunk in chunks})\n",
        "        for dir_path in unique_dirs:\n",
        "            if os.path.exists(dir_path):\n",
        "                for f in os.listdir(dir_path):\n",
        "                    os.unlink(os.path.join(dir_path, f))\n",
        "                os.rmdir(dir_path)\n",
        "\n",
        "        # æå–æ‰€æœ‰å‡ºç°çš„è¯­è¨€ï¼ˆå»é‡ï¼‰\n",
        "        all_languages = list({seg.get(\"language\", \"unknown\") for seg in all_segments})\n",
        "        primary_language = results[0][\"language\"] if results else \"en\"\n",
        "\n",
        "        # è®¡ç®—ä¼šè®®æ€»æ—¶é•¿ï¼ˆç§’ï¼‰\n",
        "        total_seconds = all_segments[-1][\"end\"] - all_segments[0][\"start\"] if all_segments else 0\n",
        "\n",
        "        logger.log_step(\"éŸ³é¢‘è½¬å½•\", \"success\", {\"æ€»ç‰‡æ®µæ•°\": len(all_segments), \"æ‰€æœ‰è¯­è¨€\": all_languages})\n",
        "        print(f\"âœ… è½¬å½•å®Œæˆï¼ˆ{len(all_segments)}ä¸ªç‰‡æ®µï¼Œä¸»è¦è¯­è¨€: {primary_language}ï¼Œæ‰€æœ‰è¯­è¨€: {all_languages}ï¼‰\")\n",
        "        return \" \".join([s[\"text\"] for s in all_segments]), all_segments, primary_language, all_languages, total_seconds\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"éŸ³é¢‘è½¬å½•\", \"failed\", error=str(e))\n",
        "        raise RuntimeError(f\"è½¬å½•è¿‡ç¨‹å¤±è´¥: {str(e)}\")\n",
        "\n",
        "# ======================\n",
        "# ä¼šè®®å†…å®¹åˆ†æ\n",
        "# ======================\n",
        "class ChunkAnalysis(BaseModel):\n",
        "    summary: str = Field(description=\"è¯¥ç‰‡æ®µçš„æ€»ç»“ï¼ˆ100-200å­—ï¼‰\")\n",
        "    key_points: list[str] = Field(description=\"è¯¥ç‰‡æ®µçš„å…³é”®ç‚¹é”®åˆ—è¡¨ï¼ˆå®¢è§‚äº‹å®ï¼‰\")\n",
        "    action_items: list[dict] = Field(description=\"è¡ŒåŠ¨é¡¹åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«taskã€assigneeã€due_date\")\n",
        "    topics: list[str] = Field(description=\"è®¨è®ºçš„è¯é¢˜åˆ—è¡¨\")\n",
        "    decisions: list[str] = Field(description=\"è¯¥ç‰‡æ®µä¸­è¾¾æˆçš„å…·ä½“å†³å®šï¼ˆæ˜ç¡®çš„ç»“è®ºï¼‰\")\n",
        "    concerns: list[str] = Field(description=\"è¯¥ç‰‡æ®µä¸­æå‡ºçš„æ‹…å¿§ã€é—®é¢˜æˆ–é£é™©\")\n",
        "    platform: str = Field(description=\"ä¼šè®®å‘ç”Ÿçš„å¹³å°æˆ–åœºæ‰€ï¼ˆå¦‚Zoomã€ä¼šè®®å®¤Aç­‰ï¼‰\")  # æ–°å¢å­—æ®µ\n",
        "\n",
        "class FullMeetingAnalysis(BaseModel):\n",
        "    meeting_title: str = Field(description=\"ä¼šè®®æ ‡é¢˜\")\n",
        "    participants: list[str] = Field(description=\"å‚ä¸è€…åå•\")\n",
        "    summary: str = Field(description=\"3-5æ®µå®Œæ•´ä¼šè®®æ€»ç»“\")\n",
        "    key_points: dict = Field(description=\"æŒ‰è¯é¢˜åˆ†ç»„ç»„çš„å…¨å±€å…³é”®ç‚¹ï¼ˆå®¢è§‚äº‹å®ï¼‰\")\n",
        "    action_items: list[dict] = Field(description=\"æ±‡æ€»çš„è¡ŒåŠ¨é¡¹\")\n",
        "    meeting_type: str = Field(description=\"ä¼šè®®ç±»å‹ï¼ˆå¦‚å‘¨ä¼šã€é¡¹ç›®è¯„å®¡ä¼šã€å¤´è„‘é£æš´ç­‰ï¼‰\")\n",
        "    topics_flow: list[str] = Field(description=\"ä¼šè®®è¯é¢˜æµè½¬é¡ºåº\")\n",
        "    decisions: list[str] = Field(description=\"ä¼šè®®ä¸­è¾¾æˆçš„æ‰€æœ‰å†³å®šï¼ˆæ˜ç¡®ç»“è®ºï¼Œå¦‚â€œåŒæ„é¡¹ç›®å»¶æœŸâ€ï¼‰\")\n",
        "    concerns: list[str] = Field(description=\"ä¼šè®®ä¸­æå‡ºçš„æ‰€æœ‰æ‹…å¿§ã€é—®é¢˜æˆ–é£é™©ï¼ˆå¦‚â€œèµ„æºä¸è¶³â€ï¼‰\")\n",
        "    platform: str = Field(description=\"ä¼šè®®å‘ç”Ÿçš„å¹³å°æˆ–åœºæ‰€ï¼ˆå¦‚Zoomã€Teamsã€ä¼šè®®å®¤Bç­‰ï¼‰\")  # æ–°å¢å­—æ®µ\n",
        "\n",
        "def get_chunk_analysis_chain(language):\n",
        "    parser = JsonOutputParser(pydantic_object=ChunkAnalysis)\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"åˆ†æä»¥ä¸‹ä¼šè®®ç‰‡æ®µï¼Œæå–å…³é”®ä¿¡æ¯ï¼ˆç”¨{language}ï¼‰ï¼š\n",
        "{format_instructions}\n",
        "æ³¨æ„ï¼š\n",
        "- key_pointsï¼šå®¢è§‚äº‹å®ï¼ˆå¦‚â€œé¡¹ç›®è¿›åº¦è½å20%â€ï¼‰\n",
        "- decisionsï¼šæ˜ç¡®è¾¾æˆçš„ç»“è®ºï¼ˆå¦‚â€œå†³å®šå¢åŠ 2åå¼€å‘äººå‘˜â€ï¼‰\n",
        "- concernsï¼šæå‡ºçš„æ‹…å¿§ï¼ˆå¦‚â€œé¢„ç®—å¯èƒ½è¶…æ”¯â€ï¼‰\n",
        "- platformï¼šä¼šè®®è¿›è¡Œçš„å¹³å°æˆ–ç‰©ç†åœºæ‰€ï¼ˆå¦‚Zoomã€å…¬å¸3æ¥¼ä¼šè®®å®¤ç­‰ï¼‰\n",
        "ä¼šè®®ç‰‡æ®µï¼š{transcript}\"\"\",\n",
        "        input_variables=[\"transcript\"],\n",
        "        partial_variables={\n",
        "            \"language\": \"ä¸­æ–‡\" if language.startswith('zh') else \"English\",\n",
        "            \"format_instructions\": parser.get_format_instructions()\n",
        "        }\n",
        "    )\n",
        "    return prompt | ChatOpenAI(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        temperature=0.3,\n",
        "        openai_api_key=OPENAI_API_KEY\n",
        "    ) | parser\n",
        "\n",
        "def get_full_analysis_chain(language):\n",
        "    parser = JsonOutputParser(pydantic_object=FullMeetingAnalysis)\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"åŸºäºä»¥ä¸‹å„ç‰‡æ®µåˆ†æï¼Œç”Ÿæˆå®Œæ•´ä¼šè®®æŠ¥å‘Šï¼ˆç”¨{language}ï¼‰ï¼š\n",
        "{format_instructions}\n",
        "æ³¨æ„ï¼š\n",
        "- key_pointsï¼šä»…åŒ…å«å®¢è§‚äº‹å®ï¼Œä¸åŒ…å«ç»“è®º\n",
        "- decisionsï¼šå¿…é¡»æ˜¯æ˜ç¡®è¾¾æˆçš„ç»“è®ºï¼ˆæœ‰å…·ä½“ç»“æœï¼‰\n",
        "- concernsï¼šå¿…é¡»æ˜¯æå‡ºçš„é—®é¢˜æˆ–é£é™©ï¼ˆæœªè§£å†³çš„æ‹…å¿§ï¼‰\n",
        "- platformï¼šæ˜ç¡®ä¼šè®®å‘ç”Ÿçš„å¹³å°æˆ–åœºæ‰€ï¼ˆå¦‚Zoomã€Teamsã€æ€»éƒ¨ä¼šè®®å®¤ç­‰ï¼‰\n",
        "- meeting_typeï¼šæ˜ç¡®ä¼šè®®ç±»å‹ï¼ˆå¦‚å‘¨ä¾‹ä¼šã€é¡¹ç›®å¯åŠ¨ä¼šã€è¯„å®¡ä¼šç­‰ï¼‰\n",
        "ç‰‡æ®µåˆ†æï¼š{chunk_analyses}\"\"\",\n",
        "        input_variables=[\"chunk_analyses\"],\n",
        "        partial_variables={\n",
        "            \"language\": \"ä¸­æ–‡\" if language.startswith('zh') else \"English\",\n",
        "            \"format_instructions\": parser.get_format_instructions()\n",
        "        }\n",
        "    )\n",
        "    return prompt | ChatOpenAI(\n",
        "        model=\"gpt-4\",\n",
        "        temperature=0.3,\n",
        "        openai_api_key=OPENAI_API_KEY\n",
        "    ) | parser\n",
        "\n",
        "def analyze_meeting(transcript_segments, language='en'):\n",
        "    logger.log_step(\"ä¼šè®®åˆ†æ\", \"started\")\n",
        "    print(\"\\nå¼€å§‹åˆ†æä¼šè®®å†…å®¹...\")\n",
        "\n",
        "    try:\n",
        "        ANALYSIS_CHUNK_DURATION = 2700\n",
        "        analysis_chunks = []\n",
        "        current_chunk = []\n",
        "\n",
        "        for seg in transcript_segments:\n",
        "            if not current_chunk:\n",
        "                current_chunk.append(seg)\n",
        "            else:\n",
        "                if seg[\"end\"] - current_chunk[0][\"start\"] <= ANALYSIS_CHUNK_DURATION:\n",
        "                    current_chunk.append(seg)\n",
        "                else:\n",
        "                    analysis_chunks.append(current_chunk)\n",
        "                    current_chunk = [seg]\n",
        "        if current_chunk:\n",
        "            analysis_chunks.append(current_chunk)\n",
        "\n",
        "        print(f\"ğŸ“ å°†ä¼šè®®å†…å®¹åˆ†ä¸º {len(analysis_chunks)} ä¸ªåˆ†æå—\")\n",
        "\n",
        "        chunk_analyses = []\n",
        "        chunk_chain = get_chunk_analysis_chain(language)\n",
        "\n",
        "        for i, chunk in enumerate(tqdm(analysis_chunks, desc=\"åˆ†æè¿›åº¦\")):\n",
        "            chunk_text = \"\\n\".join([\n",
        "                f\"[{str(datetime.timedelta(seconds=int(seg['start'])))}] {seg['text']}\"\n",
        "                for seg in chunk\n",
        "            ])\n",
        "\n",
        "            try:\n",
        "                analysis = chunk_chain.invoke({\"transcript\": chunk_text[:12000]})\n",
        "                chunk_analyses.append({\n",
        "                    \"chunk_id\": i,\n",
        "                    \"start_time\": chunk[0][\"start\"],\n",
        "                    \"end_time\": chunk[-1][\"end\"],\n",
        "                    \"analysis\": analysis\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ åˆ†æå— {i} å¤±è´¥: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        if not chunk_analyses:\n",
        "            raise RuntimeError(\"æ‰€æœ‰åˆ†æå—å¤„ç†å¤±è´¥ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š\")\n",
        "\n",
        "        full_chain = get_full_analysis_chain(language)\n",
        "        full_analysis = full_chain.invoke({\n",
        "            \"chunk_analyses\": json.dumps([\n",
        "                {\n",
        "                    \"æ—¶é—´æ®µ\": f\"{str(datetime.timedelta(seconds=int(c['start_time'])))} - {str(datetime.timedelta(seconds=int(c['end_time'])))}\",\n",
        "                    \"åˆ†æ\": c[\"analysis\"]\n",
        "                } for c in chunk_analyses\n",
        "            ], ensure_ascii=False)\n",
        "        })\n",
        "\n",
        "        if hasattr(full_analysis, 'dict'):\n",
        "            full_analysis = full_analysis.dict()\n",
        "\n",
        "        full_analysis[\"language\"] = language\n",
        "        full_analysis[\"date\"] = datetime.datetime.now().isoformat()\n",
        "\n",
        "        logger.log_step(\"ä¼šè®®åˆ†æ\", \"success\")\n",
        "        print(f\"âœ… ä¼šè®®åˆ†æå®Œæˆï¼ˆ{len(full_analysis.get('topics_flow', []))}ä¸ªè¯é¢˜ï¼Œ{len(full_analysis.get('action_items', []))}ä¸ªè¡ŒåŠ¨é¡¹ï¼‰\")\n",
        "        return full_analysis\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"ä¼šè®®åˆ†æå¤±è´¥: {str(e)}\"\n",
        "        logger.log_step(\"ä¼šè®®åˆ†æ\", \"failed\", error=error_msg)\n",
        "        print(f\"âŒ {error_msg}\")\n",
        "        return {\"error\": error_msg}\n",
        "\n",
        "# ======================\n",
        "# Notionæ“ä½œ\n",
        "# ======================\n",
        "def create_notion_report(meeting_data, transcript_segments):\n",
        "    logger.log_step(\"åˆ›å»ºNotionæŠ¥å‘Š\", \"started\")\n",
        "\n",
        "    try:\n",
        "        try:\n",
        "            parent_page = notion.pages.retrieve(NOTION_PAGE_ID)\n",
        "            parent_title = parent_page.get('properties', {}).get('title', {}).get('title', [{}])[0].get('plain_text', 'æ— æ ‡é¢˜é¡µé¢')\n",
        "            print(f\"âœ… æˆåŠŸè®¿é—®çˆ¶é¡µé¢: {parent_title}\")\n",
        "        except errors.APIResponseError as e:\n",
        "            raise RuntimeError(f\"Notionçˆ¶é¡µé¢è®¿é—®å¤±è´¥: {str(e)}\")\n",
        "\n",
        "        new_page = notion.pages.create(\n",
        "            parent={\"page_id\": NOTION_PAGE_ID},\n",
        "            properties={\n",
        "                \"title\": {\n",
        "                    \"title\": [{\"text\": {\"content\": meeting_data.get(\"meeting_title\", \"ä¼šè®®æŠ¥å‘Š\")[:200]}}]\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "        page_id = new_page[\"id\"]\n",
        "\n",
        "        # æ„å»ºæŠ¥å‘Šå†…å®¹å—\n",
        "        children_blocks = []\n",
        "\n",
        "        # 1. ä¼šè®®æ¦‚è§ˆ\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"ä¼šè®®æ¦‚è§ˆ\"}}]}\n",
        "        })\n",
        "        overview_text = f\"\"\"\n",
        "        **æ ‡é¢˜**: {meeting_data.get('meeting_title', 'æœªå‘½åä¼šè®®')}\n",
        "        **æ—¥æœŸ**: {meeting_data.get('date', datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'))}\n",
        "        **å‚ä¸è€…**: {', '.join(meeting_data.get('participants', [])) or 'æœªè¯†åˆ«'}\n",
        "        **ç±»å‹**: {meeting_data.get('meeting_type', 'æœªæŒ‡å®š')}\n",
        "        **å¹³å°**: {meeting_data.get('platform', 'æœªæŒ‡å®š')}\n",
        "        **è¯­è¨€**: {', '.join(meeting_data.get('all_languages', [])) or 'æœªçŸ¥'}\n",
        "        \"\"\"\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"paragraph\",\n",
        "            \"paragraph\": {\"rich_text\": [{\"text\": {\"content\": overview_text.strip()}}]}\n",
        "        })\n",
        "\n",
        "        # 2. è¯é¢˜æµè½¬\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"è¯é¢˜æµè½¬é¡ºåº\"}}]}\n",
        "        })\n",
        "        max_topics = 20\n",
        "        for topic in meeting_data.get('topics_flow', [])[:max_topics]:\n",
        "            children_blocks.append({\n",
        "                \"object\": \"block\",\n",
        "                \"type\": \"numbered_list_item\",\n",
        "                \"numbered_list_item\": {\"rich_text\": [{\"text\": {\"content\": topic}}]}\n",
        "            })\n",
        "\n",
        "        # 3. ä¼šè®®æ€»ç»“\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"ä¼šè®®æ€»ç»“\"}}]}\n",
        "        })\n",
        "        max_summary_paras = 5\n",
        "        for para in meeting_data.get('summary', '').split('\\n\\n')[:max_summary_paras]:\n",
        "            if para.strip():\n",
        "                children_blocks.append({\n",
        "                    \"object\": \"block\",\n",
        "                    \"type\": \"paragraph\",\n",
        "                    \"paragraph\": {\"rich_text\": [{\"text\": {\"content\": para.strip()}}]}\n",
        "                })\n",
        "\n",
        "        # 4. å…³é”®è¦ç‚¹\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"å…³é”®è¦ç‚¹\"}}]}\n",
        "        })\n",
        "        key_points = meeting_data.get('key_points', {})\n",
        "        max_key_topics = 10\n",
        "        for topic, points in list(key_points.items())[:max_key_topics]:\n",
        "            children_blocks.append({\n",
        "                \"object\": \"block\",\n",
        "                \"type\": \"heading_3\",\n",
        "                \"heading_3\": {\"rich_text\": [{\"text\": {\"content\": topic}}]}\n",
        "            })\n",
        "            max_points_per_topic = 5\n",
        "            for point in points[:max_points_per_topic]:\n",
        "                children_blocks.append({\n",
        "                    \"object\": \"block\",\n",
        "                    \"type\": \"bulleted_list_item\",\n",
        "                    \"bulleted_list_item\": {\"rich_text\": [{\"text\": {\"content\": point}}]}\n",
        "                })\n",
        "\n",
        "        # 5. è¡ŒåŠ¨é¡¹è¡¨æ ¼\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"è¡ŒåŠ¨é¡¹\"}}]}\n",
        "        })\n",
        "        table_rows = []\n",
        "        max_actions = 20\n",
        "        for idx, item in enumerate(meeting_data.get('action_items', [])[:max_actions]):\n",
        "            table_rows.append([\n",
        "                [{\"text\": {\"content\": str(idx+1)}}],\n",
        "                [{\"text\": {\"content\": item.get('task', '')}}],\n",
        "                [{\"text\": {\"content\": item.get('assignee', 'æœªåˆ†é…')}}],\n",
        "                [{\"text\": {\"content\": item.get('due_date', 'æ— ')}}]\n",
        "            ])\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"table\",\n",
        "            \"table\": {\n",
        "                \"table_width\": 4,\n",
        "                \"has_column_header\": True,\n",
        "                \"children\": [\n",
        "                    {\n",
        "                        \"object\": \"block\",\n",
        "                        \"type\": \"table_row\",\n",
        "                        \"table_row\": {\n",
        "                            \"cells\": [\n",
        "                                [{\"text\": {\"content\": \"åºå·\"}}],\n",
        "                                [{\"text\": {\"content\": \"ä»»åŠ¡\"}}],\n",
        "                                [{\"text\": {\"content\": \"è´Ÿè´£äºº\"}}],\n",
        "                                [{\"text\": {\"content\": \"æˆªæ­¢æ—¥æœŸ\"}}]\n",
        "                            ]\n",
        "                        }\n",
        "                    },\n",
        "                    *[{\n",
        "                        \"object\": \"block\",\n",
        "                        \"type\": \"table_row\",\n",
        "                        \"table_row\": {\"cells\": cells}\n",
        "                    } for cells in table_rows]\n",
        "                ]\n",
        "            }\n",
        "        })\n",
        "\n",
        "        # 6. è½¬å½•æ–‡æœ¬\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": f\"ä¼šè®®è½¬å½•æ–‡æœ¬ï¼ˆèŠ‚é€‰ï¼Œå…±{len(transcript_segments)}æ¡ï¼‰\"}}]}\n",
        "        })\n",
        "        for seg in transcript_segments[:MAX_TRANSCRIPT_SEGMENTS]:\n",
        "            time_str = str(datetime.timedelta(seconds=int(seg[\"start\"])))\n",
        "            children_blocks.append({\n",
        "                \"object\": \"block\",\n",
        "                \"type\": \"paragraph\",\n",
        "                \"paragraph\": {\"rich_text\": [{\"text\": {\"content\": f\"[{time_str}] {seg['text']}\"}}]}\n",
        "            })\n",
        "\n",
        "        # æ£€æŸ¥æ€»å—æ•°\n",
        "        if len(children_blocks) > 100:\n",
        "            children_blocks = children_blocks[:100]\n",
        "            print(f\"âš ï¸ è­¦å‘Šï¼šå†…å®¹å—æ•°é‡è¶…é™ï¼Œå·²æˆªæ–­ä¸º100ä¸ªå—\")\n",
        "\n",
        "        notion.blocks.children.append(block_id=page_id, children=children_blocks)\n",
        "        report_url = new_page.get(\"url\", \"\")\n",
        "\n",
        "        logger.log_step(\"åˆ›å»ºNotionæŠ¥å‘Š\", \"success\", {\"æŠ¥å‘ŠURL\": report_url, \"æ€»å—æ•°\": len(children_blocks)})\n",
        "        print(f\"âœ… NotionæŠ¥å‘Šå·²ç”Ÿæˆï¼ˆæ€»å—æ•°: {len(children_blocks)}ï¼‰\")\n",
        "        return report_url\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"åˆ›å»ºNotionæŠ¥å‘Š\", \"failed\", error=str(e))\n",
        "        print(f\"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# å†™å…¥Notionæ•°æ®åº“ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šåŒ¹é…æ–°è¡¨å¤´ï¼‰\n",
        "def write_to_notion_database(meeting_data, full_transcript, all_languages, total_seconds):\n",
        "    logger.log_step(\"å†™å…¥Notionæ•°æ®åº“\", \"started\")\n",
        "\n",
        "    try:\n",
        "        # è¾…åŠ©å‡½æ•°ï¼šæˆªæ–­æ–‡æœ¬åˆ°Notionå…è®¸çš„æœ€å¤§é•¿åº¦\n",
        "        def truncate_text(text, max_length):\n",
        "            if text and len(text) > max_length:\n",
        "                return text[:max_length-3] + \"...\"  # é¢„ç•™3ä¸ªå­—ç¬¦ç»™çœç•¥å·\n",
        "            return text or \"\"\n",
        "\n",
        "        # è¾…åŠ©å‡½æ•°ï¼šè½¬æ¢ç§’æ•°ä¸º\"xå°æ—¶xåˆ†é’Ÿ\"æ ¼å¼ï¼ˆä¸è¶³1åˆ†é’ŸæŒ‰1åˆ†é’Ÿç®—ï¼‰\n",
        "        def format_duration(seconds):\n",
        "            hours = seconds // 3600\n",
        "            remaining_seconds = seconds % 3600\n",
        "            minutes = (remaining_seconds + 59) // 60  # å‘ä¸Šå–æ•´\n",
        "            return f\"{hours}å°æ—¶{minutes}åˆ†é’Ÿ\"\n",
        "\n",
        "        # è¾…åŠ©å‡½æ•°ï¼šè½¬æ¢ç§’æ•°ä¸ºæ€»åˆ†é’Ÿæ•°ï¼ˆç”¨äºæ•°å­—ç±»å‹çš„Durationå­—æ®µï¼‰\n",
        "        def get_total_minutes(seconds):\n",
        "            return (seconds + 59) // 60  # å‘ä¸Šå–æ•´\n",
        "\n",
        "        # 1. å‡†å¤‡æ•°æ®åº“å­—æ®µå†…å®¹ï¼ˆä¸¥æ ¼åŒ¹é…è¡¨å¤´ï¼‰\n",
        "        database_properties = {\n",
        "            # ä¼šè®®æ ‡é¢˜ï¼ˆæ–‡æœ¬ç±»å‹ï¼‰\n",
        "            \"Meeting Title\": {\"title\": [{\"text\": {\"content\": meeting_data.get(\"meeting_title\", \"Untitled\")}}]},\n",
        "            # å‚ä¸è€…ï¼ˆæ–‡æœ¬ç±»å‹ï¼‰\n",
        "            \"Participant\": {\n",
        "                \"rich_text\": [{\"text\": {\"content\": truncate_text(\n",
        "                    \", \".join(meeting_data.get(\"participant\", [])),\n",
        "                    NOTION_RICH_TEXT_LIMIT\n",
        "                )}}]\n",
        "            },\n",
        "            # æ—¥æœŸï¼ˆæ—¥æœŸç±»å‹ï¼Œæ ¼å¼xxxxå¹´xxæœˆxxæ—¥ï¼‰\n",
        "            \"Date\": {\n",
        "                \"date\": {\n",
        "                    \"start\": datetime.datetime.now().strftime(\"%Y-%m-%d\"),  # Notionæ—¥æœŸæ ¼å¼éœ€ä¸ºISO\n",
        "                    \"end\": None\n",
        "                }\n",
        "            },\n",
        "            # æ—¶é•¿ï¼ˆæ•°å­—ç±»å‹ï¼Œå­˜å‚¨æ€»åˆ†é’Ÿæ•°ï¼‰\n",
        "            \"Duration\": {\n",
        "                \"number\": get_total_minutes(total_seconds)\n",
        "            },\n",
        "            # ä¼šè®®ç±»å‹ï¼ˆæ–‡æœ¬ç±»å‹ï¼‰\n",
        "            \"Meeting Type\": {\n",
        "                \"rich_text\": [{\"text\": {\"content\": truncate_text(\n",
        "                    meeting_data.get(\"meeting_type\", \"æœªæŒ‡å®š\"),\n",
        "                    NOTION_RICH_TEXT_LIMIT\n",
        "                )}}]\n",
        "            },\n",
        "            # ä¼šè®®å¹³å°ï¼ˆé€‰æ‹©ç±»å‹ï¼Œç¡®ä¿å€¼åœ¨é¢„è®¾é€‰é¡¹ä¸­ï¼‰\n",
        "            \"Platform\": {\"select\": {\"name\": meeting_data.get(\"platform\", \"Unknown\")}},\n",
        "            # ä¼šè®®å®Œæ•´åŸæ–‡ï¼ˆæ–‡æœ¬ç±»å‹ï¼‰\n",
        "            \"original meeting script\": {\n",
        "                \"rich_text\": [{\"text\": {\"content\": truncate_text(\n",
        "                    full_transcript,\n",
        "                    NOTION_RICH_TEXT_LIMIT\n",
        "                )}}]\n",
        "            },\n",
        "            # ä¼šè®®è¯­è¨€ï¼ˆæ–‡æœ¬ç±»å‹ï¼‰\n",
        "            \"language\": {\n",
        "                \"rich_text\": [{\"text\": {\"content\": truncate_text(\n",
        "                    \", \".join(all_languages),\n",
        "                    NOTION_RICH_TEXT_LIMIT\n",
        "                )}}]\n",
        "            },\n",
        "            # ä¼šè®®æ€»ç»“ï¼ˆæ–‡æœ¬ç±»å‹ï¼‰\n",
        "            \"Summary\": {\n",
        "                \"rich_text\": [{\"text\": {\"content\": truncate_text(\n",
        "                    meeting_data.get(\"summary\", \"\"),\n",
        "                    NOTION_RICH_TEXT_LIMIT\n",
        "                )}}]\n",
        "            },\n",
        "            # ä¼šè®®å†³å®šï¼ˆæ–‡æœ¬ç±»å‹ï¼‰\n",
        "            \"Decisions\": {\n",
        "                \"rich_text\": [{\"text\": {\"content\": truncate_text(\n",
        "                    \"\\n\".join([f\"- {d}\" for d in meeting_data.get(\"decisions\", [])]),\n",
        "                    NOTION_RICH_TEXT_LIMIT\n",
        "                )}}]\n",
        "            },\n",
        "            # ä¼šè®®æ‹…å¿§ï¼ˆæ–‡æœ¬ç±»å‹ï¼‰\n",
        "            \"Concerns\": {\n",
        "                \"rich_text\": [{\"text\": {\"content\": truncate_text(\n",
        "                    \"\\n\".join([f\"- {c}\" for c in meeting_data.get(\"concerns\", [])]),\n",
        "                    NOTION_RICH_TEXT_LIMIT\n",
        "                )}}]\n",
        "            },\n",
        "            # ä¼šè®®è¦ç‚¹ï¼ˆæ–‡æœ¬ç±»å‹ï¼‰\n",
        "            \"Key Points\": {\n",
        "                \"rich_text\": [{\"text\": {\"content\": truncate_text(\n",
        "                    \"\\n\".join([f\"- {topic}: {', '.join(points)}\"\n",
        "                              for topic, points in meeting_data.get(\"key_points\", {}).items()]),\n",
        "                    NOTION_RICH_TEXT_LIMIT\n",
        "                )}}]\n",
        "            },\n",
        "            # è¡ŒåŠ¨é¡¹ï¼ˆæ–‡æœ¬ç±»å‹ï¼‰\n",
        "            \"Action Items\": {\n",
        "                \"rich_text\": [{\"text\": {\"content\": truncate_text(\n",
        "                    \"\\n\".join([f\"- {item.get('task', '')}ï¼ˆè´Ÿè´£äººï¼š{item.get('assignee', 'æœªåˆ†é…')}ï¼‰\"\n",
        "                              for item in meeting_data.get(\"action_items\", [])]),\n",
        "                    NOTION_RICH_TEXT_LIMIT\n",
        "                )}}]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # 2. å†™å…¥æ•°æ®åº“\n",
        "        new_db_page = notion.pages.create(\n",
        "            parent={\"database_id\": NOTION_DB_ID},\n",
        "            properties=database_properties\n",
        "        )\n",
        "\n",
        "        logger.log_step(\"å†™å…¥Notionæ•°æ®åº“\", \"success\", {\"æ•°æ®åº“é¡µé¢ID\": new_db_page[\"id\"]})\n",
        "        print(f\"âœ… æˆåŠŸå†™å…¥Notionæ•°æ®åº“ï¼ˆæ¡ç›®ID: {new_db_page['id']}ï¼‰\")\n",
        "        print(f\"â±ï¸ ä¼šè®®æ—¶é•¿: {format_duration(total_seconds)}\")\n",
        "        return new_db_page[\"id\"]\n",
        "\n",
        "    except errors.APIResponseError as e:\n",
        "        error_msg = f\"æ•°æ®åº“å†™å…¥å¤±è´¥: {str(e)}\"\n",
        "        logger.log_step(\"å†™å…¥Notionæ•°æ®åº“\", \"failed\", error=error_msg)\n",
        "        print(f\"âŒ {error_msg}ï¼ˆè¯·æ£€æŸ¥æ•°æ®åº“è¡¨å¤´åç§°å’Œç±»å‹æ˜¯å¦ä¸ä»£ç ä¸€è‡´ï¼‰\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        error_msg = f\"æ•°æ®åº“å†™å…¥å¤±è´¥: {str(e)}\"\n",
        "        logger.log_step(\"å†™å…¥Notionæ•°æ®åº“\", \"failed\", error=error_msg)\n",
        "        print(f\"âŒ {error_msg}\")\n",
        "        return None\n",
        "\n",
        "# ======================\n",
        "# è¾“å…¥å¤„ç†\n",
        "# ======================\n",
        "def handle_input():\n",
        "    print(\"\\n=== è¯·é€‰æ‹©è¾“å…¥æ–¹å¼ ===\")\n",
        "    print(\"1: ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ (.mp3/.wav/.m4a/.opus)\")\n",
        "    print(\"2: ä¸Šä¼ å¸¦æ—¶é—´æˆ³çš„è½¬å½•æ–‡æœ¬ (.txt)\")\n",
        "\n",
        "    try:\n",
        "        choice = input(\"è¯·é€‰æ‹© (1/2): \").strip() or \"1\"\n",
        "    except:\n",
        "        choice = \"1\"\n",
        "\n",
        "    if choice == \"1\":\n",
        "        print(\"\\nè¯·ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ï¼ˆæ”¯æŒ2-3å°æ—¶ä¼šè®®å½•éŸ³ï¼‰:\")\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            print(\"âš ï¸ æœªæ£€æµ‹åˆ°ä¸Šä¼ æ–‡ä»¶ï¼Œè¯·é‡è¯•\")\n",
        "            return handle_input()\n",
        "\n",
        "        filename = next(iter(uploaded.keys()))\n",
        "        audio_ext = os.path.splitext(filename)[1].lower()\n",
        "        supported_ext = ['.mp3', '.wav', '.m4a', '.opus']\n",
        "\n",
        "        if audio_ext not in supported_ext:\n",
        "            print(f\"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼ˆæ”¯æŒ: {', '.join(supported_ext)}ï¼‰\")\n",
        "            return handle_input()\n",
        "\n",
        "        audio_path = f\"/tmp/{filename}\"\n",
        "        with open(audio_path, 'wb') as f:\n",
        "            f.write(uploaded[filename])\n",
        "\n",
        "        duration = get_audio_duration(audio_path)\n",
        "        if duration < 3600:\n",
        "            print(f\"âš ï¸ æ£€æµ‹åˆ°éŸ³é¢‘æ—¶é•¿è¾ƒçŸ­ï¼ˆ{duration/60:.1f}åˆ†é’Ÿï¼‰\")\n",
        "            if input(\"æ˜¯å¦ç»§ç»­ä½¿ç”¨é•¿ä¼šè®®æ¨¡å¼å¤„ç†? (y/n): \").strip().lower() != 'y':\n",
        "                os.unlink(audio_path)\n",
        "                print(\"å·²åˆ‡æ¢åˆ°æ™®é€šæ¨¡å¼\")\n",
        "                return handle_input()\n",
        "\n",
        "        print(f\"ğŸµ éŸ³é¢‘ä¿¡æ¯: {filename}ï¼ˆ{duration/60:.1f}åˆ†é’Ÿï¼‰\")\n",
        "\n",
        "        print(\"\\nâš¡ è¯·é€‰æ‹©è½¬å½•æ¨¡å‹:\")\n",
        "        print(\"1: base.en - å¿«é€Ÿæ¨¡å¼ï¼ˆé€‚åˆæ¸…æ™°è¯­éŸ³ï¼‰\")\n",
        "        print(\"2: small.en - å¹³è¡¡æ¨¡å¼ï¼ˆæ¨èï¼Œé€Ÿåº¦ä¸ç²¾åº¦å…¼é¡¾ï¼‰\")\n",
        "        print(\"3: medium.en - é«˜ç²¾åº¦æ¨¡å¼ï¼ˆé€‚åˆå¤æ‚ä¼šè®®ï¼‰\")\n",
        "\n",
        "        model_choice = input(\"è¯·é€‰æ‹© (1-3ï¼Œé»˜è®¤2): \").strip() or \"2\"\n",
        "        model_map = {\"1\": \"base.en\", \"2\": \"small.en\", \"3\": \"medium.en\"}\n",
        "        model_size = model_map.get(model_choice, \"small.en\")\n",
        "        print(f\"å°†ä½¿ç”¨ {model_size} æ¨¡å‹è¿›è¡Œè½¬å½•\")\n",
        "\n",
        "        # æ–°å¢è¿”å›æ€»æ—¶é•¿ï¼ˆç§’ï¼‰\n",
        "        transcript, segments, primary_language, all_languages, total_seconds = transcribe_long_audio(audio_path, model_size)\n",
        "        return transcript, segments, primary_language, all_languages, total_seconds\n",
        "\n",
        "    elif choice == \"2\":\n",
        "        print(\"\\nè¯·ä¸Šä¼ å¸¦æ—¶é—´æˆ³çš„è½¬å½•æ–‡æœ¬ï¼ˆæ ¼å¼ç¤ºä¾‹: [00:05:10] å‘è¨€äºº: ...ï¼‰:\")\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            print(\"âš ï¸ æœªæ£€æµ‹åˆ°ä¸Šä¼ æ–‡ä»¶ï¼Œè¯·é‡è¯•\")\n",
        "            return handle_input()\n",
        "\n",
        "        filename = next(iter(uploaded.keys()))\n",
        "        if not filename.endswith('.txt'):\n",
        "            print(\"âŒ ä»…æ”¯æŒ.txtæ ¼å¼çš„æ–‡æœ¬æ–‡ä»¶\")\n",
        "            return handle_input()\n",
        "\n",
        "        try:\n",
        "            transcript_text = uploaded[filename].decode('utf-8')\n",
        "            segments = []\n",
        "            time_pattern = re.compile(r'\\[(\\d+:\\d+:\\d+)\\]')\n",
        "\n",
        "            for line in transcript_text.split('\\n'):\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                match = time_pattern.search(line)\n",
        "                if match:\n",
        "                    time_str = match.group(1)\n",
        "                    text = time_pattern.sub('', line).strip()\n",
        "                    h, m, s = map(int, time_str.split(':'))\n",
        "                    start_time = h * 3600 + m * 60 + s\n",
        "                    segments.append({\n",
        "                        \"text\": text,\n",
        "                        \"start\": start_time,\n",
        "                        \"end\": start_time + 30,\n",
        "                        \"language\": detect(text) if text.strip() else \"en\"\n",
        "                    })\n",
        "\n",
        "            if not segments:\n",
        "                raise ValueError(\"æœªæ£€æµ‹åˆ°æœ‰æ•ˆæ—¶é—´æˆ³ï¼Œè¯·æ£€æŸ¥æ–‡æœ¬æ ¼å¼\")\n",
        "\n",
        "            # è®¡ç®—æ€»æ—¶é•¿ï¼ˆç§’ï¼‰\n",
        "            total_seconds = segments[-1][\"end\"] - segments[0][\"start\"] if segments else 0\n",
        "\n",
        "            all_languages = list({seg[\"language\"] for seg in segments})\n",
        "            primary_language = all_languages[0] if all_languages else \"en\"\n",
        "            print(f\"âœ… å·²è§£æè½¬å½•æ–‡æœ¬ï¼ˆ{len(segments)}ä¸ªç‰‡æ®µï¼Œæ£€æµ‹è¯­è¨€: {all_languages}ï¼‰\")\n",
        "            return transcript_text, segments, primary_language, all_languages, total_seconds\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ æ–‡æœ¬è§£æå¤±è´¥: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    else:\n",
        "        print(\"âš ï¸ æ— æ•ˆé€‰æ‹©ï¼Œé»˜è®¤ä½¿ç”¨éŸ³é¢‘è¾“å…¥\")\n",
        "        return handle_input()\n",
        "\n",
        "# ======================\n",
        "# ä¸»å‡½æ•°\n",
        "# ======================\n",
        "def main():\n",
        "    print(\"=== ä¼šè®®åˆ†æå·¥å…· ===\")\n",
        "    logger.log_step(\"ä¼šè®®å¤„ç†æµç¨‹\", \"started\")\n",
        "\n",
        "    try:\n",
        "        if not torch.cuda.is_available():\n",
        "            raise RuntimeError(\"è¯·åˆ‡æ¢åˆ°GPUç¯å¢ƒï¼Œè¯·åˆ‡æ¢åˆ°GPUç¯å¢ƒï¼ˆRuntime > Change runtime type > é€‰æ‹©GPUï¼‰\")\n",
        "        print(f\"âœ… æ£€æµ‹åˆ°GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "        # å¤„ç†è¾“å…¥ï¼ˆè·å–æ€»æ—¶é•¿ï¼‰\n",
        "        transcript, segments, language, all_languages, total_seconds = handle_input()\n",
        "        if not transcript or not segments:\n",
        "            raise RuntimeError(\"æœªè·å–åˆ°æœ‰æ•ˆä¼šè®®å†…å®¹\")\n",
        "\n",
        "        # åˆ†æä¼šè®®\n",
        "        meeting_data = analyze_meeting(segments, language)\n",
        "        if \"error\" in meeting_data:\n",
        "            raise RuntimeError(meeting_data[\"error\"])\n",
        "\n",
        "        # è¡¥å……è¯­è¨€ä¿¡æ¯åˆ°ä¼šè®®æ•°æ®\n",
        "        meeting_data[\"all_languages\"] = all_languages\n",
        "\n",
        "        # ç”ŸæˆNotionæŠ¥å‘Š\n",
        "        report_url = create_notion_report(meeting_data, segments)\n",
        "        if not report_url:\n",
        "            raise RuntimeError(\"æ— æ³•ç”ŸæˆNotionæŠ¥å‘Š\")\n",
        "\n",
        "        # å†™å…¥æ•°æ®åº“\n",
        "        db_entry_id = write_to_notion_database(meeting_data, transcript, all_languages, total_seconds)\n",
        "        if not db_entry_id:\n",
        "            raise RuntimeError(\"æ•°æ®åº“å†™å…¥å¤±è´¥ï¼Œä½†æŠ¥å‘Šå·²ç”Ÿæˆ\")\n",
        "\n",
        "        # ä¿å­˜æ—¥å¿—å¹¶è¾“å‡ºç»“æœ\n",
        "        log_file = logger.save_logs()\n",
        "        print(f\"\\nğŸ‰ ä¼šè®®å¤„ç†å®Œæˆï¼\")\n",
        "        print(f\"ğŸ“„ ä¼šè®®æŠ¥å‘Š: {report_url}\")\n",
        "        print(f\"ğŸ“Š æ•°æ®åº“æ¡ç›®ID: {db_entry_id}\")\n",
        "        print(f\"ğŸ“‹ å¤„ç†æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}\")\n",
        "\n",
        "        from IPython.display import HTML\n",
        "        display(HTML(f'<a href=\"{report_url}\" target=\"_blank\">ç‚¹å‡»æŸ¥çœ‹Notionä¼šè®®æŠ¥å‘Š</a>'))\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.save_logs(\"meeting_error_logs.json\")\n",
        "        print(f\"\\nâŒ å¤„ç†å¤±è´¥: {str(e)}\")\n",
        "        print(\"é”™è¯¯è¯¦æƒ…å·²ä¿å­˜åˆ° meeting_error_logs.json\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "DnI0aBLIl8Mf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOLBE8pAfqJWAVmi7incS8J",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}