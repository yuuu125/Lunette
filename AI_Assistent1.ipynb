{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuuu125/Lunette/blob/main/AI_Assistent1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qq1cOplyCeys",
        "outputId": "0fbf4c40-78da-4f57-f41f-8de9f4108d3b",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28.1\n",
            "  Downloading openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: notion-client in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28.1) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28.1) (3.11.15)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Requirement already satisfied: httpx>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from notion-client) (0.27.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.15.0->notion-client) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.15.0->notion-client) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.15.0->notion-client) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.15.0->notion-client) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.15.0->notion-client) (1.3.1)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.15.0->notion-client) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28.1) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28.1) (2.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (1.20.1)\n",
            "Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.37.0\n",
            "    Uninstalling openai-1.37.0:\n",
            "      Successfully uninstalled openai-1.37.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-openai 0.1.9 requires openai<2.0.0,>=1.26.0, but you have openai 0.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "openai"
                ]
              },
              "id": "b634d9af59154da7badf30a5a3d8b596"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-c6cshuv1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-c6cshuv1\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "0% [Waiting for headers] [Connected to ppa.launchpadcontent.net (185.125.190.80\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.28.1 python-docx notion-client langdetect pydub\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg  # ç¡®ä¿å®‰è£…å¿…è¦çš„ä¾èµ–\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import openai\n",
        "import whisper\n",
        "from docx import Document\n",
        "from google.colab import files, userdata\n",
        "from notion_client import Client\n",
        "from langdetect import detect, LangDetectException\n",
        "import datetime\n",
        "import tempfile\n",
        "import torch\n",
        "from pydub import AudioSegment\n",
        "import subprocess\n",
        "\n",
        "try:\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    NOTION_TOKEN = userdata.get('NOTION_TOKEN')\n",
        "    NOTION_DB_ID = userdata.get('NOTION_DB_ID')\n",
        "\n",
        "    if not OPENAI_API_KEY:\n",
        "        raise ValueError(\"OPENAI_API_KEY not set\")\n",
        "    if not NOTION_TOKEN:\n",
        "        print(\"âš ï¸ Notion token missing - feature disabled\")\n",
        "    if not NOTION_DB_ID:\n",
        "        print(\"âš ï¸ Notion DB ID missing - feature disabled\")\n",
        "\n",
        "    openai.api_key = OPENAI_API_KEY\n",
        "    print(\"âœ… OpenAI API key set\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Key retrieval failed: {str(e)}\")\n",
        "\n",
        "def get_audio_duration(audio_path):\n",
        "    \"\"\"ä½¿ç”¨ffmpegè·å–ç²¾ç¡®éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"format=duration\",\n",
        "             \"-of\", \"default=noprint_wrappers=1:nokey=1\", audio_path],\n",
        "            capture_output=True, text=True\n",
        "        )\n",
        "        return float(result.stdout)\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ æ— æ³•è·å–ç²¾ç¡®æ—¶é•¿ï¼Œä½¿ç”¨ä¼°ç®—å€¼: {e}\")\n",
        "        # ä¼°ç®—ï¼š8KB/s æ˜¯å¸¸è§éŸ³é¢‘æ¯”ç‰¹ç‡\n",
        "        return max(30, os.path.getsize(audio_path) // 8000)\n",
        "\n",
        "def transcribe_audio(audio_path, model_size=\"base\"):\n",
        "    \"\"\"ä½¿ç”¨Whisperè½¬å½•éŸ³é¢‘æ–‡ä»¶\"\"\"\n",
        "    print(f\"ğŸ”Š Starting transcription with Whisper ({model_size} model)...\")\n",
        "\n",
        "    try:\n",
        "        # æ£€æŸ¥GPUåŠ é€Ÿ\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"ğŸ’» Using device: {device.upper()}\")\n",
        "\n",
        "        # åŠ è½½æ¨¡å‹\n",
        "        model = whisper.load_model(model_size, device=device)\n",
        "        print(f\"âœ… Loaded Whisper {model_size} model\")\n",
        "\n",
        "        # è½¬å½•éŸ³é¢‘\n",
        "        result = model.transcribe(\n",
        "            audio_path,\n",
        "            fp16=(device == \"cuda\"),\n",
        "            verbose=True,\n",
        "            task=\"transcribe\"\n",
        "        )\n",
        "\n",
        "        transcription = result[\"text\"]\n",
        "        print(f\"âœ… Transcription complete! Characters: {len(transcription)}\")\n",
        "        return transcription\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Transcription failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def test_notion_connection():\n",
        "    \"\"\"æµ‹è¯•Notionè¿æ¥æ˜¯å¦æœ‰æ•ˆ\"\"\"\n",
        "    try:\n",
        "        # åˆå§‹åŒ–Notionå®¢æˆ·ç«¯ï¼Œæ˜¾å¼é…ç½®å®¢æˆ·ç«¯ç¦ç”¨ä»£ç†\n",
        "        notion = Client(\n",
        "        auth=NOTION_TOKEN,\n",
        "        client=httpx.Client(proxies=None)  # æ˜¾å¼ç¦ç”¨ä»£ç†\n",
        "        )\n",
        "        notion.databases.retrieve(database_id=NOTION_DB_ID)\n",
        "        print(\"âœ… Notion connection verified\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Notion connection failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def clean_transcript(text):\n",
        "    \"\"\"Cleans raw transcript text\"\"\"\n",
        "    text = re.sub(r'\\d{1,2}:\\d{2}:\\d{2}', '', text)\n",
        "    text = re.sub(r'Speaker\\s*\\d+:?', '', text)\n",
        "    return re.sub(r'\\n\\s*\\n', '\\n\\n', text).strip()\n",
        "\n",
        "def segment_text(text):\n",
        "    \"\"\"Segments text into paragraphs\"\"\"\n",
        "    return [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "\n",
        "def handle_transcript_input():\n",
        "    \"\"\"Handles transcript input methods\"\"\"\n",
        "    print(\"\\n=== Handling Transcript Input ===\")\n",
        "    print(\"Choose input method:\")\n",
        "    print(\"1 - Upload text file (.txt or .docx)\")\n",
        "    print(\"2 - Paste text directly\")\n",
        "    print(\"3 - Upload audio file (transcribe with Whisper)\")\n",
        "\n",
        "    input_method = input(\"Your choice (1/2/3): \")\n",
        "    transcript_text = \"\"\n",
        "\n",
        "    # æ–‡æœ¬æ–‡ä»¶ä¸Šä¼ \n",
        "    if input_method == \"1\":\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            print(\"âš ï¸ No files uploaded, switching to paste\")\n",
        "            transcript_text = input(\"Paste meeting transcript: \")\n",
        "        else:\n",
        "            filename = list(uploaded.keys())[0]\n",
        "            print(f\"âœ… Uploaded: {filename}\")\n",
        "\n",
        "            # æ–‡æœ¬æ–‡ä»¶å¤„ç†\n",
        "            if filename.endswith('.txt'):\n",
        "                transcript_text = uploaded[filename].decode('utf-8')\n",
        "\n",
        "            # DOCXå¤„ç†\n",
        "            elif filename.endswith('.docx'):\n",
        "                with tempfile.NamedTemporaryFile(delete=False, suffix='.docx') as tmp:\n",
        "                    tmp.write(uploaded[filename])\n",
        "                    tmp_path = tmp.name\n",
        "\n",
        "                doc = Document(tmp_path)\n",
        "                transcript_text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "                os.unlink(tmp_path)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "    # æ–‡æœ¬ç²˜è´´\n",
        "    elif input_method == \"2\":\n",
        "        transcript_text = input(\"Paste meeting transcript: \")\n",
        "\n",
        "    # éŸ³é¢‘æ–‡ä»¶å¤„ç†\n",
        "    elif input_method == \"3\":\n",
        "        uploaded_audio = files.upload()\n",
        "        if not uploaded_audio:\n",
        "            print(\"âš ï¸ No audio files uploaded, switching to text paste\")\n",
        "            transcript_text = input(\"Paste meeting transcript: \")\n",
        "        else:\n",
        "            filename = list(uploaded_audio.keys())[0]\n",
        "            print(f\"âœ… Uploaded audio: {filename}\")\n",
        "\n",
        "            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶\n",
        "            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(filename)[1]) as tmp:\n",
        "                tmp.write(uploaded_audio[filename])\n",
        "                audio_path = tmp.name\n",
        "\n",
        "            print(\"\\nâš¡ Select transcription speed:\")\n",
        "            print(\"1 - Fast (tiny model, fastest, lower accuracy)\")\n",
        "            print(\"2 - Balanced (base model, recommended)\")\n",
        "            print(\"3 - High Quality (small model, slower)\")\n",
        "\n",
        "            speed_choice = input(\"Your choice (1/2/3): \") or \"2\"\n",
        "            model_map = {\"1\": \"tiny\", \"2\": \"base\", \"3\": \"small\"}\n",
        "            model_size = model_map.get(speed_choice, \"base\")\n",
        "\n",
        "            # è·å–éŸ³é¢‘æ—¶é•¿\n",
        "            try:\n",
        "                duration = get_audio_duration(audio_path)\n",
        "                print(f\"â± Audio duration: {duration//60:.0f}m {duration%60:.0f}s\")\n",
        "\n",
        "                # æ—¶é—´ä¼°ç®—\n",
        "                time_estimates = {\"tiny\": 0.3, \"base\": 0.8, \"small\": 2.0}\n",
        "                est_sec = duration * time_estimates[model_size]\n",
        "                print(f\"â³ Estimated processing time: ~{est_sec//60:.0f}m {est_sec%60:.0f}s\")\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Duration estimation failed: {e}\")\n",
        "\n",
        "            # è½¬å½•éŸ³é¢‘\n",
        "            transcript_text = transcribe_audio(audio_path, model_size)\n",
        "\n",
        "            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
        "            os.unlink(audio_path)\n",
        "\n",
        "    else:\n",
        "        print(\"âš ï¸ Invalid option, defaulting to text paste\")\n",
        "        transcript_text = input(\"Paste meeting transcript: \")\n",
        "\n",
        "    cleaned_text = clean_transcript(transcript_text)\n",
        "    segments = segment_text(cleaned_text)\n",
        "\n",
        "    print(f\"ğŸ“ Processed text: {len(segments)} segments, {len(cleaned_text)} characters\")\n",
        "    return cleaned_text, segments\n",
        "\n",
        "def analyze_with_gpt(text, language='en'):\n",
        "    \"\"\"Analyzes text with GPT API\"\"\"\n",
        "    print(\"\\n=== Analyzing with GPT ===\")\n",
        "\n",
        "    if not openai.api_key:\n",
        "        print(\"âŒ OpenAI API key missing\")\n",
        "        return {\"error\": \"OpenAI API key not set\", \"fallback_used\": True}, 0\n",
        "\n",
        "    # Language mapping\n",
        "    lang_map = {'zh': 'Chinese', 'es': 'Spanish', 'fr': 'French', 'en': 'English'}\n",
        "    lang_name = lang_map.get(language[:2], 'English')\n",
        "\n",
        "    # System prompt setup\n",
        "    system_prompt = f\"\"\"\n",
        "    You are a professional meeting analyst. Extract key information:\n",
        "    - Respond in {lang_name}\n",
        "    - Use this JSON format:\n",
        "    {{\n",
        "        \"meeting_title\": \"Meeting Title\",\n",
        "        \"participants\": [\"Attendee1\", \"Attendee2\"],\n",
        "        \"summary\": \"Meeting summary\",\n",
        "        \"action_items\": [{{\"task\": \"Task\", \"assignee\": \"Owner\"}}],\n",
        "        \"key_points\": {{\n",
        "            \"concerns\": [],\n",
        "            \"decisions\": [],\n",
        "            \"deadlines\": [],\n",
        "            \"updates\": []\n",
        "        }},\n",
        "        \"meeting_type\": \"Meeting type\",\n",
        "        \"platform\": \"Platform\",\n",
        "        \"fallback_used\": false\n",
        "    }}\n",
        "\n",
        "    Extraction rules:\n",
        "    1. meeting_title: Extract from start/end or generate\n",
        "    2. participants: Extract all attendees\n",
        "    3. Focus on meeting start/end sections\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt = f\"Meeting transcript:\\n{text[:10000]}\"\n",
        "\n",
        "    try:\n",
        "        # GPT API call\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message['content']\n",
        "        result = json.loads(content)\n",
        "        tokens_used = response.usage['total_tokens']\n",
        "\n",
        "        print(f\"âœ… GPT analysis complete! Tokens: {tokens_used}\")\n",
        "        print(f\"Meeting title: {result.get('meeting_title', 'N/A')}\")\n",
        "        print(f\"Participants: {len(result.get('participants', []))}\")\n",
        "        print(f\"Meeting type: {result.get('meeting_type', 'N/A')}\")\n",
        "        print(f\"Action items: {len(result.get('action_items', []))}\")\n",
        "\n",
        "        # Fallback for action items\n",
        "        if not result.get('action_items'):\n",
        "            result['fallback_used'] = True\n",
        "            print(\"âš ï¸ No action items detected\")\n",
        "\n",
        "        return result, tokens_used\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ GPT analysis failed: {str(e)}\")\n",
        "        return {\n",
        "            \"error\": str(e),\n",
        "            \"fallback_used\": True\n",
        "        }, 0\n",
        "\n",
        "def create_notion_entry(meeting_data):\n",
        "    \"\"\"Creates Notion database entry\"\"\"\n",
        "    if not NOTION_TOKEN or not NOTION_DB_ID:\n",
        "        print(\"âš ï¸ Notion config incomplete - skipping\")\n",
        "        return False\n",
        "\n",
        "    print(\"\\n=== Syncing to Notion ===\")\n",
        "\n",
        "    try:\n",
        "        # åˆå§‹åŒ–Notionå®¢æˆ·ç«¯ï¼Œæ˜¾å¼é…ç½®å®¢æˆ·ç«¯ç¦ç”¨ä»£ç†\n",
        "        notion = Client(\n",
        "        auth=NOTION_TOKEN,\n",
        "        client=httpx.Client(proxies=None)  # æ˜¾å¼ç¦ç”¨ä»£ç†\n",
        ")\n",
        "\n",
        "        # Prepare properties\n",
        "        properties = {\n",
        "            \"Meeting Title\": {\"title\": [{\"text\": {\"content\": meeting_data.get(\"meeting_title\", \"Untitled\")}}]},\n",
        "            \"Participant\": {\"rich_text\": [{\"text\": {\"content\": \", \".join(meeting_data.get(\"participants\", [\"Unknown\"]))}}]},\n",
        "            \"Date & Duration\": {\"date\": {\"start\": meeting_data.get(\"date\", datetime.datetime.now().isoformat())}},\n",
        "            \"Meeting Type\": {\"rich_text\": [{\"text\": {\"content\": meeting_data.get(\"meeting_type\", \"Other\")}}]},\n",
        "            \"Platform\": {\"select\": {\"name\": meeting_data.get(\"platform\", \"Unknown\")}},\n",
        "            \"Summary\": {\"rich_text\": [{\"text\": {\"content\": meeting_data.get(\"summary\", \"\")}}]},\n",
        "            \"Key Points\": {\"rich_text\": [{\"text\": {\"content\": format_key_points(meeting_data)}}]},\n",
        "            \"Action Items\": {\"rich_text\": [{\"text\": {\"content\": format_action_items(meeting_data)}}]},\n",
        "        }\n",
        "\n",
        "        # Create entry\n",
        "        new_page = notion.pages.create(\n",
        "            parent={\"database_id\": NOTION_DB_ID},\n",
        "            properties=properties\n",
        "        )\n",
        "\n",
        "        print(f\"âœ… Notion entry created! ID: {new_page['id']}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Notion sync failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def format_key_points(data):\n",
        "    \"\"\"Formats key points for Notion\"\"\"\n",
        "    points = []\n",
        "    key_points = data.get(\"key_points\", {})\n",
        "    for category, items in key_points.items():\n",
        "        if items and isinstance(items, list):\n",
        "            points.append(f\"{category.upper()}:\")\n",
        "            points.extend([f\"- {item}\" for item in items])\n",
        "    return \"\\n\".join(points)\n",
        "\n",
        "def format_action_items(data):\n",
        "    \"\"\"Formats action items for Notion\"\"\"\n",
        "    action_items = data.get(\"action_items\", [])\n",
        "    if not action_items or not isinstance(action_items, list):\n",
        "        return \"No action items\"\n",
        "\n",
        "    formatted = []\n",
        "    for item in action_items:\n",
        "        if isinstance(item, dict):\n",
        "            task = item.get('task', 'Unknown task')\n",
        "            assignee = item.get('assignee', 'Unassigned')\n",
        "            formatted.append(f\"- {task} (Owner: {assignee})\")\n",
        "        else:\n",
        "            formatted.append(f\"- {str(item)}\")\n",
        "    return \"\\n\".join(formatted)\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main workflow execution\"\"\"\n",
        "    if not openai.api_key:\n",
        "        print(\"âŒ OpenAI API key missing\")\n",
        "        return\n",
        "\n",
        "    logs = {\"steps\": [], \"errors\": []}\n",
        "\n",
        "    # Test Notion connection\n",
        "    if NOTION_TOKEN and NOTION_DB_ID:\n",
        "        if not test_notion_connection():\n",
        "            print(\"âš ï¸ Notion connection failed\")\n",
        "\n",
        "    try:\n",
        "        # Process input\n",
        "        cleaned_text, segments = handle_transcript_input()\n",
        "        logs[\"steps\"].append({\n",
        "            \"step\": \"Text input\",\n",
        "            \"segment_count\": len(segments),\n",
        "            \"status\": \"success\"\n",
        "        })\n",
        "\n",
        "        # Detect language\n",
        "        try:\n",
        "            language = detect(cleaned_text[:500]) if cleaned_text else 'en'\n",
        "        except LangDetectException:\n",
        "            language = 'en'\n",
        "        print(f\"ğŸŒ Detected language: {language}\")\n",
        "\n",
        "        # GPT analysis\n",
        "        gpt_results, tokens_used = analyze_with_gpt(cleaned_text, language)\n",
        "\n",
        "        if \"error\" in gpt_results:\n",
        "            logs[\"steps\"].append({\n",
        "                \"step\": \"GPT analysis\",\n",
        "                \"status\": \"failed\",\n",
        "                \"error\": gpt_results[\"error\"]\n",
        "            })\n",
        "            print(f\"âŒ GPT failed: {gpt_results['error']}\")\n",
        "            return\n",
        "        else:\n",
        "            logs[\"steps\"].append({\n",
        "                \"step\": \"GPT analysis\",\n",
        "                \"tokens_used\": tokens_used,\n",
        "                \"meeting_title\": gpt_results.get(\"meeting_title\"),\n",
        "                \"participants_count\": len(gpt_results.get(\"participants\", [])),\n",
        "                \"meeting_type\": gpt_results.get(\"meeting_type\"),\n",
        "                \"action_items_count\": len(gpt_results.get(\"action_items\", [])),\n",
        "                \"status\": \"success\"\n",
        "            })\n",
        "\n",
        "        # Add date and sync to Notion\n",
        "        gpt_results[\"date\"] = datetime.datetime.now().isoformat()\n",
        "        notion_success = create_notion_entry(gpt_results)\n",
        "        logs[\"steps\"].append({\n",
        "            \"step\": \"Notion sync\",\n",
        "            \"status\": \"success\" if notion_success else \"failed\"\n",
        "        })\n",
        "\n",
        "        # Save logs\n",
        "        with open(\"meeting_logs.json\", \"w\") as f:\n",
        "            json.dump(logs, f, indent=2)\n",
        "\n",
        "        print(\"\\nâœ… Process complete! Logs saved\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logs[\"errors\"].append(str(e))\n",
        "        print(f\"\\nâŒ Process error: {str(e)}\")\n",
        "        with open(\"error_log.json\", \"w\") as f:\n",
        "            json.dump(logs, f, indent=2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsNbU1oC8N9I",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# å®‰è£…ä¾èµ–\n",
        "!pip uninstall -y langchain langchain-core langchain-community langchain-openai openai notion-client\n",
        "!pip install langchain==0.2.0\n",
        "!pip install langchain-core==0.2.38\n",
        "!pip install langchain-community==0.2.0\n",
        "!pip install langchain-openai==0.1.9\n",
        "!pip install openai==1.37.0\n",
        "!pip install notion-client==2.0.0\n",
        "!pip install tqdm python-docx langdetect pydub httpx==0.27.0\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg -y\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import openai\n",
        "import whisper\n",
        "from docx import Document\n",
        "from google.colab import files, userdata\n",
        "from notion_client import Client, errors\n",
        "from langdetect import detect, LangDetectException\n",
        "import datetime\n",
        "import tempfile\n",
        "import torch\n",
        "import subprocess\n",
        "from tqdm import tqdm\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "import httpx\n",
        "\n",
        "# æ¸…é™¤ä»£ç†ç¯å¢ƒå˜é‡\n",
        "for var in ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy']:\n",
        "    if var in os.environ:\n",
        "        del os.environ[var]\n",
        "\n",
        "# åˆå§‹åŒ–Notionå®¢æˆ·ç«¯\n",
        "notion = Client(\n",
        "    auth=userdata.get('NOTION_TOKEN'),\n",
        "    client=httpx.Client(proxies=None)\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# åˆå§‹åŒ–è®¾ç½®\n",
        "# ======================\n",
        "try:\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    NOTION_TOKEN = userdata.get('NOTION_TOKEN')\n",
        "    NOTION_DB_ID = userdata.get('NOTION_DB_ID')\n",
        "    NOTION_PAGE_ID = userdata.get('NOTION_PAGE_ID')\n",
        "\n",
        "    missing_creds = []\n",
        "    if not OPENAI_API_KEY:\n",
        "        missing_creds.append(\"OPENAI_API_KEY\")\n",
        "    if not NOTION_TOKEN:\n",
        "        missing_creds.append(\"NOTION_TOKEN\")\n",
        "    if not NOTION_DB_ID:\n",
        "        missing_creds.append(\"NOTION_DB_ID\")\n",
        "    if not NOTION_PAGE_ID:\n",
        "        missing_creds.append(\"NOTION_PAGE_ID\")\n",
        "\n",
        "    if missing_creds:\n",
        "        raise ValueError(f\"ç¼ºå°‘å‡­è¯: {', '.join(missing_creds)}\")\n",
        "\n",
        "    print(\"âœ… æ‰€æœ‰å‡­è¯å·²è®¾ç½®\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ å‡­è¯è·å–å¤±è´¥: {str(e)}\")\n",
        "    print(\"\\nğŸ”§ è®¾ç½®è¯´æ˜:\")\n",
        "    print(\"1. ç‚¹å‡»å·¦ä¾§è¾¹æ çš„é’¥åŒ™å›¾æ ‡ï¼ˆColabå¯†é’¥ï¼‰\")\n",
        "    print(\"2. æ·»åŠ ä»¥ä¸‹å¯†é’¥:\")\n",
        "    print(\"   - OPENAI_API_KEY: ä½ çš„OpenAI APIå¯†é’¥\")\n",
        "    print(\"   - NOTION_TOKEN: ä½ çš„Notioné›†æˆä»¤ç‰Œ\")\n",
        "    print(\"   - NOTION_DB_ID: Notionæ•°æ®åº“ID\")\n",
        "    print(\"   - NOTION_PAGE_ID: æŠ¥å‘Šçˆ¶é¡µé¢ID\")\n",
        "    print(\"3. æ·»åŠ åé‡æ–°è¿è¡Œæ­¤å•å…ƒæ ¼\")\n",
        "    raise\n",
        "\n",
        "# ======================\n",
        "# æ—¥å¿—ç³»ç»Ÿ\n",
        "# ======================\n",
        "class MeetingLogger:\n",
        "    def __init__(self):\n",
        "        self.logs = {\n",
        "            \"start_time\": datetime.datetime.now().isoformat(),\n",
        "            \"steps\": [],\n",
        "            \"errors\": [],\n",
        "            \"metrics\": {}\n",
        "        }\n",
        "\n",
        "    def log_step(self, step_name, status, details=None, error=None):\n",
        "        entry = {\n",
        "            \"step\": step_name,\n",
        "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "            \"status\": status\n",
        "        }\n",
        "        if details:\n",
        "            entry[\"details\"] = details\n",
        "        if error:\n",
        "            entry[\"error\"] = str(error)\n",
        "        self.logs[\"steps\"].append(entry)\n",
        "\n",
        "    def log_metric(self, name, value):\n",
        "        self.logs[\"metrics\"][name] = value\n",
        "\n",
        "    def save_logs(self, filename=\"meeting_logs.json\"):\n",
        "        with open(filename, \"w\") as f:\n",
        "            json.dump(self.logs, f, indent=2)\n",
        "        return filename\n",
        "\n",
        "    def get_console_log(self):\n",
        "        log_str = f\"=== ä¼šè®®å¤„ç†æ—¥å¿— ===\\n\"\n",
        "        log_str += f\"å¼€å§‹æ—¶é—´: {self.logs['start_time']}\\n\"\n",
        "\n",
        "        for step in self.logs[\"steps\"]:\n",
        "            status_icon = \"âœ…\" if step[\"status\"] == \"success\" else \"âŒ\"\n",
        "            log_str += f\"{status_icon} [{step['timestamp']}] {step['step']}\"\n",
        "            if \"details\" in step:\n",
        "                log_str += f\" - {step['details']}\"\n",
        "            if step[\"status\"] == \"failed\":\n",
        "                log_str += f\" - é”™è¯¯: {step.get('error', 'æœªçŸ¥')}\"\n",
        "            log_str += \"\\n\"\n",
        "\n",
        "        if self.logs[\"metrics\"]:\n",
        "            log_str += \"\\n=== æŒ‡æ ‡ ===\\n\"\n",
        "            for metric, value in self.logs[\"metrics\"].items():\n",
        "                log_str += f\"- {metric}: {value}\\n\"\n",
        "\n",
        "        return log_str\n",
        "\n",
        "logger = MeetingLogger()\n",
        "\n",
        "# ======================\n",
        "# å·¥å…·å‡½æ•°ï¼šå¤„ç†åµŒå¥—ç»“æ„\n",
        "# ======================\n",
        "def flatten_key_points(key_points):\n",
        "    \"\"\"å°†key_pointsä¸­çš„åµŒå¥—ç»“æ„ï¼ˆå­—å…¸/åˆ—è¡¨ï¼‰è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œé€‚é…Notionæ ¼å¼\"\"\"\n",
        "    flattened = {}\n",
        "    for category, items in key_points.items():\n",
        "        flattened_items = []\n",
        "        for item in items:\n",
        "            # å¤„ç†å­—å…¸ç±»å‹ï¼ˆå¦‚{\"éƒ¨é—¨\": [\"é—®é¢˜1\", \"é—®é¢˜2\"]}ï¼‰\n",
        "            if isinstance(item, dict):\n",
        "                dict_strings = []\n",
        "                for k, v in item.items():\n",
        "                    # å­—å…¸çš„å€¼å¦‚æœæ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå¸¦ç¬¦å·çš„å­—ç¬¦ä¸²\n",
        "                    if isinstance(v, list):\n",
        "                        list_str = \"â€¢ \".join([str(i) for i in v])\n",
        "                        dict_strings.append(f\"{k}ï¼šâ€¢ {list_str}\")\n",
        "                    else:\n",
        "                        dict_strings.append(f\"{k}ï¼š{v}\")\n",
        "                flattened_items.append(\"ï¼› \".join(dict_strings))\n",
        "\n",
        "            # å¤„ç†åˆ—è¡¨ç±»å‹ï¼ˆå¦‚[\"é—®é¢˜1\", \"é—®é¢˜2\"]ï¼‰\n",
        "            elif isinstance(item, list):\n",
        "                list_str = \"â€¢ \".join([str(i) for i in item])\n",
        "                flattened_items.append(f\"â€¢ {list_str}\")\n",
        "\n",
        "            # å­—ç¬¦ä¸²ç›´æ¥ä¿ç•™\n",
        "            else:\n",
        "                flattened_items.append(str(item))\n",
        "        flattened[category] = flattened_items\n",
        "    return flattened\n",
        "\n",
        "# ======================\n",
        "# éŸ³é¢‘å¤„ç†\n",
        "# ======================\n",
        "def get_audio_duration(audio_path):\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"format=duration\",\n",
        "             \"-of\", \"default=noprint_wrappers=1:nokey=1\", audio_path],\n",
        "            capture_output=True, text=True\n",
        "        )\n",
        "        duration = float(result.stdout)\n",
        "        logger.log_metric(\"éŸ³é¢‘æ—¶é•¿(ç§’)\", duration)\n",
        "        return duration\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"è·å–éŸ³é¢‘æ—¶é•¿\", \"warning\", error=e)\n",
        "        return max(30, os.path.getsize(audio_path) // 8000)\n",
        "\n",
        "def transcribe_audio(audio_path, model_size=\"base\"):\n",
        "    logger.log_step(\"éŸ³é¢‘è½¬å½•\", \"started\", {\"æ¨¡å‹å¤§å°\": model_size, \"éŸ³é¢‘è·¯å¾„\": audio_path})\n",
        "\n",
        "    try:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        logger.log_step(\"ç¡¬ä»¶æ£€æŸ¥\", \"success\", {\"è®¾å¤‡\": device})\n",
        "\n",
        "        model = whisper.load_model(model_size, device=device)\n",
        "        logger.log_step(\"åŠ è½½æ¨¡å‹\", \"success\")\n",
        "\n",
        "        result = model.transcribe(\n",
        "            audio_path,\n",
        "            fp16=(device == \"cuda\"),\n",
        "            verbose=False,\n",
        "            task=\"transcribe\"\n",
        "        )\n",
        "\n",
        "        transcription = result[\"text\"]\n",
        "        detected_lang = result[\"language\"]\n",
        "        logger.log_step(\"éŸ³é¢‘è½¬å½•\", \"success\", {\n",
        "            \"å­—ç¬¦æ•°\": len(transcription),\n",
        "            \"æ£€æµ‹è¯­è¨€\": detected_lang\n",
        "        })\n",
        "\n",
        "        return transcription, detected_lang\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"éŸ³é¢‘è½¬å½•\", \"failed\", error=e)\n",
        "        raise\n",
        "\n",
        "# ======================\n",
        "# ä¼šè®®åˆ†ææ¨¡å‹ä¸å¤„ç†\n",
        "# ======================\n",
        "class MeetingAnalysis(BaseModel):\n",
        "    meeting_title: str = Field(description=\"ä¼šè®®æ ‡é¢˜\")\n",
        "    participants: list[str] = Field(description=\"å‚ä¸è€…åå•\")\n",
        "    summary: str = Field(description=\"3-5æ®µä¼šè®®æ€»ç»“\")\n",
        "    key_points: dict = Field(description=\"æŒ‰concernsã€decisionsã€updatesã€risksåˆ†ç»„çš„å…³é”®ç‚¹ï¼ˆå‡ä¸ºæ•°ç»„ï¼‰\")\n",
        "    action_items: list[dict] = Field(description=\"è¡ŒåŠ¨é¡¹åˆ—è¡¨ï¼ŒåŒ…å«taskã€assigneeã€due_date\")\n",
        "    meeting_type: str = Field(description=\"ä¼šè®®ç±»å‹\")\n",
        "    platform: str = Field(description=\"ä¼šè®®å¹³å°\")\n",
        "\n",
        "def setup_langchain_chains(language='zh'):\n",
        "    lang_map = {\n",
        "        'zh': \"ç”¨ä¸­æ–‡åˆ†æä¼šè®®è®°å½•ï¼Œè¾“å‡ºä¸¥æ ¼ç¬¦åˆJSONæ ¼å¼ï¼Œkey_pointsçš„å­å­—æ®µå‡ä¸ºæ•°ç»„ï¼ˆç”¨[]åŒ…è£¹ï¼‰\",\n",
        "        'en': \"Analyze the meeting transcript in English, output strict JSON with key_points as arrays\",\n",
        "        'fr': \"Analyser le procÃ¨s-verbal en franÃ§ais, sortie JSON stricte avec key_points en tableaux\"\n",
        "    }\n",
        "    lang_instruction = lang_map.get(language[:2], lang_map['zh'])\n",
        "\n",
        "    parser = JsonOutputParser(pydantic_object=MeetingAnalysis)\n",
        "\n",
        "    prompt_template = PromptTemplate(\n",
        "        template=\"\"\"\n",
        "        {lang_instruction}\n",
        "\n",
        "        {format_instructions}\n",
        "\n",
        "        ### ä¼šè®®è®°å½•:\n",
        "        {transcript}\n",
        "\n",
        "        è¯·ä¸¥æ ¼æŒ‰ç…§æ ¼å¼è¦æ±‚è¾“å‡ºï¼Œç¡®ä¿JSONç»“æ„æ­£ç¡®ã€‚\n",
        "        \"\"\",\n",
        "        input_variables=[\"transcript\"],\n",
        "        partial_variables={\n",
        "            \"lang_instruction\": lang_instruction,\n",
        "            \"format_instructions\": parser.get_format_instructions()\n",
        "        }\n",
        "    )\n",
        "\n",
        "    llm = ChatOpenAI(\n",
        "        openai_api_key=OPENAI_API_KEY,\n",
        "        temperature=0.3,\n",
        "        model=\"gpt-3.5-turbo\"\n",
        "    )\n",
        "\n",
        "    # ä½¿ç”¨æ–°çš„é“¾å¼ç»“æ„\n",
        "    analysis_chain = prompt_template | llm | parser\n",
        "\n",
        "    return analysis_chain\n",
        "\n",
        "def analyze_meeting(transcript, language='zh'):\n",
        "    logger.log_step(\"åˆ†æä¼šè®®\", \"started\", {\"è¯­è¨€\": language})\n",
        "    print(\"\\nå¼€å§‹åˆ†æä¼šè®®å†…å®¹...\")\n",
        "\n",
        "    try:\n",
        "        analysis_chain = setup_langchain_chains(language)\n",
        "        processed_transcript = transcript[:15000]\n",
        "        print(f\"ä½¿ç”¨çš„è½¬å½•æ–‡æœ¬é•¿åº¦: {len(processed_transcript)}å­—ç¬¦\")\n",
        "\n",
        "        parsed = analysis_chain.invoke({\"transcript\": processed_transcript})\n",
        "\n",
        "        parsed[\"language\"] = language\n",
        "        parsed[\"date\"] = datetime.datetime.now().isoformat()\n",
        "\n",
        "        if not parsed.get(\"action_items\"):\n",
        "            logger.log_step(\"æ£€æŸ¥è¡ŒåŠ¨é¡¹\", \"warning\", \"æœªæ£€æµ‹åˆ°è¡ŒåŠ¨é¡¹\")\n",
        "            print(\"âš ï¸ æœªæ£€æµ‹åˆ°è¡ŒåŠ¨é¡¹\")\n",
        "            parsed[\"fallback_used\"] = True\n",
        "        else:\n",
        "            parsed[\"fallback_used\"] = False\n",
        "\n",
        "        logger.log_step(\"åˆ†æä¼šè®®\", \"success\", {\n",
        "            \"æ ‡é¢˜\": parsed[\"meeting_title\"],\n",
        "            \"å‚ä¸è€…æ•°é‡\": len(parsed[\"participants\"]),\n",
        "            \"è¡ŒåŠ¨é¡¹æ•°é‡\": len(parsed[\"action_items\"])\n",
        "        })\n",
        "        print(f\"âœ… ä¼šè®®åˆ†æå®Œæˆ (æ ‡é¢˜: {parsed['meeting_title']})\")\n",
        "        return parsed\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"åˆ†æä¼šè®®å¤±è´¥: {str(e)}\"\n",
        "        logger.log_step(\"åˆ†æä¼šè®®\", \"failed\", error=error_msg)\n",
        "        print(f\"âŒ {error_msg}\")\n",
        "        return {\"error\": error_msg, \"fallback_used\": True}\n",
        "\n",
        "# ======================\n",
        "# NotionæŠ¥å‘Šç”Ÿæˆ\n",
        "# ======================\n",
        "def create_notion_report_page(meeting_data, transcript, logs):\n",
        "    logger.log_step(\"åˆ›å»ºNotionæŠ¥å‘Š\", \"started\")\n",
        "\n",
        "    try:\n",
        "        global notion\n",
        "\n",
        "        # éªŒè¯çˆ¶é¡µé¢\n",
        "        try:\n",
        "            parent_page = notion.pages.retrieve(NOTION_PAGE_ID)\n",
        "            page_title = parent_page.get('properties', {}).get('title', {}).get('title', [{}])[0].get('plain_text', 'æ— æ ‡é¢˜')\n",
        "            logger.log_step(\"çˆ¶é¡µé¢æ£€æŸ¥\", \"success\", {\"é¡µé¢ID\": NOTION_PAGE_ID, \"æ ‡é¢˜\": page_title})\n",
        "            print(f\"âœ… æˆåŠŸè®¿é—®çˆ¶é¡µé¢: {page_title} (ID: {NOTION_PAGE_ID[:8]}...)\")\n",
        "        except errors.APIResponseError as e:\n",
        "            if e.status == 404:\n",
        "                error_msg = f\"çˆ¶é¡µé¢ä¸å­˜åœ¨ (ID: {NOTION_PAGE_ID})ã€‚è¯·æ£€æŸ¥IDæ˜¯å¦æ­£ç¡®ã€‚\"\n",
        "            elif e.status == 403:\n",
        "                error_msg = f\"æ²¡æœ‰è®¿é—®çˆ¶é¡µé¢çš„æƒé™ (ID: {NOTION_PAGE_ID})ã€‚è¯·å°†é¡µé¢å…±äº«ç»™Notioné›†æˆã€‚\"\n",
        "            else:\n",
        "                error_msg = f\"è®¿é—®çˆ¶é¡µé¢å¤±è´¥: {str(e)}\"\n",
        "            logger.log_step(\"çˆ¶é¡µé¢æ£€æŸ¥\", \"failed\", error=error_msg)\n",
        "            print(f\"âŒ {error_msg}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            error_msg = f\"çˆ¶é¡µé¢æ£€æŸ¥å‡ºé”™: {str(e)}\"\n",
        "            logger.log_step(\"çˆ¶é¡µé¢æ£€æŸ¥\", \"failed\", error=error_msg)\n",
        "            print(f\"âŒ {error_msg}\")\n",
        "            return None\n",
        "\n",
        "        # åˆ›å»ºå­é¡µé¢\n",
        "        new_page = notion.pages.create(\n",
        "            parent={\"page_id\": NOTION_PAGE_ID},\n",
        "            properties={\n",
        "                \"title\": {\n",
        "                    \"title\": [\n",
        "                        {\n",
        "                            \"text\": {\n",
        "                                \"content\": meeting_data.get(\"meeting_title\", \"ä¼šè®®æŠ¥å‘Š\")[:200]\n",
        "                            }\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "        page_id = new_page[\"id\"]\n",
        "        logger.log_step(\"åˆ›å»ºå­é¡µé¢\", \"success\", {\"é¡µé¢ID\": page_id})\n",
        "        print(f\"âœ… å·²åˆ›å»ºå­é¡µé¢ (ID: {page_id[:8]}...)\")\n",
        "\n",
        "        # æ„å»ºæŠ¥å‘Šå†…å®¹\n",
        "        children_blocks = []\n",
        "\n",
        "        # 1. ä¼šè®®è¯¦æƒ…\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"ä¼šè®®è¯¦æƒ…\"}}]}\n",
        "        })\n",
        "\n",
        "        details_text = f\"\"\"\n",
        "        **æ—¥æœŸ**: {meeting_data.get('date', 'æœªçŸ¥')}\n",
        "        **å‚ä¸è€…**: {', '.join(meeting_data.get('participants', []))}\n",
        "        **è¯­è¨€**: {meeting_data.get('language', 'æœªçŸ¥')}\n",
        "        **å¹³å°**: {meeting_data.get('platform', 'æœªçŸ¥')}\n",
        "        **ä¼šè®®ç±»å‹**: {meeting_data.get('meeting_type', 'æœªçŸ¥')}\n",
        "        \"\"\"\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"paragraph\",\n",
        "            \"paragraph\": {\"rich_text\": [{\"text\": {\"content\": details_text.strip()}}]}\n",
        "        })\n",
        "\n",
        "        # 2. ä¼šè®®æ€»ç»“\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"æ€»ç»“\"}}]}\n",
        "        })\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"paragraph\",\n",
        "            \"paragraph\": {\"rich_text\": [{\"text\": {\"content\": meeting_data.get('summary', '')}}]}\n",
        "        })\n",
        "\n",
        "        # 3. å…³é”®ç‚¹ï¼ˆä¿®å¤åµŒå¥—ç»“æ„é—®é¢˜ï¼‰\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"å…³é”®ç‚¹\"}}]}\n",
        "        })\n",
        "\n",
        "        key_points = meeting_data.get('key_points', {})\n",
        "        key_points = flatten_key_points(key_points)\n",
        "\n",
        "        for category, items in key_points.items():\n",
        "            children_blocks.append({\n",
        "                \"object\": \"block\",\n",
        "                \"type\": \"heading_3\",\n",
        "                \"heading_3\": {\"rich_text\": [{\"text\": {\"content\": category.capitalize()}}]}\n",
        "            })\n",
        "\n",
        "            if items:\n",
        "                for item in items:\n",
        "                    children_blocks.append({\n",
        "                        \"object\": \"block\",\n",
        "                        \"type\": \"bulleted_list_item\",\n",
        "                        \"bulleted_list_item\": {\"rich_text\": [{\"text\": {\"content\": item}}]}\n",
        "                    })\n",
        "\n",
        "        # 4. è¡ŒåŠ¨é¡¹\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"è¡ŒåŠ¨é¡¹\"}}]}\n",
        "        })\n",
        "\n",
        "        table_rows = []\n",
        "        for idx, item in enumerate(meeting_data.get('action_items', [])):\n",
        "            task = item.get('task', '')\n",
        "            assignee = item.get('assignee', 'æœªåˆ†é…')\n",
        "            due_date = item.get('due_date', 'æ— ')\n",
        "\n",
        "            table_rows.append([\n",
        "                [{\"text\": {\"content\": str(idx+1)}}],\n",
        "                [{\"text\": {\"content\": task}}],\n",
        "                [{\"text\": {\"content\": assignee}}],\n",
        "                [{\"text\": {\"content\": due_date}}]\n",
        "            ])\n",
        "\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"table\",\n",
        "            \"table\": {\n",
        "                \"table_width\": 4,\n",
        "                \"has_column_header\": True,\n",
        "                \"has_row_header\": False,\n",
        "                \"children\": [\n",
        "                    {\n",
        "                        \"object\": \"block\",\n",
        "                        \"type\": \"table_row\",\n",
        "                        \"table_row\": {\n",
        "                            \"cells\": [\n",
        "                                [{\"text\": {\"content\": \"åºå·\"}}],\n",
        "                                [{\"text\": {\"content\": \"ä»»åŠ¡\"}}],\n",
        "                                [{\"text\": {\"content\": \"è´Ÿè´£äºº\"}}],\n",
        "                                [{\"text\": {\"content\": \"æˆªæ­¢æ—¥æœŸ\"}}]\n",
        "                            ]\n",
        "                        }\n",
        "                    },\n",
        "                    *[{\n",
        "                        \"object\": \"block\",\n",
        "                        \"type\": \"table_row\",\n",
        "                        \"table_row\": {\"cells\": cells}\n",
        "                    } for cells in table_rows]\n",
        "                ]\n",
        "            }\n",
        "        })\n",
        "\n",
        "        # 5. å¤„ç†æ—¥å¿—\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"å¤„ç†æ—¥å¿—\"}}]}\n",
        "        })\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"code\",\n",
        "            \"code\": {\n",
        "                \"rich_text\": [{\"text\": {\"content\": logger.get_console_log()}}],\n",
        "                \"language\": \"plain text\"\n",
        "            }\n",
        "        })\n",
        "\n",
        "        # æ·»åŠ å†…å®¹åˆ°é¡µé¢\n",
        "        notion.blocks.children.append(\n",
        "            block_id=page_id,\n",
        "            children=children_blocks\n",
        "        )\n",
        "        logger.log_step(\"æ·»åŠ å†…å®¹åˆ°é¡µé¢\", \"success\")\n",
        "        print(f\"âœ… å·²æ·»åŠ å†…å®¹åˆ°å­é¡µé¢\")\n",
        "\n",
        "\n",
        "        # å…³è”æ•°æ®åº“ï¼ˆä¿®æ”¹åï¼‰\n",
        "        if NOTION_DB_ID:\n",
        "            try:\n",
        "                db = notion.databases.retrieve(NOTION_DB_ID)\n",
        "                logger.log_step(\"æ•°æ®åº“éªŒè¯\", \"success\", {\"db_id\": NOTION_DB_ID})\n",
        "\n",
        "        # æ‰‹åŠ¨æŒ‡å®šä½ çš„å…³ç³»å±æ€§åç§°\n",
        "                relation_prop_name = \"relation\"\n",
        "\n",
        "        # éªŒè¯å±æ€§\n",
        "                if relation_prop_name not in db[\"properties\"]:\n",
        "                    raise ValueError(f\"æ•°æ®åº“ä¸­ä¸å­˜åœ¨åä¸ºã€Œ{relation_prop_name}ã€çš„å±æ€§\")\n",
        "                if db[\"properties\"][relation_prop_name][\"type\"] != \"relation\":\n",
        "                    raise ValueError(f\"å±æ€§ã€Œ{relation_prop_name}ã€ä¸æ˜¯å…³ç³»ç±»å‹\")\n",
        "\n",
        "        # å…³è”\n",
        "                notion.pages.update(\n",
        "            page_id=page_id,\n",
        "            properties={\n",
        "                relation_prop_name: {\n",
        "                    \"relation\": [{\"id\": NOTION_DB_ID}]\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "                logger.log_step(\"å…³è”æ•°æ®åº“\", \"success\", {\"ä½¿ç”¨çš„å±æ€§\": relation_prop_name})\n",
        "                print(f\"âœ… å·²é€šè¿‡å±æ€§ã€Œ{relation_prop_name}ã€å…³è”åˆ°æ•°æ®åº“\")\n",
        "            except Exception as e:\n",
        "                logger.log_step(\"å…³è”æ•°æ®åº“\", \"warning\", error=str(e))\n",
        "                print(f\"âš ï¸ å…³è”æ•°æ®åº“å¤±è´¥: {str(e)}ï¼ˆä¸å½±å“æŠ¥å‘Šç”Ÿæˆï¼‰\")\n",
        "\n",
        "        report_url = new_page.get(\"url\", \"\")\n",
        "        logger.log_step(\"ç”ŸæˆNotionæŠ¥å‘Š\", \"success\", {\"URL\": report_url})\n",
        "        return report_url\n",
        "\n",
        "    except Exception as e:\n",
        "        error_details = f\"Notion APIé”™è¯¯: {str(e)}\"\n",
        "        if hasattr(e, 'response') and hasattr(e.response, 'content'):\n",
        "            error_details += f\"\\nå“åº”: {e.response.content.decode('utf-8')}\"\n",
        "        logger.log_step(\"ç”ŸæˆNotionæŠ¥å‘Š\", \"failed\", error=error_details)\n",
        "        print(f\"âŒ Notionæ“ä½œå¤±è´¥: {error_details}\")\n",
        "        return None\n",
        "\n",
        "# ======================\n",
        "# æƒé™æµ‹è¯•å‡½æ•°\n",
        "# ======================\n",
        "def test_notion_permissions():\n",
        "    print(\"\\n=== å¼€å§‹Notionæƒé™æµ‹è¯• ===\")\n",
        "    print(f\"ä½¿ç”¨çš„çˆ¶é¡µé¢ID: {NOTION_PAGE_ID[:8]}... (å®Œæ•´: {NOTION_PAGE_ID})\")\n",
        "\n",
        "    # 1. æµ‹è¯•é›†æˆä»¤ç‰Œæœ‰æ•ˆæ€§\n",
        "    try:\n",
        "        user_info = notion.users.me()\n",
        "        print(f\"âœ… é›†æˆä»¤ç‰Œæœ‰æ•ˆ (æ‰€å±å·¥ä½œç©ºé—´: {user_info.get('workspace_name', 'æœªçŸ¥')})\")\n",
        "    except errors.UnauthorizedError:\n",
        "        print(f\"âŒ é›†æˆä»¤ç‰Œæ— æ•ˆ (NOTION_TOKENé”™è¯¯)\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ éªŒè¯é›†æˆä»¤ç‰Œæ—¶å‡ºé”™: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "    # 2. æµ‹è¯•çˆ¶é¡µé¢è®¿é—®æƒé™\n",
        "    try:\n",
        "        page = notion.pages.retrieve(NOTION_PAGE_ID)\n",
        "        page_title = page.get('properties', {}).get('title', {}).get('title', [{}])[0].get('plain_text', 'æ— æ ‡é¢˜')\n",
        "        print(f\"âœ… æˆåŠŸè®¿é—®çˆ¶é¡µé¢: {page_title}\")\n",
        "    except errors.APIResponseError as e:\n",
        "        if e.status == 404:\n",
        "            print(f\"âŒ çˆ¶é¡µé¢ä¸å­˜åœ¨ (IDé”™è¯¯æˆ–é¡µé¢å·²åˆ é™¤)\")\n",
        "            return False\n",
        "        elif e.status == 403:\n",
        "            print(f\"âŒ æ²¡æœ‰è®¿é—®æƒé™ (è¯·å°†é¡µé¢å…±äº«ç»™é›†æˆ)\")\n",
        "            return False\n",
        "        else:\n",
        "            print(f\"âŒ è®¿é—®é¡µé¢æ—¶å‡ºé”™ (çŠ¶æ€ç : {e.status}): {str(e)}\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ è®¿é—®é¡µé¢æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "    # 3. æµ‹è¯•æ•°æ®åº“è®¿é—®æƒé™\n",
        "    try:\n",
        "        if NOTION_DB_ID:\n",
        "            db = notion.databases.retrieve(NOTION_DB_ID)\n",
        "            print(f\"âœ… æˆåŠŸè®¿é—®æ•°æ®åº“: {db.get('title', [{}])[0].get('plain_text', 'æ— æ ‡é¢˜')}\")\n",
        "    except errors.APIResponseError as e:\n",
        "        print(f\"âš ï¸ æ•°æ®åº“è®¿é—®è­¦å‘Š: {str(e)}ï¼ˆä»å¯ç”ŸæˆæŠ¥å‘Šï¼Œä½†å¯èƒ½æ— æ³•å…³è”ï¼‰\")\n",
        "\n",
        "    return True\n",
        "\n",
        "# ======================\n",
        "# è¾“å…¥å¤„ç†\n",
        "# ======================\n",
        "def handle_transcript_input():\n",
        "    logger.log_step(\"å¤„ç†è¾“å…¥\", \"started\")\n",
        "\n",
        "    print(\"\\n=== è¾“å…¥æ–¹å¼é€‰æ‹© ===\")\n",
        "    print(\"1: ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ (.mp3/.wav/.m4a/.opus)\")\n",
        "    print(\"2: ä¸Šä¼ æ–‡æœ¬æ–‡ä»¶ (.txt/.docx)\")\n",
        "    print(\"3: ç›´æ¥ç²˜è´´æ–‡æœ¬\")\n",
        "\n",
        "    try:\n",
        "        choice = input(\"è¯·é€‰æ‹©è¾“å…¥æ–¹å¼ (1/2/3): \").strip() or \"1\"\n",
        "    except:\n",
        "        choice = \"1\"\n",
        "\n",
        "    if choice == \"1\":\n",
        "        # éŸ³é¢‘å¤„ç†\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            logger.log_step(\"ä¸Šä¼ éŸ³é¢‘\", \"failed\", \"æœªä¸Šä¼ ä»»ä½•æ–‡ä»¶\")\n",
        "            print(\"âš ï¸ æœªä¸Šä¼ æ–‡ä»¶ï¼Œåˆ‡æ¢åˆ°æ–‡æœ¬è¾“å…¥\")\n",
        "            return handle_transcript_input()\n",
        "\n",
        "        filename = next(iter(uploaded.keys()))\n",
        "        logger.log_step(\"ä¸Šä¼ éŸ³é¢‘\", \"success\", {\"æ–‡ä»¶å\": filename, \"å¤§å°\": len(uploaded[filename])})\n",
        "        print(f\"âœ… å·²ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶: {filename}\")\n",
        "\n",
        "        ext = os.path.splitext(filename)[1].lower()\n",
        "        supported_audio = ['.mp3', '.wav', '.m4a', '.opus']\n",
        "        if ext not in supported_audio:\n",
        "            error = f\"ä¸æ”¯æŒçš„éŸ³é¢‘æ ¼å¼: {ext} (æ”¯æŒ: {', '.join(supported_audio)})\"\n",
        "            logger.log_step(\"å¤„ç†éŸ³é¢‘\", \"failed\", error=error)\n",
        "            print(f\"âŒ {error}\")\n",
        "            raise ValueError(error)\n",
        "\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=ext) as tmp:\n",
        "            tmp.write(uploaded[filename])\n",
        "            audio_path = tmp.name\n",
        "\n",
        "        # é€‰æ‹©æ¨¡å‹\n",
        "        print(\"\\nâš¡ é€‰æ‹©è½¬å½•æ¨¡å‹:\")\n",
        "        print(\"1: å¿«é€Ÿ (tiny, ä½ç²¾åº¦)\")\n",
        "        print(\"2: å¹³è¡¡ (base, æ¨è)\")\n",
        "        print(\"3: é«˜ç²¾åº¦ (small, è¾ƒæ…¢)\")\n",
        "        try:\n",
        "            model_choice = input(\"è¯·é€‰æ‹©æ¨¡å‹ (1/2/3): \").strip() or \"2\"\n",
        "        except:\n",
        "            model_choice = \"2\"\n",
        "        model_map = {\"1\": \"tiny\", \"2\": \"base\", \"3\": \"small\"}\n",
        "        model_size = model_map.get(model_choice, \"base\")\n",
        "        print(f\"ä½¿ç”¨æ¨¡å‹: {model_size}\")\n",
        "\n",
        "        # è½¬å½•\n",
        "        duration = get_audio_duration(audio_path)\n",
        "        print(f\"éŸ³é¢‘æ—¶é•¿: {duration:.1f}ç§’ï¼Œå¼€å§‹è½¬å½•...\")\n",
        "        transcript, detected_lang = transcribe_audio(audio_path, model_size)\n",
        "        os.unlink(audio_path)\n",
        "\n",
        "        print(f\"âœ… è½¬å½•å®Œæˆ (è¯­è¨€: {detected_lang})\")\n",
        "        return transcript, detected_lang\n",
        "\n",
        "    elif choice == \"2\":\n",
        "        # æ–‡æœ¬æ–‡ä»¶\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            logger.log_step(\"ä¸Šä¼ æ–‡æœ¬\", \"failed\", \"æœªä¸Šä¼ ä»»ä½•æ–‡ä»¶\")\n",
        "            print(\"âš ï¸ æœªä¸Šä¼ æ–‡ä»¶ï¼Œåˆ‡æ¢åˆ°ç›´æ¥ç²˜è´´\")\n",
        "            return handle_transcript_input()\n",
        "\n",
        "        filename = next(iter(uploaded.keys()))\n",
        "        logger.log_step(\"ä¸Šä¼ æ–‡æœ¬\", \"success\", {\"æ–‡ä»¶å\": filename})\n",
        "        print(f\"âœ… å·²ä¸Šä¼ æ–‡ä»¶: {filename}\")\n",
        "\n",
        "        try:\n",
        "            if filename.endswith('.txt'):\n",
        "                transcript = uploaded[filename].decode('utf-8')\n",
        "            elif filename.endswith('.docx'):\n",
        "                with tempfile.NamedTemporaryFile(delete=False, suffix='.docx') as tmp:\n",
        "                    tmp.write(uploaded[filename])\n",
        "                    doc = Document(tmp.name)\n",
        "                    transcript = \"\\n\".join(p.text for p in doc.paragraphs)\n",
        "                    os.unlink(tmp.name)\n",
        "            else:\n",
        "                raise ValueError(f\"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {filename} (æ”¯æŒ: .txt, .docx)\")\n",
        "\n",
        "            # æ£€æµ‹è¯­è¨€\n",
        "            lang = detect(transcript[:500]) if transcript else 'zh'\n",
        "            logger.log_step(\"æ£€æµ‹è¯­è¨€\", \"success\", {\"è¯­è¨€\": lang})\n",
        "            print(f\"âœ… è¯»å–å®Œæˆ (æ£€æµ‹è¯­è¨€: {lang})\")\n",
        "            return transcript, lang\n",
        "        except Exception as e:\n",
        "            logger.log_step(\"å¤„ç†æ–‡æœ¬æ–‡ä»¶\", \"failed\", error=str(e))\n",
        "            print(f\"âŒ å¤„ç†æ–‡ä»¶å‡ºé”™: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    elif choice == \"3\":\n",
        "        # ç›´æ¥ç²˜è´´\n",
        "        print(\"\\nè¯·ç²˜è´´ä¼šè®®è®°å½• (ç²˜è´´åæŒ‰Enterï¼Œè¾“å…¥ç©ºè¡Œç»“æŸ):\")\n",
        "        lines = []\n",
        "        while True:\n",
        "            line = input()\n",
        "            if not line:\n",
        "                break\n",
        "            lines.append(line)\n",
        "        transcript = \"\\n\".join(lines)\n",
        "\n",
        "        if not transcript.strip():\n",
        "            logger.log_step(\"è¾“å…¥æ–‡æœ¬\", \"failed\", \"æœªè¾“å…¥ä»»ä½•å†…å®¹\")\n",
        "            print(\"âš ï¸ æœªè¾“å…¥ä»»ä½•å†…å®¹ï¼Œé‡æ–°é€‰æ‹©è¾“å…¥æ–¹å¼\")\n",
        "            return handle_transcript_input()\n",
        "\n",
        "        # æ£€æµ‹è¯­è¨€\n",
        "        try:\n",
        "            lang = detect(transcript[:500])\n",
        "            logger.log_step(\"æ£€æµ‹è¯­è¨€\", \"success\", {\"è¯­è¨€\": lang})\n",
        "            print(f\"âœ… å·²è¾“å…¥æ–‡æœ¬ (æ£€æµ‹è¯­è¨€: {lang})\")\n",
        "        except:\n",
        "            lang = 'zh'\n",
        "            logger.log_step(\"æ£€æµ‹è¯­è¨€\", \"warning\", \"ä½¿ç”¨é»˜è®¤è¯­è¨€ä¸­æ–‡\")\n",
        "            print(f\"âœ… å·²è¾“å…¥æ–‡æœ¬ (ä½¿ç”¨é»˜è®¤è¯­è¨€: ä¸­æ–‡)\")\n",
        "\n",
        "        return transcript, lang\n",
        "\n",
        "    else:\n",
        "        logger.log_step(\"é€‰æ‹©è¾“å…¥æ–¹å¼\", \"warning\", \"æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤éŸ³é¢‘è¾“å…¥\")\n",
        "        print(\"âš ï¸ æ— æ•ˆé€‰æ‹©ï¼Œé»˜è®¤ä½¿ç”¨éŸ³é¢‘è¾“å…¥\")\n",
        "        return handle_transcript_input()\n",
        "\n",
        "# ======================\n",
        "# ä¸»å‡½æ•°\n",
        "# ======================\n",
        "def main():\n",
        "    logger.log_step(\"å·¥ä½œæµç¨‹\", \"started\")\n",
        "    print(\"=== ä¼šè®®è®°å½•å¤„ç†å·¥å…· ===\")\n",
        "\n",
        "    try:\n",
        "        if not test_notion_permissions():\n",
        "            print(\"\\nâŒ æƒé™æµ‹è¯•æœªé€šè¿‡ï¼Œè¯·å…ˆè§£å†³ä¸Šè¿°é—®é¢˜\")\n",
        "            log_file = logger.save_logs(\"error_logs.json\")\n",
        "            print(f\"é”™è¯¯æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}\")\n",
        "            return\n",
        "\n",
        "        transcript, language = handle_transcript_input()\n",
        "        logger.log_metric(\"è½¬å½•æ–‡æœ¬é•¿åº¦\", len(transcript))\n",
        "\n",
        "        meeting_data = analyze_meeting(transcript, language)\n",
        "        if \"error\" in meeting_data:\n",
        "            raise RuntimeError(f\"åˆ†æå¤±è´¥: {meeting_data['error']}\")\n",
        "\n",
        "        print(\"\\nå¼€å§‹åˆ›å»ºNotionæŠ¥å‘Š...\")\n",
        "        report_url = create_notion_report_page(meeting_data, transcript, logger.logs)\n",
        "\n",
        "        if not report_url:\n",
        "            raise RuntimeError(\"åˆ›å»ºNotionæŠ¥å‘Šå¤±è´¥\")\n",
        "\n",
        "        log_file = logger.save_logs()\n",
        "        print(f\"\\nğŸ‰ å¤„ç†å®Œæˆï¼\")\n",
        "        print(f\"ğŸ“„ ä¼šè®®æŠ¥å‘ŠURL: {report_url}\")\n",
        "        print(f\"ğŸ“‹ æ—¥å¿—æ–‡ä»¶: {log_file}\")\n",
        "\n",
        "        from IPython.display import HTML\n",
        "        display(HTML(f'<a href=\"{report_url}\" target=\"_blank\">ç‚¹å‡»æ‰“å¼€NotionæŠ¥å‘Š</a>'))\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"å·¥ä½œæµç¨‹\", \"failed\", error=str(e))\n",
        "        log_file = logger.save_logs(\"error_logs.json\")\n",
        "        print(f\"\\nâŒ å¤„ç†å¤±è´¥ï¼\")\n",
        "        print(f\"é”™è¯¯è¯¦æƒ…: {str(e)}\")\n",
        "        print(f\"é”™è¯¯æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å®‰è£…ä¾èµ– - ä¿®æ”¹éƒ¨åˆ†\n",
        "!pip uninstall -y whisper\n",
        "!pip install faster-whisper\n",
        "!pip install git+https://github.com/openai/whisper.git  # ä¿ç•™åŸæ¥å£ä½†ä½¿ç”¨faster-whisperåç«¯\n",
        "!sudo apt update && sudo apt install ffmpeg -y\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import openai\n",
        "import whisper\n",
        "from docx import Document\n",
        "from google.colab import files, userdata\n",
        "from notion_client import Client, errors\n",
        "from langdetect import detect, LangDetectException\n",
        "import datetime\n",
        "import tempfile\n",
        "import torch\n",
        "import subprocess\n",
        "from tqdm import tqdm\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "import httpx\n",
        "from whisper.utils import get_writer  # å¯¼å…¥faster-whisperçš„è¾…åŠ©å·¥å…·\n",
        "# æ¸…é™¤ä»£ç†ç¯å¢ƒå˜é‡\n",
        "for var in ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy']:\n",
        "    if var in os.environ:\n",
        "        del os.environ[var]\n",
        "# åˆå§‹åŒ–Notionå®¢æˆ·ç«¯\n",
        "http_client = httpx.Client()\n",
        "http_client.proxies = None  # ç¦ç”¨ä»£ç†\n",
        "\n",
        "notion = Client(\n",
        "    auth=userdata.get('NOTION_TOKEN'),\n",
        "    client=http_client\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# åˆå§‹åŒ–è®¾ç½®\n",
        "# ======================\n",
        "try:\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    NOTION_TOKEN = userdata.get('NOTION_TOKEN')\n",
        "    NOTION_DB_ID = userdata.get('NOTION_DB_ID')\n",
        "    NOTION_PAGE_ID = userdata.get('NOTION_PAGE_ID')\n",
        "\n",
        "    missing_creds = []\n",
        "    if not OPENAI_API_KEY:\n",
        "        missing_creds.append(\"OPENAI_API_KEY\")\n",
        "    if not NOTION_TOKEN:\n",
        "        missing_creds.append(\"NOTION_TOKEN\")\n",
        "    if not NOTION_DB_ID:\n",
        "        missing_creds.append(\"NOTION_DB_ID\")\n",
        "    if not NOTION_PAGE_ID:\n",
        "        missing_creds.append(\"NOTION_PAGE_ID\")\n",
        "\n",
        "    if missing_creds:\n",
        "        raise ValueError(f\"ç¼ºå°‘å‡­è¯: {', '.join(missing_creds)}\")\n",
        "\n",
        "    print(\"âœ… æ‰€æœ‰å‡­è¯å·²è®¾ç½®\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ å‡­è¯è·å–å¤±è´¥: {str(e)}\")\n",
        "    print(\"\\nğŸ”§ è®¾ç½®è¯´æ˜:\")\n",
        "    print(\"1. ç‚¹å‡»å·¦ä¾§è¾¹æ çš„é’¥åŒ™å›¾æ ‡ï¼ˆColabå¯†é’¥ï¼‰\")\n",
        "    print(\"2. æ·»åŠ ä»¥ä¸‹å¯†é’¥:\")\n",
        "    print(\"   - OPENAI_API_KEY: ä½ çš„OpenAI APIå¯†é’¥\")\n",
        "    print(\"   - NOTION_TOKEN: ä½ çš„Notioné›†æˆä»¤ç‰Œ\")\n",
        "    print(\"   - NOTION_DB_ID: Notionæ•°æ®åº“ID\")\n",
        "    print(\"   - NOTION_PAGE_ID: æŠ¥å‘Šçˆ¶é¡µé¢ID\")\n",
        "    print(\"3. æ·»åŠ åé‡æ–°è¿è¡Œæ­¤å•å…ƒæ ¼\")\n",
        "    raise\n",
        "\n",
        "# ======================\n",
        "# æ—¥å¿—ç³»ç»Ÿ\n",
        "# ======================\n",
        "class MeetingLogger:\n",
        "    def __init__(self):\n",
        "        self.logs = {\n",
        "            \"start_time\": datetime.datetime.now().isoformat(),\n",
        "            \"steps\": [],\n",
        "            \"errors\": [],\n",
        "            \"metrics\": {}\n",
        "        }\n",
        "\n",
        "    def log_step(self, step_name, status, details=None, error=None):\n",
        "        entry = {\n",
        "            \"step\": step_name,\n",
        "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "            \"status\": status\n",
        "        }\n",
        "        if details:\n",
        "            entry[\"details\"] = details\n",
        "        if error:\n",
        "            entry[\"error\"] = str(error)\n",
        "        self.logs[\"steps\"].append(entry)\n",
        "\n",
        "    def log_metric(self, name, value):\n",
        "        self.logs[\"metrics\"][name] = value\n",
        "\n",
        "    def save_logs(self, filename=\"meeting_logs.json\"):\n",
        "        with open(filename, \"w\") as f:\n",
        "            json.dump(self.logs, f, indent=2)\n",
        "        return filename\n",
        "\n",
        "    def get_console_log(self):\n",
        "        log_str = f\"=== ä¼šè®®å¤„ç†æ—¥å¿— ===\\n\"\n",
        "        log_str += f\"å¼€å§‹æ—¶é—´: {self.logs['start_time']}\\n\"\n",
        "\n",
        "        for step in self.logs[\"steps\"]:\n",
        "            status_icon = \"âœ…\" if step[\"status\"] == \"success\" else \"âŒ\"\n",
        "            log_str += f\"{status_icon} [{step['timestamp']}] {step['step']}\"\n",
        "            if \"details\" in step:\n",
        "                log_str += f\" - {step['details']}\"\n",
        "            if step[\"status\"] == \"failed\":\n",
        "                log_str += f\" - é”™è¯¯: {step.get('error', 'æœªçŸ¥')}\"\n",
        "            log_str += \"\\n\"\n",
        "\n",
        "        if self.logs[\"metrics\"]:\n",
        "            log_str += \"\\n=== æŒ‡æ ‡ ===\\n\"\n",
        "            for metric, value in self.logs[\"metrics\"].items():\n",
        "                log_str += f\"- {metric}: {value}\\n\"\n",
        "\n",
        "        return log_str\n",
        "\n",
        "logger = MeetingLogger()\n",
        "\n",
        "# ======================\n",
        "# å·¥å…·å‡½æ•°ï¼šå¤„ç†åµŒå¥—ç»“æ„\n",
        "# ======================\n",
        "def flatten_key_points(key_points):\n",
        "    \"\"\"å°†key_pointsä¸­çš„åµŒå¥—ç»“æ„ï¼ˆå­—å…¸/åˆ—è¡¨ï¼‰è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œé€‚é…Notionæ ¼å¼\"\"\"\n",
        "    flattened = {}\n",
        "    for category, items in key_points.items():\n",
        "        flattened_items = []\n",
        "        for item in items:\n",
        "            # å¤„ç†å­—å…¸ç±»å‹ï¼ˆå¦‚{\"éƒ¨é—¨\": [\"é—®é¢˜1\", \"é—®é¢˜2\"]}ï¼‰\n",
        "            if isinstance(item, dict):\n",
        "                dict_strings = []\n",
        "                for k, v in item.items():\n",
        "                    # å­—å…¸çš„å€¼å¦‚æœæ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå¸¦ç¬¦å·çš„å­—ç¬¦ä¸²\n",
        "                    if isinstance(v, list):\n",
        "                        list_str = \"â€¢ \".join([str(i) for i in v])\n",
        "                        dict_strings.append(f\"{k}ï¼šâ€¢ {list_str}\")\n",
        "                    else:\n",
        "                        dict_strings.append(f\"{k}ï¼š{v}\")\n",
        "                flattened_items.append(\"ï¼› \".join(dict_strings))\n",
        "\n",
        "            # å¤„ç†åˆ—è¡¨ç±»å‹ï¼ˆå¦‚[\"é—®é¢˜1\", \"é—®é¢˜2\"]ï¼‰\n",
        "            elif isinstance(item, list):\n",
        "                list_str = \"â€¢ \".join([str(i) for i in item])\n",
        "                flattened_items.append(f\"â€¢ {list_str}\")\n",
        "\n",
        "            # å­—ç¬¦ä¸²ç›´æ¥ä¿ç•™\n",
        "            else:\n",
        "                flattened_items.append(str(item))\n",
        "        flattened[category] = flattened_items\n",
        "    return flattened\n",
        "\n",
        "# ======================\n",
        "# éŸ³é¢‘å¤„ç† - ä¿®æ”¹éƒ¨åˆ†\n",
        "# ======================\n",
        "def transcribe_audio(audio_path, model_size=\"base\"):\n",
        "    logger.log_step(\"éŸ³é¢‘è½¬å½•\", \"started\", {\"æ¨¡å‹å¤§å°\": model_size, \"éŸ³é¢‘è·¯å¾„\": audio_path})\n",
        "\n",
        "    try:\n",
        "        # ä½¿ç”¨faster-whisperè¿›è¡Œè½¬å½•\n",
        "        from faster_whisper import WhisperModel\n",
        "\n",
        "        # æ ¹æ®GPUå¯ç”¨æ€§é€‰æ‹©è®¡ç®—è®¾å¤‡\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n",
        "\n",
        "        logger.log_step(\"åŠ è½½faster-whisperæ¨¡å‹\", \"info\", {\n",
        "            \"è®¾å¤‡\": device,\n",
        "            \"è®¡ç®—ç±»å‹\": compute_type,\n",
        "            \"æ¨¡å‹å¤§å°\": model_size\n",
        "        })\n",
        "\n",
        "        # åŠ è½½æ¨¡å‹ - ä½¿ç”¨æœ¬åœ°ç¼“å­˜é¿å…é‡å¤ä¸‹è½½\n",
        "        model = WhisperModel(\n",
        "            model_size,\n",
        "            device=device,\n",
        "            compute_type=compute_type,\n",
        "            download_root=\"/content/models\"  # è®¾ç½®æ¨¡å‹ç¼“å­˜ç›®å½•\n",
        "        )\n",
        "\n",
        "        # æ‰§è¡Œè½¬å½•\n",
        "        logger.log_step(\"å¼€å§‹è½¬å½•\", \"processing\")\n",
        "        segments, info = model.transcribe(\n",
        "            audio_path,\n",
        "            beam_size=5,  # å¹³è¡¡é€Ÿåº¦å’Œå‡†ç¡®åº¦\n",
        "            vad_filter=True,  # å¯ç”¨è¯­éŸ³æ´»åŠ¨æ£€æµ‹\n",
        "            word_timestamps=False  # ä¸éœ€è¦å•è¯çº§æ—¶é—´æˆ³\n",
        "        )\n",
        "\n",
        "        # æ£€æµ‹è¯­è¨€\n",
        "        detected_lang = info.language\n",
        "        logger.log_step(\"è¯­è¨€æ£€æµ‹\", \"success\", {\"è¯­è¨€\": detected_lang})\n",
        "\n",
        "        # æ”¶é›†è½¬å½•æ–‡æœ¬\n",
        "        transcription = \"\"\n",
        "        segment_list = []\n",
        "\n",
        "        # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºè½¬å½•è¿‡ç¨‹\n",
        "        with tqdm(total=info.duration, unit='sec', desc=\"è½¬å½•è¿›åº¦\") as pbar:\n",
        "            for segment in segments:\n",
        "                segment_list.append(segment.text)\n",
        "                pbar.update(segment.end - pbar.n)\n",
        "\n",
        "        transcription = \" \".join(segment_list)\n",
        "\n",
        "        logger.log_step(\"éŸ³é¢‘è½¬å½•\", \"success\", {\n",
        "            \"å­—ç¬¦æ•°\": len(transcription),\n",
        "            \"æ£€æµ‹è¯­è¨€\": detected_lang,\n",
        "            \"éŸ³é¢‘æ—¶é•¿\": f\"{info.duration:.2f}ç§’\"\n",
        "        })\n",
        "\n",
        "        return transcription, detected_lang\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"éŸ³é¢‘è½¬å½•\", \"failed\", error=e)\n",
        "        # å›é€€åˆ°åŸå§‹whisper\n",
        "        logger.log_step(\"å°è¯•ä½¿ç”¨åŸå§‹whisper\", \"fallback\")\n",
        "        try:\n",
        "            model = whisper.load_model(model_size)\n",
        "            result = model.transcribe(audio_path)\n",
        "            return result[\"text\"], result[\"language\"]\n",
        "        except Exception as fallback_error:\n",
        "            logger.log_step(\"åŸå§‹whisperå›é€€å¤±è´¥\", \"failed\", error=fallback_error)\n",
        "            raise RuntimeError(f\"è½¬å½•å¤±è´¥: {str(e)} | å›é€€å¤±è´¥: {str(fallback_error)}\")\n",
        "# ======================\n",
        "# ä¼šè®®åˆ†ææ¨¡å‹ä¸å¤„ç†\n",
        "# ======================\n",
        "class MeetingAnalysis(BaseModel):\n",
        "    meeting_title: str = Field(description=\"ä¼šè®®æ ‡é¢˜\")\n",
        "    participants: list[str] = Field(description=\"å‚ä¸è€…åå•\")\n",
        "    summary: str = Field(description=\"3-5æ®µä¼šè®®æ€»ç»“\")\n",
        "    key_points: dict = Field(description=\"æŒ‰concernsã€decisionsã€updatesã€risksåˆ†ç»„çš„å…³é”®ç‚¹ï¼ˆå‡ä¸ºæ•°ç»„ï¼‰\")\n",
        "    action_items: list[dict] = Field(description=\"è¡ŒåŠ¨é¡¹åˆ—è¡¨ï¼ŒåŒ…å«taskã€assigneeã€due_date\")\n",
        "    meeting_type: str = Field(description=\"ä¼šè®®ç±»å‹\")\n",
        "    platform: str = Field(description=\"ä¼šè®®å¹³å°\")\n",
        "\n",
        "def setup_langchain_chains(language='zh'):\n",
        "    lang_map = {\n",
        "        'zh': \"ç”¨ä¸­æ–‡åˆ†æä¼šè®®è®°å½•ï¼Œè¾“å‡ºä¸¥æ ¼ç¬¦åˆJSONæ ¼å¼ï¼Œkey_pointsçš„å­å­—æ®µå‡ä¸ºæ•°ç»„ï¼ˆç”¨[]åŒ…è£¹ï¼‰\",\n",
        "        'en': \"Analyze the meeting transcript in English, output strict JSON with key_points as arrays\",\n",
        "        'fr': \"Analyser le procÃ¨s-verbal en franÃ§ais, sortie JSON stricte avec key_points en tableaux\"\n",
        "    }\n",
        "    lang_instruction = lang_map.get(language[:2], lang_map['zh'])\n",
        "\n",
        "    parser = JsonOutputParser(pydantic_object=MeetingAnalysis)\n",
        "\n",
        "    prompt_template = PromptTemplate(\n",
        "        template=\"\"\"\n",
        "        {lang_instruction}\n",
        "\n",
        "        {format_instructions}\n",
        "\n",
        "        ### ä¼šè®®è®°å½•:\n",
        "        {transcript}\n",
        "\n",
        "        è¯·ä¸¥æ ¼æŒ‰ç…§æ ¼å¼è¦æ±‚è¾“å‡ºï¼Œç¡®ä¿JSONç»“æ„æ­£ç¡®ã€‚\n",
        "        \"\"\",\n",
        "        input_variables=[\"transcript\"],\n",
        "        partial_variables={\n",
        "            \"lang_instruction\": lang_instruction,\n",
        "            \"format_instructions\": parser.get_format_instructions()\n",
        "        }\n",
        "    )\n",
        "\n",
        "    llm = ChatOpenAI(\n",
        "        openai_api_key=OPENAI_API_KEY,\n",
        "        temperature=0.3,\n",
        "        model=\"gpt-3.5-turbo\"\n",
        "    )\n",
        "\n",
        "    # ä½¿ç”¨æ–°çš„é“¾å¼ç»“æ„\n",
        "    analysis_chain = prompt_template | llm | parser\n",
        "\n",
        "    return analysis_chain\n",
        "\n",
        "def analyze_meeting(transcript, language='zh'):\n",
        "    logger.log_step(\"åˆ†æä¼šè®®\", \"started\", {\"è¯­è¨€\": language})\n",
        "    print(\"\\nå¼€å§‹åˆ†æä¼šè®®å†…å®¹...\")\n",
        "\n",
        "    try:\n",
        "        analysis_chain = setup_langchain_chains(language)\n",
        "        processed_transcript = transcript[:15000]\n",
        "        print(f\"ä½¿ç”¨çš„è½¬å½•æ–‡æœ¬é•¿åº¦: {len(processed_transcript)}å­—ç¬¦\")\n",
        "\n",
        "        parsed = analysis_chain.invoke({\"transcript\": processed_transcript})\n",
        "\n",
        "        parsed[\"language\"] = language\n",
        "        parsed[\"date\"] = datetime.datetime.now().isoformat()\n",
        "\n",
        "        if not parsed.get(\"action_items\"):\n",
        "            logger.log_step(\"æ£€æŸ¥è¡ŒåŠ¨é¡¹\", \"warning\", \"æœªæ£€æµ‹åˆ°è¡ŒåŠ¨é¡¹\")\n",
        "            print(\"âš ï¸ æœªæ£€æµ‹åˆ°è¡ŒåŠ¨é¡¹\")\n",
        "            parsed[\"fallback_used\"] = True\n",
        "        else:\n",
        "            parsed[\"fallback_used\"] = False\n",
        "\n",
        "        logger.log_step(\"åˆ†æä¼šè®®\", \"success\", {\n",
        "            \"æ ‡é¢˜\": parsed[\"meeting_title\"],\n",
        "            \"å‚ä¸è€…æ•°é‡\": len(parsed[\"participants\"]),\n",
        "            \"è¡ŒåŠ¨é¡¹æ•°é‡\": len(parsed[\"action_items\"])\n",
        "        })\n",
        "        print(f\"âœ… ä¼šè®®åˆ†æå®Œæˆ (æ ‡é¢˜: {parsed['meeting_title']})\")\n",
        "        return parsed\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"åˆ†æä¼šè®®å¤±è´¥: {str(e)}\"\n",
        "        logger.log_step(\"åˆ†æä¼šè®®\", \"failed\", error=error_msg)\n",
        "        print(f\"âŒ {error_msg}\")\n",
        "        return {\"error\": error_msg, \"fallback_used\": True}\n",
        "\n",
        "# ======================\n",
        "# NotionæŠ¥å‘Šç”Ÿæˆ\n",
        "# ======================\n",
        "def create_notion_report_page(meeting_data, transcript, logs):\n",
        "    logger.log_step(\"åˆ›å»ºNotionæŠ¥å‘Š\", \"started\")\n",
        "\n",
        "    try:\n",
        "        global notion\n",
        "\n",
        "        # éªŒè¯çˆ¶é¡µé¢\n",
        "        try:\n",
        "            parent_page = notion.pages.retrieve(NOTION_PAGE_ID)\n",
        "            page_title = parent_page.get('properties', {}).get('title', {}).get('title', [{}])[0].get('plain_text', 'æ— æ ‡é¢˜')\n",
        "            logger.log_step(\"çˆ¶é¡µé¢æ£€æŸ¥\", \"success\", {\"é¡µé¢ID\": NOTION_PAGE_ID, \"æ ‡é¢˜\": page_title})\n",
        "            print(f\"âœ… æˆåŠŸè®¿é—®çˆ¶é¡µé¢: {page_title} (ID: {NOTION_PAGE_ID[:8]}...)\")\n",
        "        except errors.APIResponseError as e:\n",
        "            if e.status == 404:\n",
        "                error_msg = f\"çˆ¶é¡µé¢ä¸å­˜åœ¨ (ID: {NOTION_PAGE_ID})ã€‚è¯·æ£€æŸ¥IDæ˜¯å¦æ­£ç¡®ã€‚\"\n",
        "            elif e.status == 403:\n",
        "                error_msg = f\"æ²¡æœ‰è®¿é—®çˆ¶é¡µé¢çš„æƒé™ (ID: {NOTION_PAGE_ID})ã€‚è¯·å°†é¡µé¢å…±äº«ç»™Notioné›†æˆã€‚\"\n",
        "            else:\n",
        "                error_msg = f\"è®¿é—®çˆ¶é¡µé¢å¤±è´¥: {str(e)}\"\n",
        "            logger.log_step(\"çˆ¶é¡µé¢æ£€æŸ¥\", \"failed\", error=error_msg)\n",
        "            print(f\"âŒ {error_msg}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            error_msg = f\"çˆ¶é¡µé¢æ£€æŸ¥å‡ºé”™: {str(e)}\"\n",
        "            logger.log_step(\"çˆ¶é¡µé¢æ£€æŸ¥\", \"failed\", error=error_msg)\n",
        "            print(f\"âŒ {error_msg}\")\n",
        "            return None\n",
        "\n",
        "        # åˆ›å»ºå­é¡µé¢\n",
        "        new_page = notion.pages.create(\n",
        "            parent={\"page_id\": NOTION_PAGE_ID},\n",
        "            properties={\n",
        "                \"title\": {\n",
        "                    \"title\": [\n",
        "                        {\n",
        "                            \"text\": {\n",
        "                                \"content\": meeting_data.get(\"meeting_title\", \"ä¼šè®®æŠ¥å‘Š\")[:200]\n",
        "                            }\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "        page_id = new_page[\"id\"]\n",
        "        logger.log_step(\"åˆ›å»ºå­é¡µé¢\", \"success\", {\"é¡µé¢ID\": page_id})\n",
        "        print(f\"âœ… å·²åˆ›å»ºå­é¡µé¢ (ID: {page_id[:8]}...)\")\n",
        "\n",
        "        # æ„å»ºæŠ¥å‘Šå†…å®¹\n",
        "        children_blocks = []\n",
        "\n",
        "        # 1. ä¼šè®®è¯¦æƒ…\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"ä¼šè®®è¯¦æƒ…\"}}]}\n",
        "        })\n",
        "\n",
        "        details_text = f\"\"\"\n",
        "        **æ—¥æœŸ**: {meeting_data.get('date', 'æœªçŸ¥')}\n",
        "        **å‚ä¸è€…**: {', '.join(meeting_data.get('participants', []))}\n",
        "        **è¯­è¨€**: {meeting_data.get('language', 'æœªçŸ¥')}\n",
        "        **å¹³å°**: {meeting_data.get('platform', 'æœªçŸ¥')}\n",
        "        **ä¼šè®®ç±»å‹**: {meeting_data.get('meeting_type', 'æœªçŸ¥')}\n",
        "        \"\"\"\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"paragraph\",\n",
        "            \"paragraph\": {\"rich_text\": [{\"text\": {\"content\": details_text.strip()}}]}\n",
        "        })\n",
        "\n",
        "        # 2. ä¼šè®®æ€»ç»“\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"æ€»ç»“\"}}]}\n",
        "        })\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"paragraph\",\n",
        "            \"paragraph\": {\"rich_text\": [{\"text\": {\"content\": meeting_data.get('summary', '')}}]}\n",
        "        })\n",
        "\n",
        "        # 3. å…³é”®ç‚¹ï¼ˆä¿®å¤åµŒå¥—ç»“æ„é—®é¢˜ï¼‰\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"å…³é”®ç‚¹\"}}]}\n",
        "        })\n",
        "\n",
        "        key_points = meeting_data.get('key_points', {})\n",
        "        key_points = flatten_key_points(key_points)\n",
        "\n",
        "        for category, items in key_points.items():\n",
        "            children_blocks.append({\n",
        "                \"object\": \"block\",\n",
        "                \"type\": \"heading_3\",\n",
        "                \"heading_3\": {\"rich_text\": [{\"text\": {\"content\": category.capitalize()}}]}\n",
        "            })\n",
        "\n",
        "            if items:\n",
        "                for item in items:\n",
        "                    children_blocks.append({\n",
        "                        \"object\": \"block\",\n",
        "                        \"type\": \"bulleted_list_item\",\n",
        "                        \"bulleted_list_item\": {\"rich_text\": [{\"text\": {\"content\": item}}]}\n",
        "                    })\n",
        "\n",
        "        # 4. è¡ŒåŠ¨é¡¹\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"è¡ŒåŠ¨é¡¹\"}}]}\n",
        "        })\n",
        "\n",
        "        table_rows = []\n",
        "        for idx, item in enumerate(meeting_data.get('action_items', [])):\n",
        "            task = item.get('task', '')\n",
        "            assignee = item.get('assignee', 'æœªåˆ†é…')\n",
        "            due_date = item.get('due_date', 'æ— ')\n",
        "\n",
        "            table_rows.append([\n",
        "                [{\"text\": {\"content\": str(idx+1)}}],\n",
        "                [{\"text\": {\"content\": task}}],\n",
        "                [{\"text\": {\"content\": assignee}}],\n",
        "                [{\"text\": {\"content\": due_date}}]\n",
        "            ])\n",
        "\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"table\",\n",
        "            \"table\": {\n",
        "                \"table_width\": 4,\n",
        "                \"has_column_header\": True,\n",
        "                \"has_row_header\": False,\n",
        "                \"children\": [\n",
        "                    {\n",
        "                        \"object\": \"block\",\n",
        "                        \"type\": \"table_row\",\n",
        "                        \"table_row\": {\n",
        "                            \"cells\": [\n",
        "                                [{\"text\": {\"content\": \"åºå·\"}}],\n",
        "                                [{\"text\": {\"content\": \"ä»»åŠ¡\"}}],\n",
        "                                [{\"text\": {\"content\": \"è´Ÿè´£äºº\"}}],\n",
        "                                [{\"text\": {\"content\": \"æˆªæ­¢æ—¥æœŸ\"}}]\n",
        "                            ]\n",
        "                        }\n",
        "                    },\n",
        "                    *[{\n",
        "                        \"object\": \"block\",\n",
        "                        \"type\": \"table_row\",\n",
        "                        \"table_row\": {\"cells\": cells}\n",
        "                    } for cells in table_rows]\n",
        "                ]\n",
        "            }\n",
        "        })\n",
        "\n",
        "        # 5. å¤„ç†æ—¥å¿—\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"å¤„ç†æ—¥å¿—\"}}]}\n",
        "        })\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"code\",\n",
        "            \"code\": {\n",
        "                \"rich_text\": [{\"text\": {\"content\": logger.get_console_log()}}],\n",
        "                \"language\": \"plain text\"\n",
        "            }\n",
        "        })\n",
        "\n",
        "        # æ·»åŠ å†…å®¹åˆ°é¡µé¢\n",
        "        notion.blocks.children.append(\n",
        "            block_id=page_id,\n",
        "            children=children_blocks\n",
        "        )\n",
        "        logger.log_step(\"æ·»åŠ å†…å®¹åˆ°é¡µé¢\", \"success\")\n",
        "        print(f\"âœ… å·²æ·»åŠ å†…å®¹åˆ°å­é¡µé¢\")\n",
        "\n",
        "\n",
        "        # å…³è”æ•°æ®åº“ï¼ˆä¿®æ”¹åï¼‰\n",
        "        if NOTION_DB_ID:\n",
        "            try:\n",
        "                db = notion.databases.retrieve(NOTION_DB_ID)\n",
        "                logger.log_step(\"æ•°æ®åº“éªŒè¯\", \"success\", {\"db_id\": NOTION_DB_ID})\n",
        "\n",
        "        # æ‰‹åŠ¨æŒ‡å®šä½ çš„å…³ç³»å±æ€§åç§°\n",
        "                relation_prop_name = \"relation\"\n",
        "\n",
        "        # éªŒè¯å±æ€§\n",
        "                if relation_prop_name not in db[\"properties\"]:\n",
        "                    raise ValueError(f\"æ•°æ®åº“ä¸­ä¸å­˜åœ¨åä¸ºã€Œ{relation_prop_name}ã€çš„å±æ€§\")\n",
        "                if db[\"properties\"][relation_prop_name][\"type\"] != \"relation\":\n",
        "                    raise ValueError(f\"å±æ€§ã€Œ{relation_prop_name}ã€ä¸æ˜¯å…³ç³»ç±»å‹\")\n",
        "\n",
        "        # å…³è”\n",
        "                notion.pages.update(\n",
        "            page_id=page_id,\n",
        "            properties={\n",
        "                relation_prop_name: {\n",
        "                    \"relation\": [{\"id\": NOTION_DB_ID}]\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "                logger.log_step(\"å…³è”æ•°æ®åº“\", \"success\", {\"ä½¿ç”¨çš„å±æ€§\": relation_prop_name})\n",
        "                print(f\"âœ… å·²é€šè¿‡å±æ€§ã€Œ{relation_prop_name}ã€å…³è”åˆ°æ•°æ®åº“\")\n",
        "            except Exception as e:\n",
        "                logger.log_step(\"å…³è”æ•°æ®åº“\", \"warning\", error=str(e))\n",
        "                print(f\"âš ï¸ å…³è”æ•°æ®åº“å¤±è´¥: {str(e)}ï¼ˆä¸å½±å“æŠ¥å‘Šç”Ÿæˆï¼‰\")\n",
        "\n",
        "        report_url = new_page.get(\"url\", \"\")\n",
        "        logger.log_step(\"ç”ŸæˆNotionæŠ¥å‘Š\", \"success\", {\"URL\": report_url})\n",
        "        return report_url\n",
        "\n",
        "    except Exception as e:\n",
        "        error_details = f\"Notion APIé”™è¯¯: {str(e)}\"\n",
        "        if hasattr(e, 'response') and hasattr(e.response, 'content'):\n",
        "            error_details += f\"\\nå“åº”: {e.response.content.decode('utf-8')}\"\n",
        "        logger.log_step(\"ç”ŸæˆNotionæŠ¥å‘Š\", \"failed\", error=error_details)\n",
        "        print(f\"âŒ Notionæ“ä½œå¤±è´¥: {error_details}\")\n",
        "        return None\n",
        "\n",
        "# ======================\n",
        "# æƒé™æµ‹è¯•å‡½æ•°\n",
        "# ======================\n",
        "def test_notion_permissions():\n",
        "    print(\"\\n=== å¼€å§‹Notionæƒé™æµ‹è¯• ===\")\n",
        "    print(f\"ä½¿ç”¨çš„çˆ¶é¡µé¢ID: {NOTION_PAGE_ID[:8]}... (å®Œæ•´: {NOTION_PAGE_ID})\")\n",
        "\n",
        "    # 1. æµ‹è¯•é›†æˆä»¤ç‰Œæœ‰æ•ˆæ€§\n",
        "    try:\n",
        "        user_info = notion.users.me()\n",
        "        print(f\"âœ… é›†æˆä»¤ç‰Œæœ‰æ•ˆ (æ‰€å±å·¥ä½œç©ºé—´: {user_info.get('workspace_name', 'æœªçŸ¥')})\")\n",
        "    except errors.UnauthorizedError:\n",
        "        print(f\"âŒ é›†æˆä»¤ç‰Œæ— æ•ˆ (NOTION_TOKENé”™è¯¯)\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ éªŒè¯é›†æˆä»¤ç‰Œæ—¶å‡ºé”™: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "    # 2. æµ‹è¯•çˆ¶é¡µé¢è®¿é—®æƒé™\n",
        "    try:\n",
        "        page = notion.pages.retrieve(NOTION_PAGE_ID)\n",
        "        page_title = page.get('properties', {}).get('title', {}).get('title', [{}])[0].get('plain_text', 'æ— æ ‡é¢˜')\n",
        "        print(f\"âœ… æˆåŠŸè®¿é—®çˆ¶é¡µé¢: {page_title}\")\n",
        "    except errors.APIResponseError as e:\n",
        "        if e.status == 404:\n",
        "            print(f\"âŒ çˆ¶é¡µé¢ä¸å­˜åœ¨ (IDé”™è¯¯æˆ–é¡µé¢å·²åˆ é™¤)\")\n",
        "            return False\n",
        "        elif e.status == 403:\n",
        "            print(f\"âŒ æ²¡æœ‰è®¿é—®æƒé™ (è¯·å°†é¡µé¢å…±äº«ç»™é›†æˆ)\")\n",
        "            return False\n",
        "        else:\n",
        "            print(f\"âŒ è®¿é—®é¡µé¢æ—¶å‡ºé”™ (çŠ¶æ€ç : {e.status}): {str(e)}\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ è®¿é—®é¡µé¢æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "    # 3. æµ‹è¯•æ•°æ®åº“è®¿é—®æƒé™\n",
        "    try:\n",
        "        if NOTION_DB_ID:\n",
        "            db = notion.databases.retrieve(NOTION_DB_ID)\n",
        "            print(f\"âœ… æˆåŠŸè®¿é—®æ•°æ®åº“: {db.get('title', [{}])[0].get('plain_text', 'æ— æ ‡é¢˜')}\")\n",
        "    except errors.APIResponseError as e:\n",
        "        print(f\"âš ï¸ æ•°æ®åº“è®¿é—®è­¦å‘Š: {str(e)}ï¼ˆä»å¯ç”ŸæˆæŠ¥å‘Šï¼Œä½†å¯èƒ½æ— æ³•å…³è”ï¼‰\")\n",
        "\n",
        "    return True\n",
        "\n",
        "# ======================\n",
        "# è¾“å…¥å¤„ç†\n",
        "# ======================\n",
        "# åœ¨ handle_transcript_input å‡½æ•°ä¸­æ·»åŠ ç¼ºå¤±çš„éŸ³é¢‘ä¸Šä¼ ä»£ç \n",
        "def handle_transcript_input():\n",
        "    logger.log_step(\"å¤„ç†è¾“å…¥\", \"started\")\n",
        "\n",
        "    print(\"\\n=== è¾“å…¥æ–¹å¼é€‰æ‹© ===\")\n",
        "    print(\"1: ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ (.mp3/.wav/.m4a/.opus)\")\n",
        "    print(\"2: ä¸Šä¼ æ–‡æœ¬æ–‡ä»¶ (.txt/.docx)\")\n",
        "    print(\"3: ç›´æ¥ç²˜è´´æ–‡æœ¬\")\n",
        "\n",
        "    try:\n",
        "        choice = input(\"è¯·é€‰æ‹©è¾“å…¥æ–¹å¼ (1/2/3): \").strip() or \"1\"\n",
        "    except:\n",
        "        choice = \"1\"\n",
        "\n",
        "    if choice == \"1\":\n",
        "        # === æ·»åŠ çš„éŸ³é¢‘ä¸Šä¼ ä»£ç å¼€å§‹ ===\n",
        "        print(\"\\nè¯·ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ (æ”¯æŒ.mp3/.wav/.m4a/.opus):\")\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            logger.log_step(\"ä¸Šä¼ éŸ³é¢‘\", \"failed\", \"æœªä¸Šä¼ ä»»ä½•æ–‡ä»¶\")\n",
        "            print(\"âš ï¸ æœªä¸Šä¼ æ–‡ä»¶ï¼Œè¯·é‡æ–°é€‰æ‹©è¾“å…¥æ–¹å¼\")\n",
        "            return handle_transcript_input()\n",
        "\n",
        "        filename = next(iter(uploaded.keys()))\n",
        "        audio_ext = os.path.splitext(filename)[1].lower()\n",
        "        if audio_ext not in ['.mp3', '.wav', '.m4a', '.opus']:\n",
        "            logger.log_step(\"ä¸Šä¼ éŸ³é¢‘\", \"failed\", f\"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {filename}\")\n",
        "            print(f\"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {filename} (è¯·ä¸Šä¼ .mp3/.wav/.m4a/.opus)\")\n",
        "            return handle_transcript_input()\n",
        "\n",
        "        # ä¿å­˜ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶\n",
        "        audio_path = f\"/tmp/{filename}\"\n",
        "        with open(audio_path, 'wb') as f:\n",
        "            f.write(uploaded[filename])\n",
        "        logger.log_step(\"ä¸Šä¼ éŸ³é¢‘\", \"success\", {\"æ–‡ä»¶å\": filename, \"è·¯å¾„\": audio_path})\n",
        "        print(f\"âœ… å·²ä¸Šä¼ éŸ³é¢‘: {filename}\")\n",
        "        # === æ·»åŠ çš„éŸ³é¢‘ä¸Šä¼ ä»£ç ç»“æŸ ===\n",
        "\n",
        "        # å¢å¼ºæ¨¡å‹é€‰æ‹©\n",
        "        print(\"\\nâš¡ é€‰æ‹©è½¬å½•æ¨¡å‹ (ä½¿ç”¨faster-whisper):\")\n",
        "        print(\"1: æé€Ÿ (tiny, æœ€å¿«ä½†ç²¾åº¦è¾ƒä½)\")\n",
        "        print(\"2: å¿«é€Ÿ (base, æ¨èæ—¥å¸¸ä½¿ç”¨)\")\n",
        "        print(\"3: å¹³è¡¡ (small, é€Ÿåº¦å’Œç²¾åº¦å¹³è¡¡)\")\n",
        "        print(\"4: é«˜ç²¾åº¦ (medium, ä¼šè®®è®°å½•æ¨è)\")\n",
        "        print(\"5: ä¸“ä¸šçº§ (large, æœ€é«˜ç²¾åº¦)\")\n",
        "\n",
        "        try:\n",
        "            model_choice = input(\"è¯·é€‰æ‹©æ¨¡å‹ (1-5): \").strip() or \"4\"\n",
        "        except:\n",
        "            model_choice = \"4\"\n",
        "\n",
        "        model_map = {\n",
        "            \"1\": \"tiny\",\n",
        "            \"2\": \"base\",\n",
        "            \"3\": \"small\",\n",
        "            \"4\": \"medium\",\n",
        "            \"5\": \"large\"\n",
        "        }\n",
        "\n",
        "        model_size = model_map.get(model_choice, \"medium\")\n",
        "        print(f\"ä½¿ç”¨æ¨¡å‹: {model_size}\")\n",
        "\n",
        "        # è·å–éŸ³é¢‘æ—¶é•¿\n",
        "        duration = get_audio_duration(audio_path)\n",
        "        print(f\"éŸ³é¢‘æ—¶é•¿: {duration:.1f}ç§’ï¼Œå¼€å§‹è½¬å½•...\")\n",
        "\n",
        "        # æ·»åŠ æ€§èƒ½æç¤º\n",
        "        if duration > 600:  # è¶…è¿‡10åˆ†é’Ÿ\n",
        "            print(\"â³ è¾ƒé•¿éŸ³é¢‘å¤„ç†ä¸­... è¯·è€å¿ƒç­‰å¾… (å¯ä½¿ç”¨Colab Proè·å¾—GPUåŠ é€Ÿ)\")\n",
        "        elif model_size in [\"medium\", \"large\"]:\n",
        "            print(\"ğŸ” ä½¿ç”¨é«˜ç²¾åº¦æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´...\")\n",
        "\n",
        "        transcript, detected_lang = transcribe_audio(audio_path, model_size)\n",
        "        os.unlink(audio_path)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶\n",
        "\n",
        "        print(f\"âœ… è½¬å½•å®Œæˆ (è¯­è¨€: {detected_lang})\")\n",
        "        return transcript, detected_lang\n",
        "\n",
        "    # ... [å…¶ä»–è¾“å…¥æ–¹å¼çš„ä»£ç ä¿æŒä¸å˜] ...\n",
        "\n",
        "    elif choice == \"2\":\n",
        "        # æ–‡æœ¬æ–‡ä»¶\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            logger.log_step(\"ä¸Šä¼ æ–‡æœ¬\", \"failed\", \"æœªä¸Šä¼ ä»»ä½•æ–‡ä»¶\")\n",
        "            print(\"âš ï¸ æœªä¸Šä¼ æ–‡ä»¶ï¼Œåˆ‡æ¢åˆ°ç›´æ¥ç²˜è´´\")\n",
        "            return handle_transcript_input()\n",
        "\n",
        "        filename = next(iter(uploaded.keys()))\n",
        "        logger.log_step(\"ä¸Šä¼ æ–‡æœ¬\", \"success\", {\"æ–‡ä»¶å\": filename})\n",
        "        print(f\"âœ… å·²ä¸Šä¼ æ–‡ä»¶: {filename}\")\n",
        "\n",
        "        try:\n",
        "            if filename.endswith('.txt'):\n",
        "                transcript = uploaded[filename].decode('utf-8')\n",
        "            elif filename.endswith('.docx'):\n",
        "                with tempfile.NamedTemporaryFile(delete=False, suffix='.docx') as tmp:\n",
        "                    tmp.write(uploaded[filename])\n",
        "                    doc = Document(tmp.name)\n",
        "                    transcript = \"\\n\".join(p.text for p in doc.paragraphs)\n",
        "                    os.unlink(tmp.name)\n",
        "            else:\n",
        "                raise ValueError(f\"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {filename} (æ”¯æŒ: .txt, .docx)\")\n",
        "\n",
        "            # æ£€æµ‹è¯­è¨€\n",
        "            lang = detect(transcript[:500]) if transcript else 'zh'\n",
        "            logger.log_step(\"æ£€æµ‹è¯­è¨€\", \"success\", {\"è¯­è¨€\": lang})\n",
        "            print(f\"âœ… è¯»å–å®Œæˆ (æ£€æµ‹è¯­è¨€: {lang})\")\n",
        "            return transcript, lang\n",
        "        except Exception as e:\n",
        "            logger.log_step(\"å¤„ç†æ–‡æœ¬æ–‡ä»¶\", \"failed\", error=str(e))\n",
        "            print(f\"âŒ å¤„ç†æ–‡ä»¶å‡ºé”™: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    elif choice == \"3\":\n",
        "        # ç›´æ¥ç²˜è´´\n",
        "        print(\"\\nè¯·ç²˜è´´ä¼šè®®è®°å½• (ç²˜è´´åæŒ‰Enterï¼Œè¾“å…¥ç©ºè¡Œç»“æŸ):\")\n",
        "        lines = []\n",
        "        while True:\n",
        "            line = input()\n",
        "            if not line:\n",
        "                break\n",
        "            lines.append(line)\n",
        "        transcript = \"\\n\".join(lines)\n",
        "\n",
        "        if not transcript.strip():\n",
        "            logger.log_step(\"è¾“å…¥æ–‡æœ¬\", \"failed\", \"æœªè¾“å…¥ä»»ä½•å†…å®¹\")\n",
        "            print(\"âš ï¸ æœªè¾“å…¥ä»»ä½•å†…å®¹ï¼Œé‡æ–°é€‰æ‹©è¾“å…¥æ–¹å¼\")\n",
        "            return handle_transcript_input()\n",
        "\n",
        "        # æ£€æµ‹è¯­è¨€\n",
        "        try:\n",
        "            lang = detect(transcript[:500])\n",
        "            logger.log_step(\"æ£€æµ‹è¯­è¨€\", \"success\", {\"è¯­è¨€\": lang})\n",
        "            print(f\"âœ… å·²è¾“å…¥æ–‡æœ¬ (æ£€æµ‹è¯­è¨€: {lang})\")\n",
        "        except:\n",
        "            lang = 'zh'\n",
        "            logger.log_step(\"æ£€æµ‹è¯­è¨€\", \"warning\", \"ä½¿ç”¨é»˜è®¤è¯­è¨€ä¸­æ–‡\")\n",
        "            print(f\"âœ… å·²è¾“å…¥æ–‡æœ¬ (ä½¿ç”¨é»˜è®¤è¯­è¨€: ä¸­æ–‡)\")\n",
        "\n",
        "        return transcript, lang\n",
        "\n",
        "    else:\n",
        "        logger.log_step(\"é€‰æ‹©è¾“å…¥æ–¹å¼\", \"warning\", \"æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤éŸ³é¢‘è¾“å…¥\")\n",
        "        print(\"âš ï¸ æ— æ•ˆé€‰æ‹©ï¼Œé»˜è®¤ä½¿ç”¨éŸ³é¢‘è¾“å…¥\")\n",
        "        return handle_transcript_input()\n",
        "\n",
        "# ======================\n",
        "# ä¸»å‡½æ•°\n",
        "# ======================\n",
        "def main():\n",
        "    logger.log_step(\"å·¥ä½œæµç¨‹\", \"started\")\n",
        "    print(\"=== ä¼šè®®è®°å½•å¤„ç†å·¥å…· ===\")\n",
        "\n",
        "    try:\n",
        "        if not test_notion_permissions():\n",
        "            print(\"\\nâŒ æƒé™æµ‹è¯•æœªé€šè¿‡ï¼Œè¯·å…ˆè§£å†³ä¸Šè¿°é—®é¢˜\")\n",
        "            log_file = logger.save_logs(\"error_logs.json\")\n",
        "            print(f\"é”™è¯¯æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}\")\n",
        "            return\n",
        "\n",
        "        transcript, language = handle_transcript_input()\n",
        "        logger.log_metric(\"è½¬å½•æ–‡æœ¬é•¿åº¦\", len(transcript))\n",
        "\n",
        "        meeting_data = analyze_meeting(transcript, language)\n",
        "        if \"error\" in meeting_data:\n",
        "            raise RuntimeError(f\"åˆ†æå¤±è´¥: {meeting_data['error']}\")\n",
        "\n",
        "        print(\"\\nå¼€å§‹åˆ›å»ºNotionæŠ¥å‘Š...\")\n",
        "        report_url = create_notion_report_page(meeting_data, transcript, logger.logs)\n",
        "\n",
        "        if not report_url:\n",
        "            raise RuntimeError(\"åˆ›å»ºNotionæŠ¥å‘Šå¤±è´¥\")\n",
        "\n",
        "        log_file = logger.save_logs()\n",
        "        print(f\"\\nğŸ‰ å¤„ç†å®Œæˆï¼\")\n",
        "        print(f\"ğŸ“„ ä¼šè®®æŠ¥å‘ŠURL: {report_url}\")\n",
        "        print(f\"ğŸ“‹ æ—¥å¿—æ–‡ä»¶: {log_file}\")\n",
        "\n",
        "        from IPython.display import HTML\n",
        "        display(HTML(f'<a href=\"{report_url}\" target=\"_blank\">ç‚¹å‡»æ‰“å¼€NotionæŠ¥å‘Š</a>'))\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"å·¥ä½œæµç¨‹\", \"failed\", error=str(e))\n",
        "        log_file = logger.save_logs(\"error_logs.json\")\n",
        "        print(f\"\\nâŒ å¤„ç†å¤±è´¥ï¼\")\n",
        "        print(f\"é”™è¯¯è¯¦æƒ…: {str(e)}\")\n",
        "        print(f\"é”™è¯¯æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8ADRLMjnOY6e",
        "outputId": "aa5fcae5-df36-4c88-d843-1282fbe04400",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping whisper as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: faster-whisper in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: ctranslate2<5,>=4.0 in /usr/local/lib/python3.11/dist-packages (from faster-whisper) (4.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.11/dist-packages (from faster-whisper) (0.33.4)\n",
            "Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.11/dist-packages (from faster-whisper) (0.21.2)\n",
            "Requirement already satisfied: onnxruntime<2,>=1.14 in /usr/local/lib/python3.11/dist-packages (from faster-whisper) (1.22.1)\n",
            "Requirement already satisfied: av>=11 in /usr/local/lib/python3.11/dist-packages (from faster-whisper) (15.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from faster-whisper) (4.67.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (2.0.2)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.11/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2025.7.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (1.1.5)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (6.31.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper) (10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2025.7.14)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper) (1.3.0)\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-39ariy6n\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-39ariy6n\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/lib/python3/dist-packages (from openai-whisper==20250625) (8.10.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.61.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (2.6.0+cpu)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (3.3.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton>=2->openai-whisper==20250625) (75.2.0)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20250625) (0.44.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (2025.7.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20250625) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.2)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "3 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 3 not upgraded.\n",
            "âœ… æ‰€æœ‰å‡­è¯å·²è®¾ç½®\n",
            "=== ä¼šè®®è®°å½•å¤„ç†å·¥å…· ===\n",
            "\n",
            "=== å¼€å§‹Notionæƒé™æµ‹è¯• ===\n",
            "ä½¿ç”¨çš„çˆ¶é¡µé¢ID: 2335fee1... (å®Œæ•´: 2335fee18e378097a863fff646e8b48c)\n",
            "âœ… é›†æˆä»¤ç‰Œæœ‰æ•ˆ (æ‰€å±å·¥ä½œç©ºé—´: æœªçŸ¥)\n",
            "âœ… æˆåŠŸè®¿é—®çˆ¶é¡µé¢: Parent Page\n",
            "âœ… æˆåŠŸè®¿é—®æ•°æ®åº“: ğŸ“„ Meeting Logs\n",
            "\n",
            "=== è¾“å…¥æ–¹å¼é€‰æ‹© ===\n",
            "1: ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ (.mp3/.wav/.m4a/.opus)\n",
            "2: ä¸Šä¼ æ–‡æœ¬æ–‡ä»¶ (.txt/.docx)\n",
            "3: ç›´æ¥ç²˜è´´æ–‡æœ¬\n",
            "\n",
            "è¯·ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ (æ”¯æŒ.mp3/.wav/.m4a/.opus):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2f9983b7-8084-48b3-8e18-40c5e1e33952\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2f9983b7-8084-48b3-8e18-40c5e1e33952\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-1404911285.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2-1404911285.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    754\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         \u001b[0mtranscript\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_transcript_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"è½¬å½•æ–‡æœ¬é•¿åº¦\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2-1404911285.py\u001b[0m in \u001b[0;36mhandle_transcript_input\u001b[0;34m()\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0;31m# === æ·»åŠ çš„éŸ³é¢‘ä¸Šä¼ ä»£ç å¼€å§‹ ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nè¯·ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ (æ”¯æŒ.mp3/.wav/.m4a/.opus):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muploaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ä¸Šä¼ éŸ³é¢‘\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"failed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"æœªä¸Šä¼ ä»»ä½•æ–‡ä»¶\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å®‰è£…å¿…è¦ä¾èµ–ï¼ˆæŒ‡å®šå…¼å®¹ç‰ˆæœ¬ï¼‰\n",
        "!pip uninstall -y whisper\n",
        "!pip install faster-whisper==0.10.0  # é”å®šç‰ˆæœ¬ä»¥é¿å…APIå˜æ›´\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install tqdm python-docx notion-client langdetect langchain==0.1.13 langchain-openai==0.0.8 pydantic==2.5.2 httpx==0.27.0\n",
        "!sudo apt update && sudo apt install ffmpeg -y\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import torch\n",
        "import subprocess\n",
        "import datetime\n",
        "import tempfile\n",
        "from tqdm import tqdm\n",
        "from pydantic import BaseModel, Field\n",
        "import httpx\n",
        "import concurrent.futures\n",
        "from functools import partial\n",
        "\n",
        "# å¯¼å…¥ç¬¬ä¸‰æ–¹åº“\n",
        "from google.colab import files, userdata\n",
        "from notion_client import Client, errors\n",
        "from langdetect import detect, LangDetectException\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# æ¸…é™¤ä»£ç†ç¯å¢ƒå˜é‡ï¼ˆé¿å…ç½‘ç»œè¿æ¥é—®é¢˜ï¼‰\n",
        "for var in ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy']:\n",
        "    if var in os.environ:\n",
        "        del os.environ[var]\n",
        "\n",
        "# åˆå§‹åŒ–Notionå®¢æˆ·ç«¯ï¼ˆä¿®å¤httpxä»£ç†å‚æ•°é—®é¢˜ï¼‰\n",
        "http_client = httpx.Client()\n",
        "http_client.proxies = None  # ç¦ç”¨ä»£ç†\n",
        "\n",
        "notion = Client(\n",
        "    auth=userdata.get('NOTION_TOKEN'),\n",
        "    client=http_client\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# é…ç½®å‚æ•°ä¸åˆå§‹åŒ–\n",
        "# ======================\n",
        "try:\n",
        "    # ä»ç¯å¢ƒå˜é‡è·å–å¯†é’¥\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    NOTION_TOKEN = userdata.get('NOTION_TOKEN')\n",
        "    NOTION_PAGE_ID = userdata.get('NOTION_PAGE_ID')\n",
        "\n",
        "    # é•¿éŸ³é¢‘å¤„ç†å‚æ•°\n",
        "    CHUNK_DURATION = 900  # æ¯å—15åˆ†é’Ÿï¼ˆç§’ï¼‰\n",
        "    OVERLAP_DURATION = 30  # å—é—´é‡å 30ç§’\n",
        "    MAX_CONCURRENT_CHUNKS = 1  # å•çº¿ç¨‹å¤„ç†ï¼Œé¿å…æ–‡ä»¶ç«äº‰\n",
        "    # Notionå—æ•°é‡é™åˆ¶ç›¸å…³å‚æ•°\n",
        "    MAX_TRANSCRIPT_SEGMENTS = 50  # æœ€å¤šæ˜¾ç¤º50æ¡è½¬å½•æ–‡æœ¬ï¼ˆé¿å…å—æ•°é‡è¶…é™ï¼‰\n",
        "\n",
        "    # éªŒè¯å¿…è¦å¯†é’¥\n",
        "    missing_creds = []\n",
        "    if not OPENAI_API_KEY:\n",
        "        missing_creds.append(\"OPENAI_API_KEY\")\n",
        "    if not NOTION_TOKEN:\n",
        "        missing_creds.append(\"NOTION_TOKEN\")\n",
        "    if not NOTION_PAGE_ID:\n",
        "        missing_creds.append(\"NOTION_PAGE_ID\")\n",
        "\n",
        "    if missing_creds:\n",
        "        raise ValueError(f\"ç¼ºå°‘å¿…è¦å‡­è¯: {', '.join(missing_creds)}\")\n",
        "\n",
        "    print(\"âœ… æ‰€æœ‰å‡­è¯å·²é…ç½®å®Œæˆï¼Œå‡†å¤‡å¤„ç†ä¼šè®®å†…å®¹\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ åˆå§‹åŒ–å¤±è´¥: {str(e)}\")\n",
        "    print(\"\\nè®¾ç½®æŒ‡å—:\")\n",
        "    print(\"1. ç‚¹å‡»å·¦ä¾§è¾¹æ çš„é’¥åŒ™å›¾æ ‡ï¼ˆColab Secretsï¼‰\")\n",
        "    print(\"2. æ·»åŠ ä»¥ä¸‹å¯†é’¥:\")\n",
        "    print(\"   - OPENAI_API_KEY: ä½ çš„OpenAI APIå¯†é’¥\")\n",
        "    print(\"   - NOTION_TOKEN: Notioné›†æˆä»¤ç‰Œ\")\n",
        "    print(\"   - NOTION_PAGE_ID: ç”¨äºå­˜å‚¨æŠ¥å‘Šçš„Notioné¡µé¢ID\")\n",
        "    raise\n",
        "\n",
        "# ======================\n",
        "# æ—¥å¿—è®°å½•ç³»ç»Ÿ\n",
        "# ======================\n",
        "class MeetingLogger:\n",
        "    def __init__(self):\n",
        "        self.logs = {\n",
        "            \"start_time\": datetime.datetime.now().isoformat(),\n",
        "            \"steps\": [],\n",
        "            \"chunk_status\": {}\n",
        "        }\n",
        "\n",
        "    def log_step(self, step_name, status, details=None, error=None):\n",
        "        \"\"\"è®°å½•å¤„ç†æ­¥éª¤\"\"\"\n",
        "        entry = {\n",
        "            \"step\": step_name,\n",
        "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "            \"status\": status\n",
        "        }\n",
        "        if details:\n",
        "            entry[\"details\"] = details\n",
        "        if error:\n",
        "            entry[\"error\"] = str(error)\n",
        "        self.logs[\"steps\"].append(entry)\n",
        "\n",
        "    def log_chunk(self, chunk_id, status, error=None):\n",
        "        \"\"\"è®°å½•åˆ†å—å¤„ç†çŠ¶æ€\"\"\"\n",
        "        self.logs[\"chunk_status\"][chunk_id] = {\n",
        "            \"status\": status,\n",
        "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "            \"error\": str(error) if error else None\n",
        "        }\n",
        "\n",
        "    def get_completed_chunks(self):\n",
        "        \"\"\"è·å–æˆåŠŸçš„åˆ†å—ID\"\"\"\n",
        "        return [k for k, v in self.logs[\"chunk_status\"].items() if v[\"status\"] == \"success\"]\n",
        "\n",
        "    def get_failed_chunks(self):\n",
        "        \"\"\"è·å–å¤±è´¥çš„åˆ†å—ID\"\"\"\n",
        "        return [k for k, v in self.logs[\"chunk_status\"].items() if v[\"status\"] == \"failed\"]\n",
        "\n",
        "    def save_logs(self, filename=\"meeting_logs.json\"):\n",
        "        \"\"\"ä¿å­˜æ—¥å¿—åˆ°æ–‡ä»¶\"\"\"\n",
        "        with open(filename, \"w\") as f:\n",
        "            json.dump(self.logs, f, indent=2)\n",
        "        return filename\n",
        "\n",
        "# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ\n",
        "logger = MeetingLogger()\n",
        "\n",
        "# ======================\n",
        "# éŸ³é¢‘å¤„ç†å·¥å…·\n",
        "# ======================\n",
        "def get_audio_duration(audio_path):\n",
        "    \"\"\"è·å–éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"ffmpeg\", \"-i\", audio_path],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            text=True\n",
        "        )\n",
        "        output = result.stdout\n",
        "\n",
        "        duration_match = re.search(r\"Duration: (\\d+:\\d+:\\d+\\.\\d+)\", output)\n",
        "        if not duration_match:\n",
        "            return 0.0\n",
        "\n",
        "        duration_str = duration_match.group(1)\n",
        "        h, m, s = duration_str.split(':')\n",
        "        return float(h) * 3600 + float(m) * 60 + float(s)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"è·å–éŸ³é¢‘æ—¶é•¿\", \"warning\", error=str(e))\n",
        "        return 0.0\n",
        "\n",
        "def split_audio_into_chunks(audio_path):\n",
        "    \"\"\"å°†é•¿éŸ³é¢‘åˆ†å‰²ä¸ºå¸¦é‡å çš„å—\"\"\"\n",
        "    logger.log_step(\"éŸ³é¢‘åˆ†å—\", \"started\")\n",
        "\n",
        "    try:\n",
        "        total_duration = get_audio_duration(audio_path)\n",
        "        if total_duration <= 0:\n",
        "            raise ValueError(\"æ— æ³•è·å–æœ‰æ•ˆéŸ³é¢‘æ—¶é•¿ï¼Œå¯èƒ½æ–‡ä»¶æŸå\")\n",
        "\n",
        "        # è®¡ç®—åˆ†å—æ•°é‡\n",
        "        num_chunks = max(1, int((total_duration + CHUNK_DURATION - OVERLAP_DURATION) //\n",
        "                              (CHUNK_DURATION - OVERLAP_DURATION)))\n",
        "        logger.log_step(\"è®¡ç®—åˆ†å—æ•°é‡\", \"success\", {\"æ€»æ—¶é•¿(åˆ†é’Ÿ)\": f\"{total_duration/60:.1f}\", \"åˆ†å—æ•°\": num_chunks})\n",
        "        print(f\"ğŸ“Š éŸ³é¢‘å°†åˆ†å‰²ä¸º {num_chunks} å—ï¼ˆæ¯å—15åˆ†é’Ÿï¼Œé‡å 30ç§’ï¼‰\")\n",
        "\n",
        "        # åˆ›å»ºä¸´æ—¶ç›®å½•å­˜å‚¨åˆ†å—\n",
        "        chunk_dir = tempfile.mkdtemp()\n",
        "        chunks = []\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_time = i * (CHUNK_DURATION - OVERLAP_DURATION)\n",
        "            end_time = min(start_time + CHUNK_DURATION, total_duration)\n",
        "\n",
        "            # æ ¼å¼åŒ–ä¸ºffmpegæ—¶é—´æ ¼å¼\n",
        "            start_str = str(datetime.timedelta(seconds=start_time))\n",
        "            duration_str = str(datetime.timedelta(seconds=end_time - start_time))\n",
        "\n",
        "            chunk_path = f\"{chunk_dir}/chunk_{i:03d}.wav\"\n",
        "\n",
        "            # ä½¿ç”¨ffmpegåˆ‡å‰²éŸ³é¢‘\n",
        "            subprocess.run(\n",
        "                [\n",
        "                    \"ffmpeg\", \"-y\",  # è¦†ç›–ç°æœ‰æ–‡ä»¶\n",
        "                    \"-i\", audio_path,\n",
        "                    \"-ss\", start_str,  # èµ·å§‹æ—¶é—´\n",
        "                    \"-t\", duration_str,  # æŒç»­æ—¶é—´\n",
        "                    \"-ar\", \"16000\",  # ç»Ÿä¸€é‡‡æ ·ç‡\n",
        "                    \"-ac\", \"1\",  # å•å£°é“\n",
        "                    \"-acodec\", \"pcm_s16le\",  # æ— æŸç¼–ç \n",
        "                    chunk_path\n",
        "                ],\n",
        "                check=True,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE\n",
        "            )\n",
        "\n",
        "            # éªŒè¯åˆ†å—æ–‡ä»¶\n",
        "            if not os.path.exists(chunk_path) or os.path.getsize(chunk_path) < 1024:\n",
        "                raise RuntimeError(f\"åˆ†å— {i} ç”Ÿæˆå¤±è´¥ï¼Œæ–‡ä»¶å¤§å°å¼‚å¸¸\")\n",
        "\n",
        "            chunks.append({\n",
        "                \"id\": i,\n",
        "                \"path\": chunk_path,\n",
        "                \"start_time\": start_time,\n",
        "                \"end_time\": end_time,\n",
        "                \"dir\": chunk_dir  # ä¿ç•™ç›®å½•å¼•ç”¨ï¼Œé¿å…æå‰åˆ é™¤\n",
        "            })\n",
        "\n",
        "        logger.log_step(\"éŸ³é¢‘åˆ†å—\", \"success\")\n",
        "        return chunks\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"éŸ³é¢‘åˆ†å—\", \"failed\", error=str(e))\n",
        "        raise RuntimeError(f\"éŸ³é¢‘åˆ†å—å¤±è´¥: {str(e)}\")\n",
        "\n",
        "# ======================\n",
        "# è¯­éŸ³è½¬å½•ï¼ˆæ ¸å¿ƒä¿®å¤éƒ¨åˆ†ï¼‰\n",
        "# ======================\n",
        "# å…¨å±€æ¨¡å‹å˜é‡ï¼ˆç¡®ä¿å•ä¾‹ï¼‰\n",
        "global_model = None\n",
        "\n",
        "def transcribe_chunk(chunk):\n",
        "    \"\"\"è½¬å½•å•ä¸ªéŸ³é¢‘å—ï¼ˆä¿®å¤Segmentå±æ€§é—®é¢˜ï¼‰\"\"\"\n",
        "    chunk_id = chunk[\"id\"]\n",
        "    chunk_path = chunk[\"path\"]\n",
        "    start_time = chunk[\"start_time\"]\n",
        "\n",
        "    try:\n",
        "        from faster_whisper import WhisperModel\n",
        "        global global_model\n",
        "\n",
        "        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰\n",
        "        if global_model is None:\n",
        "            if not torch.cuda.is_available():\n",
        "                raise RuntimeError(\"è¯·åˆ‡æ¢åˆ°GPUç¯å¢ƒï¼ˆRuntime > Change runtime typeï¼‰\")\n",
        "\n",
        "            print(f\"ğŸ”§ åŠ è½½ {chunk['model_size']} æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œéœ€ä¸‹è½½çº¦1.5GBï¼‰...\")\n",
        "            global_model = WhisperModel(\n",
        "                chunk[\"model_size\"],\n",
        "                device=\"cuda\",\n",
        "                compute_type=\"float16\",\n",
        "                download_root=\"/content/models\"\n",
        "            )\n",
        "            print(f\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ\")\n",
        "\n",
        "        # éªŒè¯æ–‡ä»¶å¯è®¿é—®\n",
        "        if not os.path.exists(chunk_path):\n",
        "            raise FileNotFoundError(f\"åˆ†å—æ–‡ä»¶ä¸å­˜åœ¨: {chunk_path}\")\n",
        "\n",
        "        # æ‰§è¡Œè½¬å½•ï¼ˆä¸ä¾èµ–confidenceå±æ€§ï¼‰\n",
        "        segments, info = global_model.transcribe(\n",
        "            chunk_path,\n",
        "            beam_size=2,\n",
        "            vad_filter=True,  # å¯ç”¨è¯­éŸ³æ´»åŠ¨æ£€æµ‹\n",
        "            vad_parameters=dict(min_silence_duration_ms=300),\n",
        "            language=None  # è‡ªåŠ¨æ£€æµ‹è¯­è¨€\n",
        "        )\n",
        "\n",
        "        # å¤„ç†è½¬å½•ç»“æœï¼ˆç§»é™¤confidenceå±æ€§ï¼‰\n",
        "        timestamped_segments = []\n",
        "        for seg in segments:\n",
        "            timestamped_segments.append({\n",
        "                \"text\": seg.text.strip(),\n",
        "                \"start\": seg.start + start_time,  # è½¬æ¢ä¸ºå…¨å±€æ—¶é—´\n",
        "                \"end\": seg.end + start_time\n",
        "                # ç§»é™¤confidenceå±æ€§ï¼Œå…¼å®¹æ–°ç‰ˆæœ¬API\n",
        "            })\n",
        "\n",
        "        logger.log_chunk(chunk_id, \"success\")\n",
        "        return {\n",
        "            \"chunk_id\": chunk_id,\n",
        "            \"segments\": timestamped_segments,\n",
        "            \"language\": info.language\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_chunk(chunk_id, \"failed\", error=str(e))\n",
        "        print(f\"âš ï¸ åˆ†å— {chunk_id} è½¬å½•å¤±è´¥: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def transcribe_long_audio(audio_path, model_size=\"small.en\"):\n",
        "    \"\"\"è½¬å½•é•¿éŸ³é¢‘ï¼ˆåˆ†å—å¤„ç†ï¼‰\"\"\"\n",
        "    logger.log_step(\"éŸ³é¢‘è½¬å½•\", \"started\", {\"æ¨¡å‹\": model_size})\n",
        "\n",
        "    try:\n",
        "        global global_model\n",
        "        global_model = None  # é‡ç½®æ¨¡å‹\n",
        "\n",
        "        # åˆ†å‰²éŸ³é¢‘ä¸ºå—\n",
        "        chunks = split_audio_into_chunks(audio_path)\n",
        "        num_chunks = len(chunks)\n",
        "\n",
        "        # ä¸ºæ¯ä¸ªåˆ†å—æ·»åŠ æ¨¡å‹å‚æ•°\n",
        "        for chunk in chunks:\n",
        "            chunk[\"model_size\"] = model_size\n",
        "\n",
        "        # è½¬å½•æ‰€æœ‰åˆ†å—ï¼ˆå•çº¿ç¨‹é¿å…æ–‡ä»¶ç«äº‰ï¼‰\n",
        "        print(f\"ğŸš€ å¼€å§‹è½¬å½• {num_chunks} ä¸ªåˆ†å—...\")\n",
        "        results = []\n",
        "        for chunk in tqdm(chunks, desc=\"è½¬å½•è¿›åº¦\"):\n",
        "            result = transcribe_chunk(chunk)\n",
        "            if result:\n",
        "                results.append(result)\n",
        "\n",
        "        # é‡è¯•å¤±è´¥çš„åˆ†å—\n",
        "        failed_ids = logger.get_failed_chunks()\n",
        "        if failed_ids:\n",
        "            print(f\"ğŸ”„ é‡è¯• {len(failed_ids)} ä¸ªå¤±è´¥åˆ†å—...\")\n",
        "            for chunk in chunks:\n",
        "                if chunk[\"id\"] in failed_ids:\n",
        "                    result = transcribe_chunk(chunk)\n",
        "                    if result:\n",
        "                        results.append(result)\n",
        "\n",
        "        # æŒ‰åˆ†å—IDæ’åºå¹¶åˆå¹¶ç»“æœ\n",
        "        results.sort(key=lambda x: x[\"chunk_id\"])\n",
        "        all_segments = []\n",
        "        for res in results:\n",
        "            all_segments.extend(res[\"segments\"])\n",
        "\n",
        "        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆæœ€åæ¸…ç†ï¼Œé¿å…è¯»å–å¤±è´¥ï¼‰\n",
        "        unique_dirs = list({chunk[\"dir\"] for chunk in chunks})\n",
        "        for dir_path in unique_dirs:\n",
        "            if os.path.exists(dir_path):\n",
        "                for f in os.listdir(dir_path):\n",
        "                    os.unlink(os.path.join(dir_path, f))\n",
        "                os.rmdir(dir_path)\n",
        "\n",
        "        # æ£€æµ‹è¯­è¨€\n",
        "        language = results[0][\"language\"] if results else \"en\"\n",
        "\n",
        "        # éªŒè¯è½¬å½•ç»“æœ\n",
        "        if not all_segments:\n",
        "            raise RuntimeError(\"æœªè·å–åˆ°æœ‰æ•ˆè½¬å½•å†…å®¹ï¼Œè¯·æ£€æŸ¥éŸ³é¢‘è´¨é‡\")\n",
        "\n",
        "        logger.log_step(\"éŸ³é¢‘è½¬å½•\", \"success\", {\"æ€»ç‰‡æ®µæ•°\": len(all_segments)})\n",
        "        print(f\"âœ… è½¬å½•å®Œæˆï¼ˆ{len(all_segments)}ä¸ªç‰‡æ®µï¼Œæ£€æµ‹è¯­è¨€: {language}ï¼‰\")\n",
        "        return \" \".join([s[\"text\"] for s in all_segments]), all_segments, language\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"éŸ³é¢‘è½¬å½•\", \"failed\", error=str(e))\n",
        "        raise RuntimeError(f\"è½¬å½•è¿‡ç¨‹å¤±è´¥: {str(e)}\")\n",
        "\n",
        "# ======================\n",
        "# ä¼šè®®å†…å®¹åˆ†æï¼ˆä¿®å¤dictå±æ€§é”™è¯¯ï¼‰\n",
        "# ======================\n",
        "# åˆ†æç»“æœæ•°æ®æ¨¡å‹\n",
        "class ChunkAnalysis(BaseModel):\n",
        "    summary: str = Field(description=\"è¯¥ç‰‡æ®µçš„æ€»ç»“ï¼ˆ100-200å­—ï¼‰\")\n",
        "    key_points: list[str] = Field(description=\"è¯¥ç‰‡æ®µçš„å…³é”®ç‚¹åˆ—è¡¨\")\n",
        "    action_items: list[dict] = Field(description=\"è¡ŒåŠ¨é¡¹åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«taskã€assigneeã€due_date\")\n",
        "    topics: list[str] = Field(description=\"è®¨è®ºçš„è¯é¢˜åˆ—è¡¨\")\n",
        "\n",
        "class FullMeetingAnalysis(BaseModel):\n",
        "    meeting_title: str = Field(description=\"ä¼šè®®æ ‡é¢˜\")\n",
        "    participants: list[str] = Field(description=\"å‚ä¸è€…åå•\")\n",
        "    summary: str = Field(description=\"3-5æ®µå®Œæ•´ä¼šè®®æ€»ç»“\")\n",
        "    key_points: dict = Field(description=\"æŒ‰è¯é¢˜åˆ†ç»„çš„å…¨å±€å…³é”®ç‚¹\")\n",
        "    action_items: list[dict] = Field(description=\"æ±‡æ€»çš„è¡ŒåŠ¨é¡¹\")\n",
        "    meeting_type: str = Field(description=\"ä¼šè®®ç±»å‹\")\n",
        "    topics_flow: list[str] = Field(description=\"ä¼šè®®è¯é¢˜æµè½¬é¡ºåº\")\n",
        "\n",
        "def get_chunk_analysis_chain(language):\n",
        "    \"\"\"åˆ›å»ºåˆ†å—åˆ†æé“¾ï¼ˆæ˜¾å¼ä¼ é€’APIå¯†é’¥ï¼‰\"\"\"\n",
        "    parser = JsonOutputParser(pydantic_object=ChunkAnalysis)\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"åˆ†æä»¥ä¸‹ä¼šè®®ç‰‡æ®µï¼Œæå–å…³é”®ä¿¡æ¯ï¼ˆç”¨{language}ï¼‰ï¼š\\n{format_instructions}\\nä¼šè®®ç‰‡æ®µï¼š{transcript}\",\n",
        "        input_variables=[\"transcript\"],\n",
        "        partial_variables={\n",
        "            \"language\": \"ä¸­æ–‡\" if language.startswith('zh') else \"English\",\n",
        "            \"format_instructions\": parser.get_format_instructions()\n",
        "        }\n",
        "    )\n",
        "    # å…³é”®ä¿®å¤ï¼šæ˜¾å¼ä¼ å…¥APIå¯†é’¥\n",
        "    return prompt | ChatOpenAI(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        temperature=0.3,\n",
        "        openai_api_key=OPENAI_API_KEY  # ç›´æ¥ä½¿ç”¨å…¨å±€å˜é‡ä¸­çš„å¯†é’¥\n",
        "    ) | parser\n",
        "\n",
        "def get_full_analysis_chain(language):\n",
        "    \"\"\"åˆ›å»ºå…¨å±€åˆ†æé“¾ï¼ˆæ˜¾å¼ä¼ é€’APIå¯†é’¥ï¼‰\"\"\"\n",
        "    parser = JsonOutputParser(pydantic_object=FullMeetingAnalysis)\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"åŸºäºä»¥ä¸‹å„ç‰‡æ®µåˆ†æï¼Œç”Ÿæˆå®Œæ•´ä¼šè®®æŠ¥å‘Šï¼ˆç”¨{language}ï¼‰ï¼š\\n{format_instructions}\\nç‰‡æ®µåˆ†æï¼š{chunk_analyses}\",\n",
        "        input_variables=[\"chunk_analyses\"],\n",
        "        partial_variables={\n",
        "            \"language\": \"ä¸­æ–‡\" if language.startswith('zh') else \"English\",\n",
        "            \"format_instructions\": parser.get_format_instructions()\n",
        "        }\n",
        "    )\n",
        "    # å…³é”®ä¿®å¤ï¼šæ˜¾å¼ä¼ å…¥APIå¯†é’¥\n",
        "    return prompt | ChatOpenAI(\n",
        "        model=\"gpt-4\",\n",
        "        temperature=0.3,\n",
        "        openai_api_key=OPENAI_API_KEY  # ç›´æ¥ä½¿ç”¨å…¨å±€å˜é‡ä¸­çš„å¯†é’¥\n",
        "    ) | parser\n",
        "\n",
        "def analyze_meeting(transcript_segments, language='en'):\n",
        "    \"\"\"åˆ†æä¼šè®®å†…å®¹ï¼ˆåˆ†å—åˆ†æ+å…¨å±€æ•´åˆï¼‰\"\"\"\n",
        "    logger.log_step(\"ä¼šè®®åˆ†æ\", \"started\")\n",
        "    print(\"\\nå¼€å§‹åˆ†æä¼šè®®å†…å®¹...\")\n",
        "\n",
        "    try:\n",
        "        # æŒ‰æ—¶é—´åˆ†å‰²ä¸ºåˆ†æå—ï¼ˆ45åˆ†é’Ÿ/å—ï¼‰\n",
        "        ANALYSIS_CHUNK_DURATION = 2700\n",
        "        analysis_chunks = []\n",
        "        current_chunk = []\n",
        "\n",
        "        for seg in transcript_segments:\n",
        "            if not current_chunk:\n",
        "                current_chunk.append(seg)\n",
        "            else:\n",
        "                if seg[\"end\"] - current_chunk[0][\"start\"] <= ANALYSIS_CHUNK_DURATION:\n",
        "                    current_chunk.append(seg)\n",
        "                else:\n",
        "                    analysis_chunks.append(current_chunk)\n",
        "                    current_chunk = [seg]\n",
        "        if current_chunk:\n",
        "            analysis_chunks.append(current_chunk)\n",
        "\n",
        "        print(f\"ğŸ“ å°†ä¼šè®®å†…å®¹åˆ†ä¸º {len(analysis_chunks)} ä¸ªåˆ†æå—\")\n",
        "\n",
        "        # åˆ†å—åˆ†æ\n",
        "        chunk_analyses = []\n",
        "        chunk_chain = get_chunk_analysis_chain(language)\n",
        "\n",
        "        for i, chunk in enumerate(tqdm(analysis_chunks, desc=\"åˆ†æè¿›åº¦\")):\n",
        "            # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„å—æ–‡æœ¬\n",
        "            chunk_text = \"\\n\".join([\n",
        "                f\"[{str(datetime.timedelta(seconds=int(seg['start'])))}] {seg['text']}\"\n",
        "                for seg in chunk\n",
        "            ])\n",
        "\n",
        "            try:\n",
        "                analysis = chunk_chain.invoke({\"transcript\": chunk_text[:12000]})  # é™åˆ¶é•¿åº¦\n",
        "                chunk_analyses.append({\n",
        "                    \"chunk_id\": i,\n",
        "                    \"start_time\": chunk[0][\"start\"],\n",
        "                    \"end_time\": chunk[-1][\"end\"],\n",
        "                    \"analysis\": analysis\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ åˆ†æå— {i} å¤±è´¥: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        if not chunk_analyses:\n",
        "            raise RuntimeError(\"æ‰€æœ‰åˆ†æå—å¤„ç†å¤±è´¥ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š\")\n",
        "\n",
        "        # å…¨å±€æ•´åˆåˆ†æ - ä¿®å¤ï¼šç§»é™¤.dict()è°ƒç”¨ï¼Œå› ä¸ºåˆ†æç»“æœå·²ç»æ˜¯å­—å…¸\n",
        "        full_chain = get_full_analysis_chain(language)\n",
        "        full_analysis = full_chain.invoke({\n",
        "            \"chunk_analyses\": json.dumps([\n",
        "                {\n",
        "                    \"æ—¶é—´æ®µ\": f\"{str(datetime.timedelta(seconds=int(c['start_time'])))} - {str(datetime.timedelta(seconds=int(c['end_time'])))}\",\n",
        "                    \"åˆ†æ\": c[\"analysis\"]  # å…³é”®ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨å­—å…¸å¯¹è±¡\n",
        "                } for c in chunk_analyses\n",
        "            ], ensure_ascii=False)\n",
        "        })\n",
        "\n",
        "        # æ·»åŠ å…ƒæ•°æ® - ä¿®å¤ï¼šå¦‚æœfull_analysisæ˜¯Pydanticæ¨¡å‹ï¼Œå…ˆè½¬å­—å…¸\n",
        "        if hasattr(full_analysis, 'dict'):\n",
        "            full_analysis = full_analysis.dict()\n",
        "\n",
        "        full_analysis[\"language\"] = language\n",
        "        full_analysis[\"date\"] = datetime.datetime.now().isoformat()\n",
        "        full_analysis[\"total_duration\"] = f\"{(transcript_segments[-1]['end'] - transcript_segments[0]['start'])/3600:.2f}å°æ—¶\"\n",
        "\n",
        "        logger.log_step(\"ä¼šè®®åˆ†æ\", \"success\")\n",
        "        print(f\"âœ… ä¼šè®®åˆ†æå®Œæˆï¼ˆ{len(full_analysis['topics_flow'])}ä¸ªè¯é¢˜ï¼Œ{len(full_analysis['action_items'])}ä¸ªè¡ŒåŠ¨é¡¹ï¼‰\")\n",
        "        return full_analysis\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"ä¼šè®®åˆ†æå¤±è´¥: {str(e)}\"\n",
        "        logger.log_step(\"ä¼šè®®åˆ†æ\", \"failed\", error=error_msg)\n",
        "        print(f\"âŒ {error_msg}\")\n",
        "        return {\"error\": error_msg}\n",
        "\n",
        "# ======================\n",
        "# NotionæŠ¥å‘Šç”Ÿæˆï¼ˆä¿®å¤å—æ•°é‡è¶…é™é—®é¢˜ï¼‰\n",
        "# ======================\n",
        "def create_notion_report(meeting_data, transcript_segments):\n",
        "    \"\"\"åœ¨Notionä¸­åˆ›å»ºä¼šè®®æŠ¥å‘Šï¼ˆæ§åˆ¶å—æ•°é‡â‰¤100ï¼‰\"\"\"\n",
        "    logger.log_step(\"åˆ›å»ºNotionæŠ¥å‘Š\", \"started\")\n",
        "\n",
        "    try:\n",
        "        # éªŒè¯çˆ¶é¡µé¢æ˜¯å¦å­˜åœ¨\n",
        "        try:\n",
        "            parent_page = notion.pages.retrieve(NOTION_PAGE_ID)\n",
        "            parent_title = parent_page.get('properties', {}).get('title', {}).get('title', [{}])[0].get('plain_text', 'æ— æ ‡é¢˜é¡µé¢')\n",
        "            print(f\"âœ… æˆåŠŸè®¿é—®çˆ¶é¡µé¢: {parent_title}\")\n",
        "        except errors.APIResponseError as e:\n",
        "            raise RuntimeError(f\"Notionçˆ¶é¡µé¢è®¿é—®å¤±è´¥: {str(e)}\")\n",
        "\n",
        "        # åˆ›å»ºæ–°æŠ¥å‘Šé¡µé¢\n",
        "        new_page = notion.pages.create(\n",
        "            parent={\"page_id\": NOTION_PAGE_ID},\n",
        "            properties={\n",
        "                \"title\": {\n",
        "                    \"title\": [{\"text\": {\"content\": meeting_data.get(\"meeting_title\", \"ä¼šè®®æŠ¥å‘Š\")[:200]}}]\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "        page_id = new_page[\"id\"]\n",
        "\n",
        "        # æ„å»ºæŠ¥å‘Šå†…å®¹å—ï¼ˆæ§åˆ¶æ€»æ•°é‡â‰¤100ï¼‰\n",
        "        children_blocks = []\n",
        "\n",
        "        # 1. ä¼šè®®æ¦‚è§ˆï¼ˆçº¦2ä¸ªå—ï¼‰\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"ä¼šè®®æ¦‚è§ˆ\"}}]}\n",
        "        })\n",
        "        overview_text = f\"\"\"\n",
        "        æ ‡é¢˜: {meeting_data.get('meeting_title', 'æœªå‘½åä¼šè®®')}\n",
        "        æ—¥æœŸ: {meeting_data.get('date', datetime.datetime.now().strftime('%Y-%m-%d'))}\n",
        "        æ€»æ—¶é•¿: {meeting_data.get('total_duration', 'æœªçŸ¥')}\n",
        "        å‚ä¸è€…: {', '.join(meeting_data.get('participants', [])) or 'æœªè¯†åˆ«'}\n",
        "        è¯­è¨€: {meeting_data.get('language', 'æœªçŸ¥')}\n",
        "        \"\"\"\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"paragraph\",\n",
        "            \"paragraph\": {\"rich_text\": [{\"text\": {\"content\": overview_text.strip()}}]}\n",
        "        })\n",
        "\n",
        "        # 2. è¯é¢˜æµè½¬ï¼ˆè¯é¢˜æ•°é‡+1ä¸ªå—ï¼‰\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"è¯é¢˜æµè½¬é¡ºåº\"}}]}\n",
        "        })\n",
        "        # é™åˆ¶è¯é¢˜æ•°é‡ï¼ˆé¿å…å—è¿‡å¤šï¼‰\n",
        "        max_topics = 20  # æœ€å¤šæ˜¾ç¤º20ä¸ªè¯é¢˜\n",
        "        for topic in meeting_data.get('topics_flow', [])[:max_topics]:\n",
        "            children_blocks.append({\n",
        "                \"object\": \"block\",\n",
        "                \"type\": \"numbered_list_item\",\n",
        "                \"numbered_list_item\": {\"rich_text\": [{\"text\": {\"content\": topic}}]}\n",
        "            })\n",
        "\n",
        "        # 3. ä¼šè®®æ€»ç»“ï¼ˆæ€»ç»“æ®µè½æ•°+1ä¸ªå—ï¼‰\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"ä¼šè®®æ€»ç»“\"}}]}\n",
        "        })\n",
        "        # é™åˆ¶æ€»ç»“æ®µè½æ•°\n",
        "        max_summary_paras = 5  # æœ€å¤š5æ®µæ€»ç»“\n",
        "        for para in meeting_data.get('summary', '').split('\\n\\n')[:max_summary_paras]:\n",
        "            if para.strip():\n",
        "                children_blocks.append({\n",
        "                    \"object\": \"block\",\n",
        "                    \"type\": \"paragraph\",\n",
        "                    \"paragraph\": {\"rich_text\": [{\"text\": {\"content\": para.strip()}}]}\n",
        "                })\n",
        "\n",
        "        # 4. å…³é”®è¦ç‚¹ï¼ˆè¯é¢˜æ•°+è¦ç‚¹æ•°+1ä¸ªå—ï¼‰\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"å…³é”®è¦ç‚¹\"}}]}\n",
        "        })\n",
        "        key_points = meeting_data.get('key_points', {})\n",
        "        max_key_topics = 10  # æœ€å¤š10ä¸ªå…³é”®è¯é¢˜\n",
        "        for topic, points in list(key_points.items())[:max_key_topics]:\n",
        "            children_blocks.append({\n",
        "                \"object\": \"block\",\n",
        "                \"type\": \"heading_3\",\n",
        "                \"heading_3\": {\"rich_text\": [{\"text\": {\"content\": topic}}]}\n",
        "            })\n",
        "            # é™åˆ¶æ¯ä¸ªè¯é¢˜çš„è¦ç‚¹æ•°é‡\n",
        "            max_points_per_topic = 5  # æ¯ä¸ªè¯é¢˜æœ€å¤š5ä¸ªè¦ç‚¹\n",
        "            for point in points[:max_points_per_topic]:\n",
        "                children_blocks.append({\n",
        "                    \"object\": \"block\",\n",
        "                    \"type\": \"bulleted_list_item\",\n",
        "                    \"bulleted_list_item\": {\"rich_text\": [{\"text\": {\"content\": point}}]}\n",
        "                })\n",
        "\n",
        "        # 5. è¡ŒåŠ¨é¡¹è¡¨æ ¼ï¼ˆè¡ŒåŠ¨é¡¹æ•°+2ä¸ªå—ï¼‰\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"è¡ŒåŠ¨é¡¹\"}}]}\n",
        "        })\n",
        "        table_rows = []\n",
        "        max_actions = 20  # æœ€å¤šæ˜¾ç¤º20ä¸ªè¡ŒåŠ¨é¡¹\n",
        "        for idx, item in enumerate(meeting_data.get('action_items', [])[:max_actions]):\n",
        "            table_rows.append([\n",
        "                [{\"text\": {\"content\": str(idx+1)}}],\n",
        "                [{\"text\": {\"content\": item.get('task', '')}}],\n",
        "                [{\"text\": {\"content\": item.get('assignee', 'æœªåˆ†é…')}}],\n",
        "                [{\"text\": {\"content\": item.get('due_date', 'æ— ')}}]\n",
        "            ])\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"table\",\n",
        "            \"table\": {\n",
        "                \"table_width\": 4,\n",
        "                \"has_column_header\": True,\n",
        "                \"children\": [\n",
        "                    {\n",
        "                        \"object\": \"block\",\n",
        "                        \"type\": \"table_row\",\n",
        "                        \"table_row\": {\n",
        "                            \"cells\": [\n",
        "                                [{\"text\": {\"content\": \"åºå·\"}}],\n",
        "                                [{\"text\": {\"content\": \"ä»»åŠ¡\"}}],\n",
        "                                [{\"text\": {\"content\": \"è´Ÿè´£äºº\"}}],\n",
        "                                [{\"text\": {\"content\": \"æˆªæ­¢æ—¥æœŸ\"}}]\n",
        "                            ]\n",
        "                        }\n",
        "                    },\n",
        "                    *[{\n",
        "                        \"object\": \"block\",\n",
        "                        \"type\": \"table_row\",\n",
        "                        \"table_row\": {\"cells\": cells}\n",
        "                    } for cells in table_rows]\n",
        "                ]\n",
        "            }\n",
        "        })\n",
        "\n",
        "        # 6. è½¬å½•æ–‡æœ¬ï¼ˆé™åˆ¶ä¸ºMAX_TRANSCRIPT_SEGMENTSæ¡ï¼Œé¿å…å—è¶…é™ï¼‰\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": f\"ä¼šè®®è½¬å½•æ–‡æœ¬ï¼ˆèŠ‚é€‰ï¼Œå…±{len(transcript_segments)}æ¡ï¼‰\"}}]}\n",
        "        })\n",
        "        # åªæ˜¾ç¤ºå‰MAX_TRANSCRIPT_SEGMENTSæ¡è½¬å½•æ–‡æœ¬\n",
        "        for seg in transcript_segments[:MAX_TRANSCRIPT_SEGMENTS]:\n",
        "            time_str = str(datetime.timedelta(seconds=int(seg[\"start\"])))\n",
        "            children_blocks.append({\n",
        "                \"object\": \"block\",\n",
        "                \"type\": \"paragraph\",\n",
        "                \"paragraph\": {\"rich_text\": [{\"text\": {\"content\": f\"[{time_str}] {seg['text']}\"}}]}\n",
        "            })\n",
        "\n",
        "        # æ£€æŸ¥æ€»å—æ•°ï¼Œç¡®ä¿ä¸è¶…è¿‡100\n",
        "        if len(children_blocks) > 100:\n",
        "            # ç´§æ€¥æˆªæ–­ï¼ˆä¿ç•™æ ¸å¿ƒå†…å®¹ï¼‰\n",
        "            children_blocks = children_blocks[:100]\n",
        "            print(f\"âš ï¸ è­¦å‘Šï¼šå†…å®¹å—æ•°é‡è¶…é™ï¼Œå·²æˆªæ–­ä¸º100ä¸ªå—\")\n",
        "\n",
        "        # å°†å†…å®¹æ·»åŠ åˆ°é¡µé¢\n",
        "        notion.blocks.children.append(block_id=page_id, children=children_blocks)\n",
        "        report_url = new_page.get(\"url\", \"\")\n",
        "\n",
        "        logger.log_step(\"åˆ›å»ºNotionæŠ¥å‘Š\", \"success\", {\"æŠ¥å‘ŠURL\": report_url, \"æ€»å—æ•°\": len(children_blocks)})\n",
        "        print(f\"âœ… NotionæŠ¥å‘Šå·²ç”Ÿæˆï¼ˆæ€»å—æ•°: {len(children_blocks)}ï¼‰\")\n",
        "        return report_url\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"åˆ›å»ºNotionæŠ¥å‘Š\", \"failed\", error=str(e))\n",
        "        print(f\"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# ======================\n",
        "# è¾“å…¥å¤„ç†\n",
        "# ======================\n",
        "def handle_input():\n",
        "    \"\"\"å¤„ç†ç”¨æˆ·è¾“å…¥ï¼ˆéŸ³é¢‘æˆ–æ–‡æœ¬ï¼‰\"\"\"\n",
        "    print(\"\\n=== è¯·é€‰æ‹©è¾“å…¥æ–¹å¼ ===\")\n",
        "    print(\"1: ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ (.mp3/.wav/.m4a/.opus)\")\n",
        "    print(\"2: ä¸Šä¼ å¸¦æ—¶é—´æˆ³çš„è½¬å½•æ–‡æœ¬ (.txt)\")\n",
        "\n",
        "    try:\n",
        "        choice = input(\"è¯·é€‰æ‹© (1/2): \").strip() or \"1\"\n",
        "    except:\n",
        "        choice = \"1\"\n",
        "\n",
        "    if choice == \"1\":\n",
        "        # å¤„ç†éŸ³é¢‘è¾“å…¥\n",
        "        print(\"\\nè¯·ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ï¼ˆæ”¯æŒ2-3å°æ—¶ä¼šè®®å½•éŸ³ï¼‰:\")\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            print(\"âš ï¸ æœªæ£€æµ‹åˆ°ä¸Šä¼ æ–‡ä»¶ï¼Œè¯·é‡è¯•\")\n",
        "            return handle_input()\n",
        "\n",
        "        filename = next(iter(uploaded.keys()))\n",
        "        audio_ext = os.path.splitext(filename)[1].lower()\n",
        "        supported_ext = ['.mp3', '.wav', '.m4a', '.opus']\n",
        "\n",
        "        if audio_ext not in supported_ext:\n",
        "            print(f\"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼ˆæ”¯æŒ: {', '.join(supported_ext)}ï¼‰\")\n",
        "            return handle_input()\n",
        "\n",
        "        # ä¿å­˜éŸ³é¢‘æ–‡ä»¶\n",
        "        audio_path = f\"/tmp/{filename}\"\n",
        "        with open(audio_path, 'wb') as f:\n",
        "            f.write(uploaded[filename])\n",
        "\n",
        "        # æ£€æŸ¥éŸ³é¢‘æ—¶é•¿\n",
        "        duration = get_audio_duration(audio_path)\n",
        "        if duration < 3600:  # å°äº1å°æ—¶\n",
        "            print(f\"âš ï¸ æ£€æµ‹åˆ°éŸ³é¢‘æ—¶é•¿è¾ƒçŸ­ï¼ˆ{duration/60:.1f}åˆ†é’Ÿï¼‰\")\n",
        "            if input(\"æ˜¯å¦ç»§ç»­ä½¿ç”¨é•¿ä¼šè®®æ¨¡å¼å¤„ç†? (y/n): \").strip().lower() != 'y':\n",
        "                os.unlink(audio_path)\n",
        "                print(\"å·²åˆ‡æ¢åˆ°æ™®é€šæ¨¡å¼\")\n",
        "                return handle_input()\n",
        "\n",
        "        print(f\"ğŸµ éŸ³é¢‘ä¿¡æ¯: {filename}ï¼ˆ{duration/60:.1f}åˆ†é’Ÿï¼‰\")\n",
        "\n",
        "        # é€‰æ‹©è½¬å½•æ¨¡å‹\n",
        "        print(\"\\nâš¡ è¯·é€‰æ‹©è½¬å½•æ¨¡å‹:\")\n",
        "        print(\"1: base.en - å¿«é€Ÿæ¨¡å¼ï¼ˆé€‚åˆæ¸…æ™°è¯­éŸ³ï¼‰\")\n",
        "        print(\"2: small.en - å¹³è¡¡æ¨¡å¼ï¼ˆæ¨èï¼Œé€Ÿåº¦ä¸ç²¾åº¦å…¼é¡¾ï¼‰\")\n",
        "        print(\"3: medium.en - é«˜ç²¾åº¦æ¨¡å¼ï¼ˆé€‚åˆå¤æ‚ä¼šè®®ï¼‰\")\n",
        "\n",
        "        model_choice = input(\"è¯·é€‰æ‹© (1-3ï¼Œé»˜è®¤2): \").strip() or \"2\"\n",
        "        model_map = {\"1\": \"base.en\", \"2\": \"small.en\", \"3\": \"medium.en\"}\n",
        "        model_size = model_map.get(model_choice, \"small.en\")\n",
        "        print(f\"å°†ä½¿ç”¨ {model_size} æ¨¡å‹è¿›è¡Œè½¬å½•\")\n",
        "\n",
        "        # æ‰§è¡Œè½¬å½•\n",
        "        try:\n",
        "            transcript, segments, language = transcribe_long_audio(audio_path, model_size)\n",
        "            return transcript, segments, language\n",
        "        except Exception as e:\n",
        "            if os.path.exists(audio_path):\n",
        "                os.unlink(audio_path)\n",
        "            raise\n",
        "\n",
        "    elif choice == \"2\":\n",
        "        # å¤„ç†æ–‡æœ¬è¾“å…¥\n",
        "        print(\"\\nè¯·ä¸Šä¼ å¸¦æ—¶é—´æˆ³çš„è½¬å½•æ–‡æœ¬ï¼ˆæ ¼å¼ç¤ºä¾‹: [00:05:10] å‘è¨€äºº: ...ï¼‰:\")\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            print(\"âš ï¸ æœªæ£€æµ‹åˆ°ä¸Šä¼ æ–‡ä»¶ï¼Œè¯·é‡è¯•\")\n",
        "            return handle_input()\n",
        "\n",
        "        filename = next(iter(uploaded.keys()))\n",
        "        if not filename.endswith('.txt'):\n",
        "            print(\"âŒ ä»…æ”¯æŒ.txtæ ¼å¼çš„æ–‡æœ¬æ–‡ä»¶\")\n",
        "            return handle_input()\n",
        "\n",
        "        # è§£ææ–‡æœ¬\n",
        "        try:\n",
        "            transcript_text = uploaded[filename].decode('utf-8')\n",
        "            segments = []\n",
        "            time_pattern = re.compile(r'\\[(\\d+:\\d+:\\d+)\\]')  # åŒ¹é…[HH:MM:SS]\n",
        "\n",
        "            for line in transcript_text.split('\\n'):\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                match = time_pattern.search(line)\n",
        "                if match:\n",
        "                    time_str = match.group(1)\n",
        "                    text = time_pattern.sub('', line).strip()\n",
        "                    # è½¬æ¢æ—¶é—´ä¸ºç§’\n",
        "                    h, m, s = map(int, time_str.split(':'))\n",
        "                    start_time = h * 3600 + m * 60 + s\n",
        "                    segments.append({\n",
        "                        \"text\": text,\n",
        "                        \"start\": start_time,\n",
        "                        \"end\": start_time + 30  # ä¼°ç®—ç»“æŸæ—¶é—´\n",
        "                    })\n",
        "\n",
        "            if not segments:\n",
        "                raise ValueError(\"æœªæ£€æµ‹åˆ°æœ‰æ•ˆæ—¶é—´æˆ³ï¼Œè¯·æ£€æŸ¥æ–‡æœ¬æ ¼å¼\")\n",
        "\n",
        "            # æ£€æµ‹è¯­è¨€\n",
        "            language = detect(transcript_text[:500]) if transcript_text else 'en'\n",
        "            print(f\"âœ… å·²è§£æè½¬å½•æ–‡æœ¬ï¼ˆ{len(segments)}ä¸ªç‰‡æ®µï¼Œæ£€æµ‹è¯­è¨€: {language}ï¼‰\")\n",
        "            return transcript_text, segments, language\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ æ–‡æœ¬è§£æå¤±è´¥: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    else:\n",
        "        print(\"âš ï¸ æ— æ•ˆé€‰æ‹©ï¼Œé»˜è®¤ä½¿ç”¨éŸ³é¢‘è¾“å…¥\")\n",
        "        return handle_input()\n",
        "\n",
        "# ======================\n",
        "# ä¸»å‡½æ•°\n",
        "# ======================\n",
        "def main():\n",
        "    \"\"\"ä¸»å‡½æ•°ï¼šåè°ƒä¼šè®®å¤„ç†æµç¨‹\"\"\"\n",
        "    print(\"=== ä¼šè®®åˆ†æå·¥å…· ===\")\n",
        "    logger.log_step(\"ä¼šè®®å¤„ç†æµç¨‹\", \"started\")\n",
        "\n",
        "    try:\n",
        "        # éªŒè¯GPUç¯å¢ƒ\n",
        "        if not torch.cuda.is_available():\n",
        "            raise RuntimeError(\"è¯·åˆ‡æ¢åˆ°GPUç¯å¢ƒï¼ˆRuntime > Change runtime type > é€‰æ‹©GPUï¼‰\")\n",
        "        print(f\"âœ… æ£€æµ‹åˆ°GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "        # å¤„ç†è¾“å…¥\n",
        "        transcript, segments, language = handle_input()\n",
        "        if not transcript or not segments:\n",
        "            raise RuntimeError(\"æœªè·å–åˆ°æœ‰æ•ˆä¼šè®®å†…å®¹\")\n",
        "\n",
        "        # åˆ†æä¼šè®®\n",
        "        meeting_data = analyze_meeting(segments, language)\n",
        "        if \"error\" in meeting_data:\n",
        "            raise RuntimeError(meeting_data[\"error\"])\n",
        "\n",
        "        # ç”ŸæˆNotionæŠ¥å‘Š\n",
        "        report_url = create_notion_report(meeting_data, segments)\n",
        "        if not report_url:\n",
        "            raise RuntimeError(\"æ— æ³•ç”ŸæˆNotionæŠ¥å‘Š\")\n",
        "\n",
        "        # ä¿å­˜æ—¥å¿—å¹¶è¾“å‡ºç»“æœ\n",
        "        log_file = logger.save_logs()\n",
        "        print(f\"\\nğŸ‰ ä¼šè®®å¤„ç†å®Œæˆï¼\")\n",
        "        print(f\"ğŸ“„ ä¼šè®®æŠ¥å‘Š: {report_url}\")\n",
        "        print(f\"ğŸ“‹ å¤„ç†æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}\")\n",
        "\n",
        "        # æ˜¾ç¤ºæŠ¥å‘Šé“¾æ¥\n",
        "        from IPython.display import HTML\n",
        "        display(HTML(f'<a href=\"{report_url}\" target=\"_blank\">ç‚¹å‡»æŸ¥çœ‹Notionä¼šè®®æŠ¥å‘Š</a>'))\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.save_logs(\"meeting_error_logs.json\")\n",
        "        print(f\"\\nâŒ å¤„ç†å¤±è´¥: {str(e)}\")\n",
        "        print(\"é”™è¯¯è¯¦æƒ…å·²ä¿å­˜åˆ° meeting_error_logs.json\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "NNnUaMEXajrA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5225dd308a5d4cb48bc302acb181a51a",
            "6d21b6d3ac7c4581aa961c2f137f69e5",
            "05510ae00af1484688597ea503caabbb",
            "80051aec09bc46bebf1d5ded88dd4c0b",
            "09c17810f3804b26b6081a02656a1b18",
            "843c05f5495247b381100505b7c705c6",
            "802582a9652746388242a5268447c6c0",
            "cdc8b29074ed48e1911f4e00a2a4d504",
            "108d76ae6f0849329b4d676c664bc9ce",
            "cbb1035b5d844018a8a3a4667667297e",
            "70c9bc2c823c41ed9a20587c08392164",
            "4d367cbf85394d18b92bb1eb2a084df4",
            "44b846a51b424fd596ffb878c653c4cf",
            "8751f7b6210f415abe0dc830d008cf7d",
            "47f335afd5fe494387deffc67087a5e4",
            "a61edf70480d441d8d2e942672c864eb",
            "9242974f1a45442c808fa0705f352966",
            "c27f195f3b3b464faade50443e552535",
            "534d82a48a1e461ab585882259d05c68",
            "2259349b9ea349788c3970bbe2c8ec83",
            "312bb335266d4ed69f8f7252305cd0fe",
            "56c3c6fc295849079148c1a12c6e8044",
            "798068dc88924125ab22f10886f23128",
            "500cab9c8d54477b8d36e2db22a54977",
            "91b0bed8d5fe4bed99769996f50e3cd1",
            "e8fb5ed8a63d4ed19b984a4d2ad5b89b",
            "a22957686d6e477fbe661dc8ec093028",
            "2d42717cecb9449796c0506f7424ffc3",
            "c049b95701a84e1a9ba5a8724e055c98",
            "1efb88bd6b7b42d88a065122dd707a20",
            "95bfe467e7464e8aae9fd99facc8e8a6",
            "5c9d245e038440079e80a6d5ddef382b",
            "dd2e117d0235474abf0627de4a9f6d02",
            "0b76a8e7784a4af982866b0f86002d18",
            "1aa693eef9df43309264df780dca2ac2",
            "959b21bc5db542ff9d898f88ee0c6693",
            "3b1bfa6c69604fbf84f326cb08464bed",
            "40fa370d582544f4b9127b03f9c488cd",
            "4cd96c6f61a54489b1e4457c3c6a6014",
            "5943bdf0509c472e8e5bb06f907eefb0",
            "1da30c836bdc460cab8d43cf1cd79f28",
            "90d3160710484c0dab01c261f5618cf9",
            "0c1bcff65e354aa9be14f2e0427da6e3",
            "6337cdf81d1e4ed68b4991add998577e"
          ]
        },
        "collapsed": true,
        "outputId": "e66734c1-3891-4af3-be77-4f92077acad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping whisper as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: faster-whisper==0.10.0 in /usr/local/lib/python3.11/dist-packages (0.10.0)\n",
            "Requirement already satisfied: av==11.* in /usr/local/lib/python3.11/dist-packages (from faster-whisper==0.10.0) (11.0.0)\n",
            "Requirement already satisfied: ctranslate2<5,>=4.0 in /usr/local/lib/python3.11/dist-packages (from faster-whisper==0.10.0) (4.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.11/dist-packages (from faster-whisper==0.10.0) (0.33.4)\n",
            "Requirement already satisfied: tokenizers<0.16,>=0.13 in /usr/local/lib/python3.11/dist-packages (from faster-whisper==0.10.0) (0.15.2)\n",
            "Requirement already satisfied: onnxruntime<2,>=1.14 in /usr/local/lib/python3.11/dist-packages (from faster-whisper==0.10.0) (1.22.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from ctranslate2<5,>=4.0->faster-whisper==0.10.0) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ctranslate2<5,>=4.0->faster-whisper==0.10.0) (1.26.4)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.11/dist-packages (from ctranslate2<5,>=4.0->faster-whisper==0.10.0) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper==0.10.0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper==0.10.0) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper==0.10.0) (23.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper==0.10.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper==0.10.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper==0.10.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper==0.10.0) (1.1.5)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper==0.10.0) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper==0.10.0) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper==0.10.0) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper==0.10.0) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper==0.10.0) (10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper==0.10.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper==0.10.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper==0.10.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper==0.10.0) (2025.7.14)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper==0.10.0) (1.3.0)\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-pbi2civ5\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-pbi2civ5\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (1.26.4)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20250625) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: notion-client in /usr/local/lib/python3.11/dist-packages (2.4.0)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: langchain==0.1.13 in /usr/local/lib/python3.11/dist-packages (0.1.13)\n",
            "Requirement already satisfied: langchain-openai==0.0.8 in /usr/local/lib/python3.11/dist-packages (0.0.8)\n",
            "Requirement already satisfied: pydantic==2.5.2 in /usr/local/lib/python3.11/dist-packages (2.5.2)\n",
            "Requirement already satisfied: httpx==0.27.0 in /usr/local/lib/python3.11/dist-packages (0.27.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.13) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.13) (2.0.41)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.13) (3.11.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.13) (0.6.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.13) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.29 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.13) (0.0.38)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.33 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.13) (0.1.53)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.13) (0.0.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.13) (0.1.147)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.13) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.13) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.13) (8.5.0)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.0.8) (1.96.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.0.8) (0.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic==2.5.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in /usr/local/lib/python3.11/dist-packages (from pydantic==2.5.2) (2.14.5)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.11/dist-packages (from pydantic==2.5.2) (4.14.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.0) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.0) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.0) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.0) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.0) (1.3.1)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx==0.27.0) (0.16.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.13) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.13) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.13) (3.0.0)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.2.0,>=0.1.33->langchain==0.1.13) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.13) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.13) (1.0.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.8) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.8) (0.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.1.13) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.1.13) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.13) (3.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.5.2->langchain-openai==0.0.8) (2024.11.6)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.13) (1.1.0)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "36 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n",
            "âœ… æ‰€æœ‰å‡­è¯å·²é…ç½®å®Œæˆï¼Œå‡†å¤‡å¤„ç†ä¼šè®®å†…å®¹\n",
            "=== ä¼šè®®åˆ†æå·¥å…· ===\n",
            "âœ… æ£€æµ‹åˆ°GPU: Tesla T4\n",
            "\n",
            "=== è¯·é€‰æ‹©è¾“å…¥æ–¹å¼ ===\n",
            "1: ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ (.mp3/.wav/.m4a/.opus)\n",
            "2: ä¸Šä¼ å¸¦æ—¶é—´æˆ³çš„è½¬å½•æ–‡æœ¬ (.txt)\n",
            "è¯·é€‰æ‹© (1/2): 1\n",
            "\n",
            "è¯·ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ï¼ˆæ”¯æŒ2-3å°æ—¶ä¼šè®®å½•éŸ³ï¼‰:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-10296944-e10b-4c47-a713-8f88e7da665b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-10296944-e10b-4c47-a713-8f88e7da665b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test.mp3 to test.mp3\n",
            "âš ï¸ æ£€æµ‹åˆ°éŸ³é¢‘æ—¶é•¿è¾ƒçŸ­ï¼ˆ52.6åˆ†é’Ÿï¼‰\n",
            "æ˜¯å¦ç»§ç»­ä½¿ç”¨é•¿ä¼šè®®æ¨¡å¼å¤„ç†? (y/n): y\n",
            "ğŸµ éŸ³é¢‘ä¿¡æ¯: test.mp3ï¼ˆ52.6åˆ†é’Ÿï¼‰\n",
            "\n",
            "âš¡ è¯·é€‰æ‹©è½¬å½•æ¨¡å‹:\n",
            "1: base.en - å¿«é€Ÿæ¨¡å¼ï¼ˆé€‚åˆæ¸…æ™°è¯­éŸ³ï¼‰\n",
            "2: small.en - å¹³è¡¡æ¨¡å¼ï¼ˆæ¨èï¼Œé€Ÿåº¦ä¸ç²¾åº¦å…¼é¡¾ï¼‰\n",
            "3: medium.en - é«˜ç²¾åº¦æ¨¡å¼ï¼ˆé€‚åˆå¤æ‚ä¼šè®®ï¼‰\n",
            "è¯·é€‰æ‹© (1-3ï¼Œé»˜è®¤2): 2\n",
            "å°†ä½¿ç”¨ small.en æ¨¡å‹è¿›è¡Œè½¬å½•\n",
            "ğŸ“Š éŸ³é¢‘å°†åˆ†å‰²ä¸º 4 å—ï¼ˆæ¯å—15åˆ†é’Ÿï¼Œé‡å 30ç§’ï¼‰\n",
            "ğŸš€ å¼€å§‹è½¬å½• 4 ä¸ªåˆ†å—...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rè½¬å½•è¿›åº¦:   0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ åŠ è½½ small.en æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œéœ€ä¸‹è½½çº¦1.5GBï¼‰...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5225dd308a5d4cb48bc302acb181a51a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d367cbf85394d18b92bb1eb2a084df4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocabulary.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "798068dc88924125ab22f10886f23128"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.bin:   0%|          | 0.00/484M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b76a8e7784a4af982866b0f86002d18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… æ¨¡å‹åŠ è½½æˆåŠŸ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "è½¬å½•è¿›åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:54<00:00, 28.72s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… è½¬å½•å®Œæˆï¼ˆ847ä¸ªç‰‡æ®µï¼Œæ£€æµ‹è¯­è¨€: enï¼‰\n",
            "\n",
            "å¼€å§‹åˆ†æä¼šè®®å†…å®¹...\n",
            "ğŸ“ å°†ä¼šè®®å†…å®¹åˆ†ä¸º 2 ä¸ªåˆ†æå—\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "åˆ†æè¿›åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ä¼šè®®åˆ†æå®Œæˆï¼ˆ7ä¸ªè¯é¢˜ï¼Œ0ä¸ªè¡ŒåŠ¨é¡¹ï¼‰\n",
            "âœ… æˆåŠŸè®¿é—®çˆ¶é¡µé¢: Parent Page\n",
            "âœ… NotionæŠ¥å‘Šå·²ç”Ÿæˆï¼ˆæ€»å—æ•°: 74ï¼‰\n",
            "\n",
            "ğŸ‰ ä¼šè®®å¤„ç†å®Œæˆï¼\n",
            "ğŸ“„ ä¼šè®®æŠ¥å‘Š: https://www.notion.so/Interview-with-Lauren-Graham-and-Game-Session-with-Maya-2355fee18e3781208495cbbb48558454\n",
            "ğŸ“‹ å¤„ç†æ—¥å¿—å·²ä¿å­˜åˆ°: meeting_logs.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<a href=\"https://www.notion.so/Interview-with-Lauren-Graham-and-Game-Session-with-Maya-2355fee18e3781208495cbbb48558454\" target=\"_blank\">ç‚¹å‡»æŸ¥çœ‹Notionä¼šè®®æŠ¥å‘Š</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å®‰è£…å¿…è¦ä¾èµ–ï¼ˆæŒ‡å®šå…¼å®¹ç‰ˆæœ¬ï¼‰\n",
        "!pip uninstall -y whisper\n",
        "!pip install faster-whisper==0.10.0  # é”å®šç‰ˆæœ¬ä»¥é¿å…APIå˜æ›´\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install tqdm python-docx notion-client langdetect langchain==0.1.13 langchain-openai==0.0.8 pydantic==2.5.2 httpx==0.27.0\n",
        "!sudo apt update && sudo apt install ffmpeg -y\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import torch\n",
        "import subprocess\n",
        "import datetime\n",
        "import tempfile\n",
        "from tqdm import tqdm\n",
        "from pydantic import BaseModel, Field\n",
        "import httpx\n",
        "import concurrent.futures\n",
        "from functools import partial\n",
        "\n",
        "# å¯¼å…¥ç¬¬ä¸‰æ–¹åº“\n",
        "from google.colab import files, userdata\n",
        "from notion_client import Client, errors\n",
        "from langdetect import detect, LangDetectException\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# æ¸…é™¤ä»£ç†ç¯å¢ƒå˜é‡ï¼ˆé¿å…ç½‘ç»œè¿æ¥é—®é¢˜ï¼‰\n",
        "for var in ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy']:\n",
        "    if var in os.environ:\n",
        "        del os.environ[var]\n",
        "\n",
        "# åˆå§‹åŒ–Notionå®¢æˆ·ç«¯ï¼ˆä¿®å¤httpxä»£ç†å‚æ•°é—®é¢˜ï¼‰\n",
        "http_client = httpx.Client()\n",
        "http_client.proxies = None  # ç¦ç”¨ä»£ç†\n",
        "\n",
        "notion = Client(\n",
        "    auth=userdata.get('NOTION_TOKEN'),\n",
        "    client=http_client\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# é…ç½®å‚æ•°ä¸åˆå§‹åŒ–\n",
        "# ======================\n",
        "try:\n",
        "    # ä»ç¯å¢ƒå˜é‡è·å–å¯†é’¥\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    NOTION_TOKEN = userdata.get('NOTION_TOKEN')\n",
        "    NOTION_PAGE_ID = userdata.get('NOTION_PAGE_ID')\n",
        "    NOTION_DB_ID = userdata.get('NOTION_DB_ID')  # æ•°æ®åº“ID\n",
        "\n",
        "    # é•¿éŸ³é¢‘å¤„ç†å‚æ•°\n",
        "    CHUNK_DURATION = 900  # æ¯å—15åˆ†é’Ÿï¼ˆç§’ï¼‰\n",
        "    OVERLAP_DURATION = 30  # å—é—´é‡å 30ç§’\n",
        "    MAX_CONCURRENT_CHUNKS = 1  # å•çº¿ç¨‹å¤„ç†ï¼Œé¿å…æ–‡ä»¶ç«äº‰\n",
        "    # Notionå—æ•°é‡é™åˆ¶ç›¸å…³å‚æ•°\n",
        "    MAX_TRANSCRIPT_SEGMENTS = 50  # æœ€å¤šæ˜¾ç¤º50æ¡è½¬å½•æ–‡æœ¬\n",
        "    NOTION_RICH_TEXT_LIMIT = 1950  # Notion rich_textå­—æ®µæœ€å¤§é•¿åº¦ï¼ˆç•™50å­—ç¬¦ä½™é‡ï¼‰\n",
        "\n",
        "    # éªŒè¯å¿…è¦å¯†é’¥\n",
        "    missing_creds = []\n",
        "    if not OPENAI_API_KEY:\n",
        "        missing_creds.append(\"OPENAI_API_KEY\")\n",
        "    if not NOTION_TOKEN:\n",
        "        missing_creds.append(\"NOTION_TOKEN\")\n",
        "    if not NOTION_PAGE_ID:\n",
        "        missing_creds.append(\"NOTION_PAGE_ID\")\n",
        "    if not NOTION_DB_ID:\n",
        "        missing_creds.append(\"NOTION_DB_ID\")\n",
        "\n",
        "    if missing_creds:\n",
        "        raise ValueError(f\"ç¼ºå°‘å¿…è¦å‡­è¯: {', '.join(missing_creds)}\")\n",
        "\n",
        "    print(\"âœ… æ‰€æœ‰å‡­è¯å·²é…ç½®å®Œæˆï¼Œå‡†å¤‡å¤„ç†ä¼šè®®å†…å®¹\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ åˆå§‹åŒ–å¤±è´¥: {str(e)}\")\n",
        "    print(\"\\nè®¾ç½®æŒ‡å—:\")\n",
        "    print(\"1. ç‚¹å‡»å·¦ä¾§è¾¹æ çš„é’¥åŒ™å›¾æ ‡ï¼ˆColab Secretsï¼‰\")\n",
        "    print(\"2. æ·»åŠ ä»¥ä¸‹å¯†é’¥:\")\n",
        "    print(\"   - OPENAI_API_KEY: ä½ çš„OpenAI APIå¯†é’¥\")\n",
        "    print(\"   - NOTION_TOKEN: Notioné›†æˆä»¤ç‰Œ\")\n",
        "    print(\"   - NOTION_PAGE_ID: ä¼šè®®é›†æˆé¡µID\")\n",
        "    print(\"   - NOTION_DB_ID: ç›®æ ‡æ•°æ®åº“ID\")\n",
        "    raise\n",
        "\n",
        "# ======================\n",
        "# æ—¥å¿—è®°å½•ç³»ç»Ÿ\n",
        "# ======================\n",
        "class MeetingLogger:\n",
        "    def __init__(self):\n",
        "        self.logs = {\n",
        "            \"start_time\": datetime.datetime.now().isoformat(),\n",
        "            \"steps\": [],\n",
        "            \"chunk_status\": {}\n",
        "        }\n",
        "\n",
        "    def log_step(self, step_name, status, details=None, error=None):\n",
        "        entry = {\n",
        "            \"step\": step_name,\n",
        "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "            \"status\": status\n",
        "        }\n",
        "        if details:\n",
        "            entry[\"details\"] = details\n",
        "        if error:\n",
        "            entry[\"error\"] = str(error)\n",
        "        self.logs[\"steps\"].append(entry)\n",
        "\n",
        "    def log_chunk(self, chunk_id, status, error=None):\n",
        "        self.logs[\"chunk_status\"][chunk_id] = {\n",
        "            \"status\": status,\n",
        "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "            \"error\": str(error) if error else None\n",
        "        }\n",
        "\n",
        "    def get_completed_chunks(self):\n",
        "        return [k for k, v in self.logs[\"chunk_status\"].items() if v[\"status\"] == \"success\"]\n",
        "\n",
        "    def get_failed_chunks(self):\n",
        "        return [k for k, v in self.logs[\"chunk_status\"].items() if v[\"status\"] == \"failed\"]\n",
        "\n",
        "    def save_logs(self, filename=\"meeting_logs.json\"):\n",
        "        with open(filename, \"w\") as f:\n",
        "            json.dump(self.logs, f, indent=2)\n",
        "        return filename\n",
        "\n",
        "logger = MeetingLogger()\n",
        "\n",
        "# ======================\n",
        "# éŸ³é¢‘å¤„ç†å·¥å…·\n",
        "# ======================\n",
        "def get_audio_duration(audio_path):\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"ffmpeg\", \"-i\", audio_path],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            text=True\n",
        "        )\n",
        "        output = result.stdout\n",
        "\n",
        "        duration_match = re.search(r\"Duration: (\\d+:\\d+:\\d+\\.\\d+)\", output)\n",
        "        if not duration_match:\n",
        "            return 0.0\n",
        "\n",
        "        duration_str = duration_match.group(1)\n",
        "        h, m, s = duration_str.split(':')\n",
        "        return float(h) * 3600 + float(m) * 60 + float(s)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"è·å–éŸ³é¢‘æ—¶é•¿\", \"warning\", error=str(e))\n",
        "        return 0.0\n",
        "\n",
        "def split_audio_into_chunks(audio_path):\n",
        "    logger.log_step(\"éŸ³é¢‘åˆ†å—\", \"started\")\n",
        "\n",
        "    try:\n",
        "        total_duration = get_audio_duration(audio_path)\n",
        "        if total_duration <= 0:\n",
        "            raise ValueError(\"æ— æ³•è·å–æœ‰æ•ˆéŸ³é¢‘æ—¶é•¿ï¼Œå¯èƒ½æ–‡ä»¶æŸå\")\n",
        "\n",
        "        num_chunks = max(1, int((total_duration + CHUNK_DURATION - OVERLAP_DURATION) //\n",
        "                              (CHUNK_DURATION - OVERLAP_DURATION)))\n",
        "        logger.log_step(\"è®¡ç®—åˆ†å—æ•°é‡\", \"success\", {\"æ€»æ—¶é•¿(åˆ†é’Ÿ)\": f\"{total_duration/60:.1f}\", \"åˆ†å—æ•°\": num_chunks})\n",
        "        print(f\"ğŸ“Š éŸ³é¢‘å°†åˆ†å‰²ä¸º {num_chunks} å—ï¼ˆæ¯å—15åˆ†é’Ÿï¼Œé‡å 30ç§’ï¼‰\")\n",
        "\n",
        "        chunk_dir = tempfile.mkdtemp()\n",
        "        chunks = []\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_time = i * (CHUNK_DURATION - OVERLAP_DURATION)\n",
        "            end_time = min(start_time + CHUNK_DURATION, total_duration)\n",
        "\n",
        "            start_str = str(datetime.timedelta(seconds=start_time))\n",
        "            duration_str = str(datetime.timedelta(seconds=end_time - start_time))\n",
        "\n",
        "            chunk_path = f\"{chunk_dir}/chunk_{i:03d}.wav\"\n",
        "\n",
        "            subprocess.run(\n",
        "                [\n",
        "                    \"ffmpeg\", \"-y\",\n",
        "                    \"-i\", audio_path,\n",
        "                    \"-ss\", start_str,\n",
        "                    \"-t\", duration_str,\n",
        "                    \"-ar\", \"16000\",\n",
        "                    \"-ac\", \"1\",\n",
        "                    \"-acodec\", \"pcm_s16le\",\n",
        "                    chunk_path\n",
        "                ],\n",
        "                check=True,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE\n",
        "            )\n",
        "\n",
        "            if not os.path.exists(chunk_path) or os.path.getsize(chunk_path) < 1024:\n",
        "                raise RuntimeError(f\"åˆ†å— {i} ç”Ÿæˆå¤±è´¥ï¼Œæ–‡ä»¶å¤§å°å¼‚å¸¸\")\n",
        "\n",
        "            chunks.append({\n",
        "                \"id\": i,\n",
        "                \"path\": chunk_path,\n",
        "                \"start_time\": start_time,\n",
        "                \"end_time\": end_time,\n",
        "                \"dir\": chunk_dir\n",
        "            })\n",
        "\n",
        "        logger.log_step(\"éŸ³é¢‘åˆ†å—\", \"success\")\n",
        "        return chunks\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"éŸ³é¢‘åˆ†å—\", \"failed\", error=str(e))\n",
        "        raise RuntimeError(f\"éŸ³é¢‘åˆ†å—å¤±è´¥: {str(e)}\")\n",
        "\n",
        "# ======================\n",
        "# è¯­éŸ³è½¬å½•\n",
        "# ======================\n",
        "global_model = None\n",
        "\n",
        "def transcribe_chunk(chunk):\n",
        "    chunk_id = chunk[\"id\"]\n",
        "    chunk_path = chunk[\"path\"]\n",
        "    start_time = chunk[\"start_time\"]\n",
        "\n",
        "    try:\n",
        "        from faster_whisper import WhisperModel\n",
        "        global global_model\n",
        "\n",
        "        if global_model is None:\n",
        "            if not torch.cuda.is_available():\n",
        "                raise RuntimeError(\"è¯·åˆ‡æ¢åˆ°GPUç¯å¢ƒï¼ˆRuntime > Change runtime typeï¼‰\")\n",
        "\n",
        "            print(f\"ğŸ”§ åŠ è½½ {chunk['model_size']} æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œéœ€ä¸‹è½½çº¦1.5GBï¼‰...\")\n",
        "            global_model = WhisperModel(\n",
        "                chunk[\"model_size\"],\n",
        "                device=\"cuda\",\n",
        "                compute_type=\"float16\",\n",
        "                download_root=\"/content/models\"\n",
        "            )\n",
        "            print(f\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ\")\n",
        "\n",
        "        if not os.path.exists(chunk_path):\n",
        "            raise FileNotFoundError(f\"åˆ†å—æ–‡ä»¶ä¸å­˜åœ¨: {chunk_path}\")\n",
        "\n",
        "        segments, info = global_model.transcribe(\n",
        "            chunk_path,\n",
        "            beam_size=2,\n",
        "            vad_filter=True,\n",
        "            vad_parameters=dict(min_silence_duration_ms=300),\n",
        "            language=None  # è‡ªåŠ¨æ£€æµ‹è¯­è¨€\n",
        "        )\n",
        "\n",
        "        timestamped_segments = []\n",
        "        for seg in segments:\n",
        "            timestamped_segments.append({\n",
        "                \"text\": seg.text.strip(),\n",
        "                \"start\": seg.start + start_time,\n",
        "                \"end\": seg.end + start_time,\n",
        "                \"language\": info.language  # è®°å½•å½“å‰å—çš„è¯­è¨€\n",
        "            })\n",
        "\n",
        "        logger.log_chunk(chunk_id, \"success\")\n",
        "        return {\n",
        "            \"chunk_id\": chunk_id,\n",
        "            \"segments\": timestamped_segments,\n",
        "            \"language\": info.language\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_chunk(chunk_id, \"failed\", error=str(e))\n",
        "        print(f\"âš ï¸ åˆ†å— {chunk_id} è½¬å½•å¤±è´¥: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def transcribe_long_audio(audio_path, model_size=\"small.en\"):\n",
        "    logger.log_step(\"éŸ³é¢‘è½¬å½•\", \"started\", {\"æ¨¡å‹\": model_size})\n",
        "\n",
        "    try:\n",
        "        global global_model\n",
        "        global_model = None\n",
        "\n",
        "        chunks = split_audio_into_chunks(audio_path)\n",
        "        num_chunks = len(chunks)\n",
        "\n",
        "        for chunk in chunks:\n",
        "            chunk[\"model_size\"] = model_size\n",
        "\n",
        "        print(f\"ğŸš€ å¼€å§‹è½¬å½• {num_chunks} ä¸ªåˆ†å—...\")\n",
        "        results = []\n",
        "        for chunk in tqdm(chunks, desc=\"è½¬å½•è¿›åº¦\"):\n",
        "            result = transcribe_chunk(chunk)\n",
        "            if result:\n",
        "                results.append(result)\n",
        "\n",
        "        failed_ids = logger.get_failed_chunks()\n",
        "        if failed_ids:\n",
        "            print(f\"ğŸ”„ é‡è¯• {len(failed_ids)} ä¸ªå¤±è´¥åˆ†å—...\")\n",
        "            for chunk in chunks:\n",
        "                if chunk[\"id\"] in failed_ids:\n",
        "                    result = transcribe_chunk(chunk)\n",
        "                    if result:\n",
        "                        results.append(result)\n",
        "\n",
        "        results.sort(key=lambda x: x[\"chunk_id\"])\n",
        "        all_segments = []\n",
        "        for res in results:\n",
        "            all_segments.extend(res[\"segments\"])\n",
        "\n",
        "        unique_dirs = list({chunk[\"dir\"] for chunk in chunks})\n",
        "        for dir_path in unique_dirs:\n",
        "            if os.path.exists(dir_path):\n",
        "                for f in os.listdir(dir_path):\n",
        "                    os.unlink(os.path.join(dir_path, f))\n",
        "                os.rmdir(dir_path)\n",
        "\n",
        "        # æå–æ‰€æœ‰å‡ºç°çš„è¯­è¨€ï¼ˆå»é‡ï¼‰\n",
        "        all_languages = list({seg.get(\"language\", \"unknown\") for seg in all_segments})\n",
        "        primary_language = results[0][\"language\"] if results else \"en\"\n",
        "\n",
        "        # è®¡ç®—ä¼šè®®æ€»æ—¶é•¿ï¼ˆç§’ï¼‰\n",
        "        total_seconds = all_segments[-1][\"end\"] - all_segments[0][\"start\"] if all_segments else 0\n",
        "\n",
        "        logger.log_step(\"éŸ³é¢‘è½¬å½•\", \"success\", {\"æ€»ç‰‡æ®µæ•°\": len(all_segments), \"æ‰€æœ‰è¯­è¨€\": all_languages})\n",
        "        print(f\"âœ… è½¬å½•å®Œæˆï¼ˆ{len(all_segments)}ä¸ªç‰‡æ®µï¼Œä¸»è¦è¯­è¨€: {primary_language}ï¼Œæ‰€æœ‰è¯­è¨€: {all_languages}ï¼‰\")\n",
        "        return \" \".join([s[\"text\"] for s in all_segments]), all_segments, primary_language, all_languages, total_seconds\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"éŸ³é¢‘è½¬å½•\", \"failed\", error=str(e))\n",
        "        raise RuntimeError(f\"è½¬å½•è¿‡ç¨‹å¤±è´¥: {str(e)}\")\n",
        "\n",
        "# ======================\n",
        "# ä¼šè®®å†…å®¹åˆ†æ\n",
        "# ======================\n",
        "class ChunkAnalysis(BaseModel):\n",
        "    summary: str = Field(description=\"è¯¥ç‰‡æ®µçš„æ€»ç»“ï¼ˆ100-200å­—ï¼‰\")\n",
        "    key_points: list[str] = Field(description=\"è¯¥ç‰‡æ®µçš„å…³é”®ç‚¹é”®åˆ—è¡¨ï¼ˆå®¢è§‚äº‹å®ï¼‰\")\n",
        "    action_items: list[dict] = Field(description=\"è¡ŒåŠ¨é¡¹åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«taskã€assigneeã€due_date\")\n",
        "    topics: list[str] = Field(description=\"è®¨è®ºçš„è¯é¢˜åˆ—è¡¨\")\n",
        "    decisions: list[str] = Field(description=\"è¯¥ç‰‡æ®µä¸­è¾¾æˆçš„å…·ä½“å†³å®šï¼ˆæ˜ç¡®çš„ç»“è®ºï¼‰\")\n",
        "    concerns: list[str] = Field(description=\"è¯¥ç‰‡æ®µä¸­æå‡ºçš„æ‹…å¿§ã€é—®é¢˜æˆ–é£é™©\")\n",
        "    platform: str = Field(description=\"ä¼šè®®å‘ç”Ÿçš„å¹³å°æˆ–åœºæ‰€ï¼ˆå¦‚Zoomã€ä¼šè®®å®¤Aç­‰ï¼‰\")  # æ–°å¢å­—æ®µ\n",
        "\n",
        "class FullMeetingAnalysis(BaseModel):\n",
        "    meeting_title: str = Field(description=\"ä¼šè®®æ ‡é¢˜\")\n",
        "    participants: list[str] = Field(description=\"å‚ä¸è€…åå•\")\n",
        "    summary: str = Field(description=\"3-5æ®µå®Œæ•´ä¼šè®®æ€»ç»“\")\n",
        "    key_points: dict = Field(description=\"æŒ‰è¯é¢˜åˆ†ç»„ç»„çš„å…¨å±€å…³é”®ç‚¹ï¼ˆå®¢è§‚äº‹å®ï¼‰\")\n",
        "    action_items: list[dict] = Field(description=\"æ±‡æ€»çš„è¡ŒåŠ¨é¡¹\")\n",
        "    meeting_type: str = Field(description=\"ä¼šè®®ç±»å‹ï¼ˆå¦‚å‘¨ä¼šã€é¡¹ç›®è¯„å®¡ä¼šã€å¤´è„‘é£æš´ç­‰ï¼‰\")\n",
        "    topics_flow: list[str] = Field(description=\"ä¼šè®®è¯é¢˜æµè½¬é¡ºåº\")\n",
        "    decisions: list[str] = Field(description=\"ä¼šè®®ä¸­è¾¾æˆçš„æ‰€æœ‰å†³å®šï¼ˆæ˜ç¡®ç»“è®ºï¼Œå¦‚â€œåŒæ„é¡¹ç›®å»¶æœŸâ€ï¼‰\")\n",
        "    concerns: list[str] = Field(description=\"ä¼šè®®ä¸­æå‡ºçš„æ‰€æœ‰æ‹…å¿§ã€é—®é¢˜æˆ–é£é™©ï¼ˆå¦‚â€œèµ„æºä¸è¶³â€ï¼‰\")\n",
        "    platform: str = Field(description=\"ä¼šè®®å‘ç”Ÿçš„å¹³å°æˆ–åœºæ‰€ï¼ˆå¦‚Zoomã€Teamsã€ä¼šè®®å®¤Bç­‰ï¼‰\")  # æ–°å¢å­—æ®µ\n",
        "\n",
        "def get_chunk_analysis_chain(language):\n",
        "    parser = JsonOutputParser(pydantic_object=ChunkAnalysis)\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"åˆ†æä»¥ä¸‹ä¼šè®®ç‰‡æ®µï¼Œæå–å…³é”®ä¿¡æ¯ï¼ˆç”¨{language}ï¼‰ï¼š\n",
        "{format_instructions}\n",
        "æ³¨æ„ï¼š\n",
        "- key_pointsï¼šå®¢è§‚äº‹å®ï¼ˆå¦‚â€œé¡¹ç›®è¿›åº¦è½å20%â€ï¼‰\n",
        "- decisionsï¼šæ˜ç¡®è¾¾æˆçš„ç»“è®ºï¼ˆå¦‚â€œå†³å®šå¢åŠ 2åå¼€å‘äººå‘˜â€ï¼‰\n",
        "- concernsï¼šæå‡ºçš„æ‹…å¿§ï¼ˆå¦‚â€œé¢„ç®—å¯èƒ½è¶…æ”¯â€ï¼‰\n",
        "- platformï¼šä¼šè®®è¿›è¡Œçš„å¹³å°æˆ–ç‰©ç†åœºæ‰€ï¼ˆå¦‚Zoomã€å…¬å¸3æ¥¼ä¼šè®®å®¤ç­‰ï¼‰\n",
        "ä¼šè®®ç‰‡æ®µï¼š{transcript}\"\"\",\n",
        "        input_variables=[\"transcript\"],\n",
        "        partial_variables={\n",
        "            \"language\": \"ä¸­æ–‡\" if language.startswith('zh') else \"English\",\n",
        "            \"format_instructions\": parser.get_format_instructions()\n",
        "        }\n",
        "    )\n",
        "    return prompt | ChatOpenAI(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        temperature=0.3,\n",
        "        openai_api_key=OPENAI_API_KEY\n",
        "    ) | parser\n",
        "\n",
        "def get_full_analysis_chain(language):\n",
        "    parser = JsonOutputParser(pydantic_object=FullMeetingAnalysis)\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"åŸºäºä»¥ä¸‹å„ç‰‡æ®µåˆ†æï¼Œç”Ÿæˆå®Œæ•´ä¼šè®®æŠ¥å‘Šï¼ˆç”¨{language}ï¼‰ï¼š\n",
        "{format_instructions}\n",
        "æ³¨æ„ï¼š\n",
        "- key_pointsï¼šä»…åŒ…å«å®¢è§‚äº‹å®ï¼Œä¸åŒ…å«ç»“è®º\n",
        "- decisionsï¼šå¿…é¡»æ˜¯æ˜ç¡®è¾¾æˆçš„ç»“è®ºï¼ˆæœ‰å…·ä½“ç»“æœï¼‰\n",
        "- concernsï¼šå¿…é¡»æ˜¯æå‡ºçš„é—®é¢˜æˆ–é£é™©ï¼ˆæœªè§£å†³çš„æ‹…å¿§ï¼‰\n",
        "- platformï¼šæ˜ç¡®ä¼šè®®å‘ç”Ÿçš„å¹³å°æˆ–åœºæ‰€ï¼ˆå¦‚Zoomã€Teamsã€æ€»éƒ¨ä¼šè®®å®¤ç­‰ï¼‰\n",
        "- meeting_typeï¼šæ˜ç¡®ä¼šè®®ç±»å‹ï¼ˆå¦‚å‘¨ä¾‹ä¼šã€é¡¹ç›®å¯åŠ¨ä¼šã€è¯„å®¡ä¼šç­‰ï¼‰\n",
        "ç‰‡æ®µåˆ†æï¼š{chunk_analyses}\"\"\",\n",
        "        input_variables=[\"chunk_analyses\"],\n",
        "        partial_variables={\n",
        "            \"language\": \"ä¸­æ–‡\" if language.startswith('zh') else \"English\",\n",
        "            \"format_instructions\": parser.get_format_instructions()\n",
        "        }\n",
        "    )\n",
        "    return prompt | ChatOpenAI(\n",
        "        model=\"gpt-4\",\n",
        "        temperature=0.3,\n",
        "        openai_api_key=OPENAI_API_KEY\n",
        "    ) | parser\n",
        "\n",
        "def analyze_meeting(transcript_segments, language='en'):\n",
        "    logger.log_step(\"ä¼šè®®åˆ†æ\", \"started\")\n",
        "    print(\"\\nå¼€å§‹åˆ†æä¼šè®®å†…å®¹...\")\n",
        "\n",
        "    try:\n",
        "        ANALYSIS_CHUNK_DURATION = 2700\n",
        "        analysis_chunks = []\n",
        "        current_chunk = []\n",
        "\n",
        "        for seg in transcript_segments:\n",
        "            if not current_chunk:\n",
        "                current_chunk.append(seg)\n",
        "            else:\n",
        "                if seg[\"end\"] - current_chunk[0][\"start\"] <= ANALYSIS_CHUNK_DURATION:\n",
        "                    current_chunk.append(seg)\n",
        "                else:\n",
        "                    analysis_chunks.append(current_chunk)\n",
        "                    current_chunk = [seg]\n",
        "        if current_chunk:\n",
        "            analysis_chunks.append(current_chunk)\n",
        "\n",
        "        print(f\"ğŸ“ å°†ä¼šè®®å†…å®¹åˆ†ä¸º {len(analysis_chunks)} ä¸ªåˆ†æå—\")\n",
        "\n",
        "        chunk_analyses = []\n",
        "        chunk_chain = get_chunk_analysis_chain(language)\n",
        "\n",
        "        for i, chunk in enumerate(tqdm(analysis_chunks, desc=\"åˆ†æè¿›åº¦\")):\n",
        "            chunk_text = \"\\n\".join([\n",
        "                f\"[{str(datetime.timedelta(seconds=int(seg['start'])))}] {seg['text']}\"\n",
        "                for seg in chunk\n",
        "            ])\n",
        "\n",
        "            try:\n",
        "                analysis = chunk_chain.invoke({\"transcript\": chunk_text[:12000]})\n",
        "                chunk_analyses.append({\n",
        "                    \"chunk_id\": i,\n",
        "                    \"start_time\": chunk[0][\"start\"],\n",
        "                    \"end_time\": chunk[-1][\"end\"],\n",
        "                    \"analysis\": analysis\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ åˆ†æå— {i} å¤±è´¥: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        if not chunk_analyses:\n",
        "            raise RuntimeError(\"æ‰€æœ‰åˆ†æå—å¤„ç†å¤±è´¥ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š\")\n",
        "\n",
        "        full_chain = get_full_analysis_chain(language)\n",
        "        full_analysis = full_chain.invoke({\n",
        "            \"chunk_analyses\": json.dumps([\n",
        "                {\n",
        "                    \"æ—¶é—´æ®µ\": f\"{str(datetime.timedelta(seconds=int(c['start_time'])))} - {str(datetime.timedelta(seconds=int(c['end_time'])))}\",\n",
        "                    \"åˆ†æ\": c[\"analysis\"]\n",
        "                } for c in chunk_analyses\n",
        "            ], ensure_ascii=False)\n",
        "        })\n",
        "\n",
        "        if hasattr(full_analysis, 'dict'):\n",
        "            full_analysis = full_analysis.dict()\n",
        "\n",
        "        full_analysis[\"language\"] = language\n",
        "        full_analysis[\"date\"] = datetime.datetime.now().isoformat()\n",
        "\n",
        "        logger.log_step(\"ä¼šè®®åˆ†æ\", \"success\")\n",
        "        print(f\"âœ… ä¼šè®®åˆ†æå®Œæˆï¼ˆ{len(full_analysis.get('topics_flow', []))}ä¸ªè¯é¢˜ï¼Œ{len(full_analysis.get('action_items', []))}ä¸ªè¡ŒåŠ¨é¡¹ï¼‰\")\n",
        "        return full_analysis\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"ä¼šè®®åˆ†æå¤±è´¥: {str(e)}\"\n",
        "        logger.log_step(\"ä¼šè®®åˆ†æ\", \"failed\", error=error_msg)\n",
        "        print(f\"âŒ {error_msg}\")\n",
        "        return {\"error\": error_msg}\n",
        "\n",
        "# ======================\n",
        "# Notionæ“ä½œ\n",
        "# ======================\n",
        "def create_notion_report(meeting_data, transcript_segments):\n",
        "    logger.log_step(\"åˆ›å»ºNotionæŠ¥å‘Š\", \"started\")\n",
        "\n",
        "    try:\n",
        "        try:\n",
        "            parent_page = notion.pages.retrieve(NOTION_PAGE_ID)\n",
        "            parent_title = parent_page.get('properties', {}).get('title', {}).get('title', [{}])[0].get('plain_text', 'æ— æ ‡é¢˜é¡µé¢')\n",
        "            print(f\"âœ… æˆåŠŸè®¿é—®çˆ¶é¡µé¢: {parent_title}\")\n",
        "        except errors.APIResponseError as e:\n",
        "            raise RuntimeError(f\"Notionçˆ¶é¡µé¢è®¿é—®å¤±è´¥: {str(e)}\")\n",
        "\n",
        "        new_page = notion.pages.create(\n",
        "            parent={\"page_id\": NOTION_PAGE_ID},\n",
        "            properties={\n",
        "                \"title\": {\n",
        "                    \"title\": [{\"text\": {\"content\": meeting_data.get(\"meeting_title\", \"ä¼šè®®æŠ¥å‘Š\")[:200]}}]\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "        page_id = new_page[\"id\"]\n",
        "\n",
        "        # æ„å»ºæŠ¥å‘Šå†…å®¹å—\n",
        "        children_blocks = []\n",
        "\n",
        "        # 1. ä¼šè®®æ¦‚è§ˆ\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"ä¼šè®®æ¦‚è§ˆ\"}}]}\n",
        "        })\n",
        "        overview_text = f\"\"\"\n",
        "        **æ ‡é¢˜**: {meeting_data.get('meeting_title', 'æœªå‘½åä¼šè®®')}\n",
        "        **æ—¥æœŸ**: {meeting_data.get('date', datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'))}\n",
        "        **å‚ä¸è€…**: {', '.join(meeting_data.get('participants', [])) or 'æœªè¯†åˆ«'}\n",
        "        **ç±»å‹**: {meeting_data.get('meeting_type', 'æœªæŒ‡å®š')}\n",
        "        **å¹³å°**: {meeting_data.get('platform', 'æœªæŒ‡å®š')}\n",
        "        **è¯­è¨€**: {', '.join(meeting_data.get('all_languages', [])) or 'æœªçŸ¥'}\n",
        "        \"\"\"\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"paragraph\",\n",
        "            \"paragraph\": {\"rich_text\": [{\"text\": {\"content\": overview_text.strip()}}]}\n",
        "        })\n",
        "\n",
        "        # 2. è¯é¢˜æµè½¬\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"è¯é¢˜æµè½¬é¡ºåº\"}}]}\n",
        "        })\n",
        "        max_topics = 20\n",
        "        for topic in meeting_data.get('topics_flow', [])[:max_topics]:\n",
        "            children_blocks.append({\n",
        "                \"object\": \"block\",\n",
        "                \"type\": \"numbered_list_item\",\n",
        "                \"numbered_list_item\": {\"rich_text\": [{\"text\": {\"content\": topic}}]}\n",
        "            })\n",
        "\n",
        "        # 3. ä¼šè®®æ€»ç»“\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"ä¼šè®®æ€»ç»“\"}}]}\n",
        "        })\n",
        "        max_summary_paras = 5\n",
        "        for para in meeting_data.get('summary', '').split('\\n\\n')[:max_summary_paras]:\n",
        "            if para.strip():\n",
        "                children_blocks.append({\n",
        "                    \"object\": \"block\",\n",
        "                    \"type\": \"paragraph\",\n",
        "                    \"paragraph\": {\"rich_text\": [{\"text\": {\"content\": para.strip()}}]}\n",
        "                })\n",
        "\n",
        "        # 4. å…³é”®è¦ç‚¹\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"å…³é”®è¦ç‚¹\"}}]}\n",
        "        })\n",
        "        key_points = meeting_data.get('key_points', {})\n",
        "        max_key_topics = 10\n",
        "        for topic, points in list(key_points.items())[:max_key_topics]:\n",
        "            children_blocks.append({\n",
        "                \"object\": \"block\",\n",
        "                \"type\": \"heading_3\",\n",
        "                \"heading_3\": {\"rich_text\": [{\"text\": {\"content\": topic}}]}\n",
        "            })\n",
        "            max_points_per_topic = 5\n",
        "            for point in points[:max_points_per_topic]:\n",
        "                children_blocks.append({\n",
        "                    \"object\": \"block\",\n",
        "                    \"type\": \"bulleted_list_item\",\n",
        "                    \"bulleted_list_item\": {\"rich_text\": [{\"text\": {\"content\": point}}]}\n",
        "                })\n",
        "\n",
        "        # 5. è¡ŒåŠ¨é¡¹è¡¨æ ¼\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"è¡ŒåŠ¨é¡¹\"}}]}\n",
        "        })\n",
        "        table_rows = []\n",
        "        max_actions = 20\n",
        "        for idx, item in enumerate(meeting_data.get('action_items', [])[:max_actions]):\n",
        "            table_rows.append([\n",
        "                [{\"text\": {\"content\": str(idx+1)}}],\n",
        "                [{\"text\": {\"content\": item.get('task', '')}}],\n",
        "                [{\"text\": {\"content\": item.get('assignee', 'æœªåˆ†é…')}}],\n",
        "                [{\"text\": {\"content\": item.get('due_date', 'æ— ')}}]\n",
        "            ])\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"table\",\n",
        "            \"table\": {\n",
        "                \"table_width\": 4,\n",
        "                \"has_column_header\": True,\n",
        "                \"children\": [\n",
        "                    {\n",
        "                        \"object\": \"block\",\n",
        "                        \"type\": \"table_row\",\n",
        "                        \"table_row\": {\n",
        "                            \"cells\": [\n",
        "                                [{\"text\": {\"content\": \"åºå·\"}}],\n",
        "                                [{\"text\": {\"content\": \"ä»»åŠ¡\"}}],\n",
        "                                [{\"text\": {\"content\": \"è´Ÿè´£äºº\"}}],\n",
        "                                [{\"text\": {\"content\": \"æˆªæ­¢æ—¥æœŸ\"}}]\n",
        "                            ]\n",
        "                        }\n",
        "                    },\n",
        "                    *[{\n",
        "                        \"object\": \"block\",\n",
        "                        \"type\": \"table_row\",\n",
        "                        \"table_row\": {\"cells\": cells}\n",
        "                    } for cells in table_rows]\n",
        "                ]\n",
        "            }\n",
        "        })\n",
        "\n",
        "        # 6. è½¬å½•æ–‡æœ¬\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": f\"ä¼šè®®è½¬å½•æ–‡æœ¬ï¼ˆèŠ‚é€‰ï¼Œå…±{len(transcript_segments)}æ¡ï¼‰\"}}]}\n",
        "        })\n",
        "        for seg in transcript_segments[:MAX_TRANSCRIPT_SEGMENTS]:\n",
        "            time_str = str(datetime.timedelta(seconds=int(seg[\"start\"])))\n",
        "            children_blocks.append({\n",
        "                \"object\": \"block\",\n",
        "                \"type\": \"paragraph\",\n",
        "                \"paragraph\": {\"rich_text\": [{\"text\": {\"content\": f\"[{time_str}] {seg['text']}\"}}]}\n",
        "            })\n",
        "\n",
        "        # æ£€æŸ¥æ€»å—æ•°\n",
        "        if len(children_blocks) > 100:\n",
        "            children_blocks = children_blocks[:100]\n",
        "            print(f\"âš ï¸ è­¦å‘Šï¼šå†…å®¹å—æ•°é‡è¶…é™ï¼Œå·²æˆªæ–­ä¸º100ä¸ªå—\")\n",
        "\n",
        "        notion.blocks.children.append(block_id=page_id, children=children_blocks)\n",
        "        report_url = new_page.get(\"url\", \"\")\n",
        "\n",
        "        logger.log_step(\"åˆ›å»ºNotionæŠ¥å‘Š\", \"success\", {\"æŠ¥å‘ŠURL\": report_url, \"æ€»å—æ•°\": len(children_blocks)})\n",
        "        print(f\"âœ… NotionæŠ¥å‘Šå·²ç”Ÿæˆï¼ˆæ€»å—æ•°: {len(children_blocks)}ï¼‰\")\n",
        "        return report_url\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"åˆ›å»ºNotionæŠ¥å‘Š\", \"failed\", error=str(e))\n",
        "        print(f\"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# å†™å…¥Notionæ•°æ®åº“ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šåŒ¹é…æ–°è¡¨å¤´ï¼‰\n",
        "def write_to_notion_database(meeting_data, full_transcript, all_languages, total_seconds):\n",
        "    logger.log_step(\"å†™å…¥Notionæ•°æ®åº“\", \"started\")\n",
        "\n",
        "    try:\n",
        "        # è¾…åŠ©å‡½æ•°ï¼šæˆªæ–­æ–‡æœ¬åˆ°Notionå…è®¸çš„æœ€å¤§é•¿åº¦\n",
        "        def truncate_text(text, max_length):\n",
        "            if text and len(text) > max_length:\n",
        "                return text[:max_length-3] + \"...\"  # é¢„ç•™3ä¸ªå­—ç¬¦ç»™çœç•¥å·\n",
        "            return text or \"\"\n",
        "\n",
        "        # è¾…åŠ©å‡½æ•°ï¼šè½¬æ¢ç§’æ•°ä¸º\"xå°æ—¶xåˆ†é’Ÿ\"æ ¼å¼ï¼ˆä¸è¶³1åˆ†é’ŸæŒ‰1åˆ†é’Ÿç®—ï¼‰\n",
        "        def format_duration(seconds):\n",
        "            hours = seconds // 3600\n",
        "            remaining_seconds = seconds % 3600\n",
        "            minutes = (remaining_seconds + 59) // 60  # å‘ä¸Šå–æ•´\n",
        "            return f\"{hours}å°æ—¶{minutes}åˆ†é’Ÿ\"\n",
        "\n",
        "        # è¾…åŠ©å‡½æ•°ï¼šè½¬æ¢ç§’æ•°ä¸ºæ€»åˆ†é’Ÿæ•°ï¼ˆç”¨äºæ•°å­—ç±»å‹çš„Durationå­—æ®µï¼‰\n",
        "        def get_total_minutes(seconds):\n",
        "            return (seconds + 59) // 60  # å‘ä¸Šå–æ•´\n",
        "\n",
        "        # 1. å‡†å¤‡æ•°æ®åº“å­—æ®µå†…å®¹ï¼ˆä¸¥æ ¼åŒ¹é…è¡¨å¤´ï¼‰\n",
        "        database_properties = {\n",
        "            # ä¼šè®®æ ‡é¢˜ï¼ˆæ–‡æœ¬ç±»å‹ï¼‰\n",
        "            \"Meeting Title\": {\"title\": [{\"text\": {\"content\": meeting_data.get(\"meeting_title\", \"Untitled\")}}]},\n",
        "            # å‚ä¸è€…ï¼ˆæ–‡æœ¬ç±»å‹ï¼‰\n",
        "            \"Participant\": {\n",
        "                \"rich_text\": [{\"text\": {\"content\": truncate_text(\n",
        "                    \", \".join(meeting_data.get(\"participant\", [])),\n",
        "                    NOTION_RICH_TEXT_LIMIT\n",
        "                )}}]\n",
        "            },\n",
        "            # æ—¥æœŸï¼ˆæ—¥æœŸç±»å‹ï¼Œæ ¼å¼xxxxå¹´xxæœˆxxæ—¥ï¼‰\n",
        "            \"Date\": {\n",
        "                \"date\": {\n",
        "                    \"start\": datetime.datetime.now().strftime(\"%Y-%m-%d\"),  # Notionæ—¥æœŸæ ¼å¼éœ€ä¸ºISO\n",
        "                    \"end\": None\n",
        "                }\n",
        "            },\n",
        "            # æ—¶é•¿ï¼ˆæ•°å­—ç±»å‹ï¼Œå­˜å‚¨æ€»åˆ†é’Ÿæ•°ï¼‰\n",
        "            \"Duration\": {\n",
        "                \"number\": get_total_minutes(total_seconds)\n",
        "            },\n",
        "            # ä¼šè®®ç±»å‹ï¼ˆæ–‡æœ¬ç±»å‹ï¼‰\n",
        "            \"Meeting Type\": {\n",
        "                \"rich_text\": [{\"text\": {\"content\": truncate_text(\n",
        "                    meeting_data.get(\"meeting_type\", \"æœªæŒ‡å®š\"),\n",
        "                    NOTION_RICH_TEXT_LIMIT\n",
        "                )}}]\n",
        "            },\n",
        "            # ä¼šè®®å¹³å°ï¼ˆé€‰æ‹©ç±»å‹ï¼Œç¡®ä¿å€¼åœ¨é¢„è®¾é€‰é¡¹ä¸­ï¼‰\n",
        "            \"Platform\": {\"select\": {\"name\": meeting_data.get(\"platform\", \"Unknown\")}},\n",
        "            # ä¼šè®®å®Œæ•´åŸæ–‡ï¼ˆæ–‡æœ¬ç±»å‹ï¼‰\n",
        "            \"original meeting script\": {\n",
        "                \"rich_text\": [{\"text\": {\"content\": truncate_text(\n",
        "                    full_transcript,\n",
        "                    NOTION_RICH_TEXT_LIMIT\n",
        "                )}}]\n",
        "            },\n",
        "            # ä¼šè®®è¯­è¨€ï¼ˆæ–‡æœ¬ç±»å‹ï¼‰\n",
        "            \"language\": {\n",
        "                \"rich_text\": [{\"text\": {\"content\": truncate_text(\n",
        "                    \", \".join(all_languages),\n",
        "                    NOTION_RICH_TEXT_LIMIT\n",
        "                )}}]\n",
        "            },\n",
        "            # ä¼šè®®æ€»ç»“ï¼ˆæ–‡æœ¬ç±»å‹ï¼‰\n",
        "            \"Summary\": {\n",
        "                \"rich_text\": [{\"text\": {\"content\": truncate_text(\n",
        "                    meeting_data.get(\"summary\", \"\"),\n",
        "                    NOTION_RICH_TEXT_LIMIT\n",
        "                )}}]\n",
        "            },\n",
        "            # ä¼šè®®å†³å®šï¼ˆæ–‡æœ¬ç±»å‹ï¼‰\n",
        "            \"Decisions\": {\n",
        "                \"rich_text\": [{\"text\": {\"content\": truncate_text(\n",
        "                    \"\\n\".join([f\"- {d}\" for d in meeting_data.get(\"decisions\", [])]),\n",
        "                    NOTION_RICH_TEXT_LIMIT\n",
        "                )}}]\n",
        "            },\n",
        "            # ä¼šè®®æ‹…å¿§ï¼ˆæ–‡æœ¬ç±»å‹ï¼‰\n",
        "            \"Concerns\": {\n",
        "                \"rich_text\": [{\"text\": {\"content\": truncate_text(\n",
        "                    \"\\n\".join([f\"- {c}\" for c in meeting_data.get(\"concerns\", [])]),\n",
        "                    NOTION_RICH_TEXT_LIMIT\n",
        "                )}}]\n",
        "            },\n",
        "            # ä¼šè®®è¦ç‚¹ï¼ˆæ–‡æœ¬ç±»å‹ï¼‰\n",
        "            \"Key Points\": {\n",
        "                \"rich_text\": [{\"text\": {\"content\": truncate_text(\n",
        "                    \"\\n\".join([f\"- {topic}: {', '.join(points)}\"\n",
        "                              for topic, points in meeting_data.get(\"key_points\", {}).items()]),\n",
        "                    NOTION_RICH_TEXT_LIMIT\n",
        "                )}}]\n",
        "            },\n",
        "            # è¡ŒåŠ¨é¡¹ï¼ˆæ–‡æœ¬ç±»å‹ï¼‰\n",
        "            \"Action Items\": {\n",
        "                \"rich_text\": [{\"text\": {\"content\": truncate_text(\n",
        "                    \"\\n\".join([f\"- {item.get('task', '')}ï¼ˆè´Ÿè´£äººï¼š{item.get('assignee', 'æœªåˆ†é…')}ï¼‰\"\n",
        "                              for item in meeting_data.get(\"action_items\", [])]),\n",
        "                    NOTION_RICH_TEXT_LIMIT\n",
        "                )}}]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # 2. å†™å…¥æ•°æ®åº“\n",
        "        new_db_page = notion.pages.create(\n",
        "            parent={\"database_id\": NOTION_DB_ID},\n",
        "            properties=database_properties\n",
        "        )\n",
        "\n",
        "        logger.log_step(\"å†™å…¥Notionæ•°æ®åº“\", \"success\", {\"æ•°æ®åº“é¡µé¢ID\": new_db_page[\"id\"]})\n",
        "        print(f\"âœ… æˆåŠŸå†™å…¥Notionæ•°æ®åº“ï¼ˆæ¡ç›®ID: {new_db_page['id']}ï¼‰\")\n",
        "        print(f\"â±ï¸ ä¼šè®®æ—¶é•¿: {format_duration(total_seconds)}\")\n",
        "        return new_db_page[\"id\"]\n",
        "\n",
        "    except errors.APIResponseError as e:\n",
        "        error_msg = f\"æ•°æ®åº“å†™å…¥å¤±è´¥: {str(e)}\"\n",
        "        logger.log_step(\"å†™å…¥Notionæ•°æ®åº“\", \"failed\", error=error_msg)\n",
        "        print(f\"âŒ {error_msg}ï¼ˆè¯·æ£€æŸ¥æ•°æ®åº“è¡¨å¤´åç§°å’Œç±»å‹æ˜¯å¦ä¸ä»£ç ä¸€è‡´ï¼‰\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        error_msg = f\"æ•°æ®åº“å†™å…¥å¤±è´¥: {str(e)}\"\n",
        "        logger.log_step(\"å†™å…¥Notionæ•°æ®åº“\", \"failed\", error=error_msg)\n",
        "        print(f\"âŒ {error_msg}\")\n",
        "        return None\n",
        "\n",
        "# ======================\n",
        "# è¾“å…¥å¤„ç†\n",
        "# ======================\n",
        "def handle_input():\n",
        "    print(\"\\n=== è¯·é€‰æ‹©è¾“å…¥æ–¹å¼ ===\")\n",
        "    print(\"1: ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ (.mp3/.wav/.m4a/.opus)\")\n",
        "    print(\"2: ä¸Šä¼ å¸¦æ—¶é—´æˆ³çš„è½¬å½•æ–‡æœ¬ (.txt)\")\n",
        "\n",
        "    try:\n",
        "        choice = input(\"è¯·é€‰æ‹© (1/2): \").strip() or \"1\"\n",
        "    except:\n",
        "        choice = \"1\"\n",
        "\n",
        "    if choice == \"1\":\n",
        "        print(\"\\nè¯·ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ï¼ˆæ”¯æŒ2-3å°æ—¶ä¼šè®®å½•éŸ³ï¼‰:\")\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            print(\"âš ï¸ æœªæ£€æµ‹åˆ°ä¸Šä¼ æ–‡ä»¶ï¼Œè¯·é‡è¯•\")\n",
        "            return handle_input()\n",
        "\n",
        "        filename = next(iter(uploaded.keys()))\n",
        "        audio_ext = os.path.splitext(filename)[1].lower()\n",
        "        supported_ext = ['.mp3', '.wav', '.m4a', '.opus']\n",
        "\n",
        "        if audio_ext not in supported_ext:\n",
        "            print(f\"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼ˆæ”¯æŒ: {', '.join(supported_ext)}ï¼‰\")\n",
        "            return handle_input()\n",
        "\n",
        "        audio_path = f\"/tmp/{filename}\"\n",
        "        with open(audio_path, 'wb') as f:\n",
        "            f.write(uploaded[filename])\n",
        "\n",
        "        duration = get_audio_duration(audio_path)\n",
        "        if duration < 3600:\n",
        "            print(f\"âš ï¸ æ£€æµ‹åˆ°éŸ³é¢‘æ—¶é•¿è¾ƒçŸ­ï¼ˆ{duration/60:.1f}åˆ†é’Ÿï¼‰\")\n",
        "            if input(\"æ˜¯å¦ç»§ç»­ä½¿ç”¨é•¿ä¼šè®®æ¨¡å¼å¤„ç†? (y/n): \").strip().lower() != 'y':\n",
        "                os.unlink(audio_path)\n",
        "                print(\"å·²åˆ‡æ¢åˆ°æ™®é€šæ¨¡å¼\")\n",
        "                return handle_input()\n",
        "\n",
        "        print(f\"ğŸµ éŸ³é¢‘ä¿¡æ¯: {filename}ï¼ˆ{duration/60:.1f}åˆ†é’Ÿï¼‰\")\n",
        "\n",
        "        print(\"\\nâš¡ è¯·é€‰æ‹©è½¬å½•æ¨¡å‹:\")\n",
        "        print(\"1: base.en - å¿«é€Ÿæ¨¡å¼ï¼ˆé€‚åˆæ¸…æ™°è¯­éŸ³ï¼‰\")\n",
        "        print(\"2: small.en - å¹³è¡¡æ¨¡å¼ï¼ˆæ¨èï¼Œé€Ÿåº¦ä¸ç²¾åº¦å…¼é¡¾ï¼‰\")\n",
        "        print(\"3: medium.en - é«˜ç²¾åº¦æ¨¡å¼ï¼ˆé€‚åˆå¤æ‚ä¼šè®®ï¼‰\")\n",
        "\n",
        "        model_choice = input(\"è¯·é€‰æ‹© (1-3ï¼Œé»˜è®¤2): \").strip() or \"2\"\n",
        "        model_map = {\"1\": \"base.en\", \"2\": \"small.en\", \"3\": \"medium.en\"}\n",
        "        model_size = model_map.get(model_choice, \"small.en\")\n",
        "        print(f\"å°†ä½¿ç”¨ {model_size} æ¨¡å‹è¿›è¡Œè½¬å½•\")\n",
        "\n",
        "        # æ–°å¢è¿”å›æ€»æ—¶é•¿ï¼ˆç§’ï¼‰\n",
        "        transcript, segments, primary_language, all_languages, total_seconds = transcribe_long_audio(audio_path, model_size)\n",
        "        return transcript, segments, primary_language, all_languages, total_seconds\n",
        "\n",
        "    elif choice == \"2\":\n",
        "        print(\"\\nè¯·ä¸Šä¼ å¸¦æ—¶é—´æˆ³çš„è½¬å½•æ–‡æœ¬ï¼ˆæ ¼å¼ç¤ºä¾‹: [00:05:10] å‘è¨€äºº: ...ï¼‰:\")\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            print(\"âš ï¸ æœªæ£€æµ‹åˆ°ä¸Šä¼ æ–‡ä»¶ï¼Œè¯·é‡è¯•\")\n",
        "            return handle_input()\n",
        "\n",
        "        filename = next(iter(uploaded.keys()))\n",
        "        if not filename.endswith('.txt'):\n",
        "            print(\"âŒ ä»…æ”¯æŒ.txtæ ¼å¼çš„æ–‡æœ¬æ–‡ä»¶\")\n",
        "            return handle_input()\n",
        "\n",
        "        try:\n",
        "            transcript_text = uploaded[filename].decode('utf-8')\n",
        "            segments = []\n",
        "            time_pattern = re.compile(r'\\[(\\d+:\\d+:\\d+)\\]')\n",
        "\n",
        "            for line in transcript_text.split('\\n'):\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                match = time_pattern.search(line)\n",
        "                if match:\n",
        "                    time_str = match.group(1)\n",
        "                    text = time_pattern.sub('', line).strip()\n",
        "                    h, m, s = map(int, time_str.split(':'))\n",
        "                    start_time = h * 3600 + m * 60 + s\n",
        "                    segments.append({\n",
        "                        \"text\": text,\n",
        "                        \"start\": start_time,\n",
        "                        \"end\": start_time + 30,\n",
        "                        \"language\": detect(text) if text.strip() else \"en\"\n",
        "                    })\n",
        "\n",
        "            if not segments:\n",
        "                raise ValueError(\"æœªæ£€æµ‹åˆ°æœ‰æ•ˆæ—¶é—´æˆ³ï¼Œè¯·æ£€æŸ¥æ–‡æœ¬æ ¼å¼\")\n",
        "\n",
        "            # è®¡ç®—æ€»æ—¶é•¿ï¼ˆç§’ï¼‰\n",
        "            total_seconds = segments[-1][\"end\"] - segments[0][\"start\"] if segments else 0\n",
        "\n",
        "            all_languages = list({seg[\"language\"] for seg in segments})\n",
        "            primary_language = all_languages[0] if all_languages else \"en\"\n",
        "            print(f\"âœ… å·²è§£æè½¬å½•æ–‡æœ¬ï¼ˆ{len(segments)}ä¸ªç‰‡æ®µï¼Œæ£€æµ‹è¯­è¨€: {all_languages}ï¼‰\")\n",
        "            return transcript_text, segments, primary_language, all_languages, total_seconds\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ æ–‡æœ¬è§£æå¤±è´¥: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    else:\n",
        "        print(\"âš ï¸ æ— æ•ˆé€‰æ‹©ï¼Œé»˜è®¤ä½¿ç”¨éŸ³é¢‘è¾“å…¥\")\n",
        "        return handle_input()\n",
        "\n",
        "# ======================\n",
        "# ä¸»å‡½æ•°\n",
        "# ======================\n",
        "def main():\n",
        "    print(\"=== ä¼šè®®åˆ†æå·¥å…· ===\")\n",
        "    logger.log_step(\"ä¼šè®®å¤„ç†æµç¨‹\", \"started\")\n",
        "\n",
        "    try:\n",
        "        if not torch.cuda.is_available():\n",
        "            raise RuntimeError(\"è¯·åˆ‡æ¢åˆ°GPUç¯å¢ƒï¼Œè¯·åˆ‡æ¢åˆ°GPUç¯å¢ƒï¼ˆRuntime > Change runtime type > é€‰æ‹©GPUï¼‰\")\n",
        "        print(f\"âœ… æ£€æµ‹åˆ°GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "        # å¤„ç†è¾“å…¥ï¼ˆè·å–æ€»æ—¶é•¿ï¼‰\n",
        "        transcript, segments, language, all_languages, total_seconds = handle_input()\n",
        "        if not transcript or not segments:\n",
        "            raise RuntimeError(\"æœªè·å–åˆ°æœ‰æ•ˆä¼šè®®å†…å®¹\")\n",
        "\n",
        "        # åˆ†æä¼šè®®\n",
        "        meeting_data = analyze_meeting(segments, language)\n",
        "        if \"error\" in meeting_data:\n",
        "            raise RuntimeError(meeting_data[\"error\"])\n",
        "\n",
        "        # è¡¥å……è¯­è¨€ä¿¡æ¯åˆ°ä¼šè®®æ•°æ®\n",
        "        meeting_data[\"all_languages\"] = all_languages\n",
        "\n",
        "        # ç”ŸæˆNotionæŠ¥å‘Š\n",
        "        report_url = create_notion_report(meeting_data, segments)\n",
        "        if not report_url:\n",
        "            raise RuntimeError(\"æ— æ³•ç”ŸæˆNotionæŠ¥å‘Š\")\n",
        "\n",
        "        # å†™å…¥æ•°æ®åº“\n",
        "        db_entry_id = write_to_notion_database(meeting_data, transcript, all_languages, total_seconds)\n",
        "        if not db_entry_id:\n",
        "            raise RuntimeError(\"æ•°æ®åº“å†™å…¥å¤±è´¥ï¼Œä½†æŠ¥å‘Šå·²ç”Ÿæˆ\")\n",
        "\n",
        "        # ä¿å­˜æ—¥å¿—å¹¶è¾“å‡ºç»“æœ\n",
        "        log_file = logger.save_logs()\n",
        "        print(f\"\\nğŸ‰ ä¼šè®®å¤„ç†å®Œæˆï¼\")\n",
        "        print(f\"ğŸ“„ ä¼šè®®æŠ¥å‘Š: {report_url}\")\n",
        "        print(f\"ğŸ“Š æ•°æ®åº“æ¡ç›®ID: {db_entry_id}\")\n",
        "        print(f\"ğŸ“‹ å¤„ç†æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}\")\n",
        "\n",
        "        from IPython.display import HTML\n",
        "        display(HTML(f'<a href=\"{report_url}\" target=\"_blank\">ç‚¹å‡»æŸ¥çœ‹Notionä¼šè®®æŠ¥å‘Š</a>'))\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.save_logs(\"meeting_error_logs.json\")\n",
        "        print(f\"\\nâŒ å¤„ç†å¤±è´¥: {str(e)}\")\n",
        "        print(\"é”™è¯¯è¯¦æƒ…å·²ä¿å­˜åˆ° meeting_error_logs.json\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "DnI0aBLIl8Mf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e7c8e58e-03de-4a57-d2a4-39c1f48d01a7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping whisper as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: faster-whisper==0.10.0 in /usr/local/lib/python3.11/dist-packages (0.10.0)\n",
            "Requirement already satisfied: av==11.* in /usr/local/lib/python3.11/dist-packages (from faster-whisper==0.10.0) (11.0.0)\n",
            "Requirement already satisfied: ctranslate2<5,>=4.0 in /usr/local/lib/python3.11/dist-packages (from faster-whisper==0.10.0) (4.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.11/dist-packages (from faster-whisper==0.10.0) (0.33.4)\n",
            "Requirement already satisfied: tokenizers<0.16,>=0.13 in /usr/local/lib/python3.11/dist-packages (from faster-whisper==0.10.0) (0.15.2)\n",
            "Requirement already satisfied: onnxruntime<2,>=1.14 in /usr/local/lib/python3.11/dist-packages (from faster-whisper==0.10.0) (1.22.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from ctranslate2<5,>=4.0->faster-whisper==0.10.0) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ctranslate2<5,>=4.0->faster-whisper==0.10.0) (1.26.4)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.11/dist-packages (from ctranslate2<5,>=4.0->faster-whisper==0.10.0) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper==0.10.0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper==0.10.0) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper==0.10.0) (23.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper==0.10.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper==0.10.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper==0.10.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper==0.10.0) (1.1.5)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper==0.10.0) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper==0.10.0) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper==0.10.0) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper==0.10.0) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper==0.10.0) (10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper==0.10.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper==0.10.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper==0.10.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper==0.10.0) (2025.7.14)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper==0.10.0) (1.3.0)\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-fmwzrpf_\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-fmwzrpf_\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (1.26.4)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20250625) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: notion-client in /usr/local/lib/python3.11/dist-packages (2.4.0)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: langchain==0.1.13 in /usr/local/lib/python3.11/dist-packages (0.1.13)\n",
            "Requirement already satisfied: langchain-openai==0.0.8 in /usr/local/lib/python3.11/dist-packages (0.0.8)\n",
            "Requirement already satisfied: pydantic==2.5.2 in /usr/local/lib/python3.11/dist-packages (2.5.2)\n",
            "Requirement already satisfied: httpx==0.27.0 in /usr/local/lib/python3.11/dist-packages (0.27.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.13) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.13) (2.0.41)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.13) (3.11.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.13) (0.6.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.13) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.29 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.13) (0.0.38)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.33 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.13) (0.1.53)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.13) (0.0.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.13) (0.1.147)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.13) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.13) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.13) (8.5.0)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.0.8) (1.96.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.0.8) (0.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic==2.5.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in /usr/local/lib/python3.11/dist-packages (from pydantic==2.5.2) (2.14.5)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.11/dist-packages (from pydantic==2.5.2) (4.14.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.0) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.0) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.0) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.0) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.0) (1.3.1)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx==0.27.0) (0.16.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.13) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.13) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.13) (3.0.0)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.2.0,>=0.1.33->langchain==0.1.13) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.13) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.13) (1.0.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.8) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.8) (0.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.1.13) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.1.13) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.13) (3.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.5.2->langchain-openai==0.0.8) (2024.11.6)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.13) (1.1.0)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "36 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n",
            "âœ… æ‰€æœ‰å‡­è¯å·²é…ç½®å®Œæˆï¼Œå‡†å¤‡å¤„ç†ä¼šè®®å†…å®¹\n",
            "=== ä¼šè®®åˆ†æå·¥å…· ===\n",
            "âœ… æ£€æµ‹åˆ°GPU: Tesla T4\n",
            "\n",
            "=== è¯·é€‰æ‹©è¾“å…¥æ–¹å¼ ===\n",
            "1: ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ (.mp3/.wav/.m4a/.opus)\n",
            "2: ä¸Šä¼ å¸¦æ—¶é—´æˆ³çš„è½¬å½•æ–‡æœ¬ (.txt)\n",
            "è¯·é€‰æ‹© (1/2): 1\n",
            "\n",
            "è¯·ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ï¼ˆæ”¯æŒ2-3å°æ—¶ä¼šè®®å½•éŸ³ï¼‰:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-427b77c4-4949-4686-9e21-75862bbd84c6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-427b77c4-4949-4686-9e21-75862bbd84c6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving DUO.mp3 to DUO.mp3\n",
            "âš ï¸ æ£€æµ‹åˆ°éŸ³é¢‘æ—¶é•¿è¾ƒçŸ­ï¼ˆ29.7åˆ†é’Ÿï¼‰\n",
            "æ˜¯å¦ç»§ç»­ä½¿ç”¨é•¿ä¼šè®®æ¨¡å¼å¤„ç†? (y/n): y\n",
            "ğŸµ éŸ³é¢‘ä¿¡æ¯: DUO.mp3ï¼ˆ29.7åˆ†é’Ÿï¼‰\n",
            "\n",
            "âš¡ è¯·é€‰æ‹©è½¬å½•æ¨¡å‹:\n",
            "1: base.en - å¿«é€Ÿæ¨¡å¼ï¼ˆé€‚åˆæ¸…æ™°è¯­éŸ³ï¼‰\n",
            "2: small.en - å¹³è¡¡æ¨¡å¼ï¼ˆæ¨èï¼Œé€Ÿåº¦ä¸ç²¾åº¦å…¼é¡¾ï¼‰\n",
            "3: medium.en - é«˜ç²¾åº¦æ¨¡å¼ï¼ˆé€‚åˆå¤æ‚ä¼šè®®ï¼‰\n",
            "è¯·é€‰æ‹© (1-3ï¼Œé»˜è®¤2): 2\n",
            "å°†ä½¿ç”¨ small.en æ¨¡å‹è¿›è¡Œè½¬å½•\n",
            "ğŸ“Š éŸ³é¢‘å°†åˆ†å‰²ä¸º 3 å—ï¼ˆæ¯å—15åˆ†é’Ÿï¼Œé‡å 30ç§’ï¼‰\n",
            "ğŸš€ å¼€å§‹è½¬å½• 3 ä¸ªåˆ†å—...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rè½¬å½•è¿›åº¦:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ åŠ è½½ small.en æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œéœ€ä¸‹è½½çº¦1.5GBï¼‰...\n",
            "âœ… æ¨¡å‹åŠ è½½æˆåŠŸ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "è½¬å½•è¿›åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:49<00:00, 16.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… è½¬å½•å®Œæˆï¼ˆ329ä¸ªç‰‡æ®µï¼Œä¸»è¦è¯­è¨€: enï¼Œæ‰€æœ‰è¯­è¨€: ['en']ï¼‰\n",
            "\n",
            "å¼€å§‹åˆ†æä¼šè®®å†…å®¹...\n",
            "ğŸ“ å°†ä¼šè®®å†…å®¹åˆ†ä¸º 1 ä¸ªåˆ†æå—\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "åˆ†æè¿›åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ä¼šè®®åˆ†æå®Œæˆï¼ˆ3ä¸ªè¯é¢˜ï¼Œ0ä¸ªè¡ŒåŠ¨é¡¹ï¼‰\n",
            "âœ… æˆåŠŸè®¿é—®çˆ¶é¡µé¢: Parent Page\n",
            "âœ… NotionæŠ¥å‘Šå·²ç”Ÿæˆï¼ˆæ€»å—æ•°: 80ï¼‰\n",
            "âœ… æˆåŠŸå†™å…¥Notionæ•°æ®åº“ï¼ˆæ¡ç›®ID: 2355fee1-8e37-819b-87c1-faffb148c942ï¼‰\n",
            "â±ï¸ ä¼šè®®æ—¶é•¿: 0.0å°æ—¶30.0åˆ†é’Ÿ\n",
            "\n",
            "ğŸ‰ ä¼šè®®å¤„ç†å®Œæˆï¼\n",
            "ğŸ“„ ä¼šè®®æŠ¥å‘Š: https://www.notion.so/Simone-Ereau-s-Journey-to-Becoming-a-Radio-Host-2355fee18e3781039b26d2638f8cb764\n",
            "ğŸ“Š æ•°æ®åº“æ¡ç›®ID: 2355fee1-8e37-819b-87c1-faffb148c942\n",
            "ğŸ“‹ å¤„ç†æ—¥å¿—å·²ä¿å­˜åˆ°: meeting_logs.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<a href=\"https://www.notion.so/Simone-Ereau-s-Journey-to-Becoming-a-Radio-Host-2355fee18e3781039b26d2638f8cb764\" target=\"_blank\">ç‚¹å‡»æŸ¥çœ‹Notionä¼šè®®æŠ¥å‘Š</a>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOLBE8pAfqJWAVmi7incS8J",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5225dd308a5d4cb48bc302acb181a51a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d21b6d3ac7c4581aa961c2f137f69e5",
              "IPY_MODEL_05510ae00af1484688597ea503caabbb",
              "IPY_MODEL_80051aec09bc46bebf1d5ded88dd4c0b"
            ],
            "layout": "IPY_MODEL_09c17810f3804b26b6081a02656a1b18"
          }
        },
        "6d21b6d3ac7c4581aa961c2f137f69e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_843c05f5495247b381100505b7c705c6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_802582a9652746388242a5268447c6c0",
            "value": "config.json:â€‡"
          }
        },
        "05510ae00af1484688597ea503caabbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdc8b29074ed48e1911f4e00a2a4d504",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_108d76ae6f0849329b4d676c664bc9ce",
            "value": 1
          }
        },
        "80051aec09bc46bebf1d5ded88dd4c0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbb1035b5d844018a8a3a4667667297e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_70c9bc2c823c41ed9a20587c08392164",
            "value": "â€‡2.66k/?â€‡[00:00&lt;00:00,â€‡74.9kB/s]"
          }
        },
        "09c17810f3804b26b6081a02656a1b18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "843c05f5495247b381100505b7c705c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "802582a9652746388242a5268447c6c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdc8b29074ed48e1911f4e00a2a4d504": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "108d76ae6f0849329b4d676c664bc9ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cbb1035b5d844018a8a3a4667667297e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70c9bc2c823c41ed9a20587c08392164": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d367cbf85394d18b92bb1eb2a084df4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44b846a51b424fd596ffb878c653c4cf",
              "IPY_MODEL_8751f7b6210f415abe0dc830d008cf7d",
              "IPY_MODEL_47f335afd5fe494387deffc67087a5e4"
            ],
            "layout": "IPY_MODEL_a61edf70480d441d8d2e942672c864eb"
          }
        },
        "44b846a51b424fd596ffb878c653c4cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9242974f1a45442c808fa0705f352966",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c27f195f3b3b464faade50443e552535",
            "value": "tokenizer.json:â€‡"
          }
        },
        "8751f7b6210f415abe0dc830d008cf7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_534d82a48a1e461ab585882259d05c68",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2259349b9ea349788c3970bbe2c8ec83",
            "value": 1
          }
        },
        "47f335afd5fe494387deffc67087a5e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_312bb335266d4ed69f8f7252305cd0fe",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_56c3c6fc295849079148c1a12c6e8044",
            "value": "â€‡2.13M/?â€‡[00:00&lt;00:00,â€‡20.9MB/s]"
          }
        },
        "a61edf70480d441d8d2e942672c864eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9242974f1a45442c808fa0705f352966": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c27f195f3b3b464faade50443e552535": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "534d82a48a1e461ab585882259d05c68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "2259349b9ea349788c3970bbe2c8ec83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "312bb335266d4ed69f8f7252305cd0fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56c3c6fc295849079148c1a12c6e8044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "798068dc88924125ab22f10886f23128": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_500cab9c8d54477b8d36e2db22a54977",
              "IPY_MODEL_91b0bed8d5fe4bed99769996f50e3cd1",
              "IPY_MODEL_e8fb5ed8a63d4ed19b984a4d2ad5b89b"
            ],
            "layout": "IPY_MODEL_a22957686d6e477fbe661dc8ec093028"
          }
        },
        "500cab9c8d54477b8d36e2db22a54977": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d42717cecb9449796c0506f7424ffc3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c049b95701a84e1a9ba5a8724e055c98",
            "value": "vocabulary.txt:â€‡"
          }
        },
        "91b0bed8d5fe4bed99769996f50e3cd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1efb88bd6b7b42d88a065122dd707a20",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_95bfe467e7464e8aae9fd99facc8e8a6",
            "value": 1
          }
        },
        "e8fb5ed8a63d4ed19b984a4d2ad5b89b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c9d245e038440079e80a6d5ddef382b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_dd2e117d0235474abf0627de4a9f6d02",
            "value": "â€‡422k/?â€‡[00:00&lt;00:00,â€‡5.70MB/s]"
          }
        },
        "a22957686d6e477fbe661dc8ec093028": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d42717cecb9449796c0506f7424ffc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c049b95701a84e1a9ba5a8724e055c98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1efb88bd6b7b42d88a065122dd707a20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "95bfe467e7464e8aae9fd99facc8e8a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c9d245e038440079e80a6d5ddef382b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd2e117d0235474abf0627de4a9f6d02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b76a8e7784a4af982866b0f86002d18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1aa693eef9df43309264df780dca2ac2",
              "IPY_MODEL_959b21bc5db542ff9d898f88ee0c6693",
              "IPY_MODEL_3b1bfa6c69604fbf84f326cb08464bed"
            ],
            "layout": "IPY_MODEL_40fa370d582544f4b9127b03f9c488cd"
          }
        },
        "1aa693eef9df43309264df780dca2ac2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cd96c6f61a54489b1e4457c3c6a6014",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5943bdf0509c472e8e5bb06f907eefb0",
            "value": "model.bin:â€‡100%"
          }
        },
        "959b21bc5db542ff9d898f88ee0c6693": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1da30c836bdc460cab8d43cf1cd79f28",
            "max": 483545366,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90d3160710484c0dab01c261f5618cf9",
            "value": 483545366
          }
        },
        "3b1bfa6c69604fbf84f326cb08464bed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c1bcff65e354aa9be14f2e0427da6e3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6337cdf81d1e4ed68b4991add998577e",
            "value": "â€‡484M/484Mâ€‡[00:01&lt;00:00,â€‡360MB/s]"
          }
        },
        "40fa370d582544f4b9127b03f9c488cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cd96c6f61a54489b1e4457c3c6a6014": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5943bdf0509c472e8e5bb06f907eefb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1da30c836bdc460cab8d43cf1cd79f28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90d3160710484c0dab01c261f5618cf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c1bcff65e354aa9be14f2e0427da6e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6337cdf81d1e4ed68b4991add998577e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}