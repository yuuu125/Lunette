{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuuu125/Lunette/blob/main/AI_Assistent1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "qq1cOplyCeys",
        "outputId": "d420d33a-e13d-41e0-d339-c1ecdb8f1fb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai==0.28.1\n",
            "  Downloading openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting notion-client\n",
            "  Downloading notion_client-2.4.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28.1) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28.1) (3.11.15)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from notion-client) (0.28.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->notion-client) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->notion-client) (2025.7.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->notion-client) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->notion-client) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->notion-client) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28.1) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28.1) (2.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (1.20.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.0->notion-client) (1.3.1)\n",
            "Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading notion_client-2.4.0-py2.py3-none-any.whl (13 kB)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=5dcd02cb1ba82e0b3a957def22cef4473eb9bfae2b19d78ef77cd66e31de1c59\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: python-docx, langdetect, openai, notion-client\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.93.3\n",
            "    Uninstalling openai-1.93.3:\n",
            "      Successfully uninstalled openai-1.93.3\n",
            "Successfully installed langdetect-1.0.9 notion-client-2.4.0 openai-0.28.1 python-docx-1.2.0\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-m63anoc3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-m63anoc3\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper==20250625)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper==20250625)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper==20250625)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper==20250625)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper==20250625)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper==20250625)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper==20250625)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper==20250625)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper==20250625)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper==20250625)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20250625) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2025.7.9)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=6ffd41740b1d48008f02ef6bcf02d07a0f29f0f2690dfa2d153848877f48e30a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-k1zhuw3s/wheels/1f/1d/98/9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20250625\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,758 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,266 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,420 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,103 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,104 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,963 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,569 kB]\n",
            "Fetched 26.6 MB in 3s (7,736 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "43 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 43 not upgraded.\n",
            "✅ OpenAI API key set\n",
            "✅ Notion connection verified\n",
            "\n",
            "=== Handling Transcript Input ===\n",
            "Choose input method:\n",
            "1 - Upload text file (.txt or .docx)\n",
            "2 - Paste text directly\n",
            "3 - Upload audio file (transcribe with Whisper)\n",
            "Your choice (1/2/3): 3\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0ec95462-2f6c-4220-9d5b-3597102ffe96\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0ec95462-2f6c-4220-9d5b-3597102ffe96\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving test.mp3 to test.mp3\n",
            "✅ Uploaded audio: test.mp3\n",
            "\n",
            "⚡ Select transcription speed:\n",
            "1 - Fast (tiny model, fastest, lower accuracy)\n",
            "2 - Balanced (base model, recommended)\n",
            "3 - High Quality (small model, slower)\n",
            "Your choice (1/2/3): 1\n",
            "⏱ Audio duration: 52m 36s\n",
            "⏳ Estimated processing time: ~15m 47s\n",
            "🔊 Starting transcription with Whisper (tiny model)...\n",
            "💻 Using device: CPU\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████| 72.1M/72.1M [00:00<00:00, 104MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded Whisper tiny model\n",
            "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
            "Detected language: English\n",
            "[00:00.000 --> 00:04.640]  Support for this podcast and the following message come from Sierra Nevada Brewing Company,\n",
            "[00:04.640 --> 00:08.560]  where pure ingredients and sustainable brewing meet a legacy of craft.\n",
            "[00:08.560 --> 00:11.560]  Share one with a friend today and taste for yourself.\n",
            "[00:11.560 --> 00:15.280]  Sierra Nevada, taste what matters, please drink responsibly.\n",
            "[00:20.080 --> 00:27.280]  From NPR and WBEZ Chicago, this is Weight Weight Don't Tell Me The NPR News Quiz.\n",
            "[00:27.520 --> 00:32.560]  I'm the guy whose voice is bigger than John Hencock's signature.\n",
            "[00:36.880 --> 00:42.000]  Bill Pernive, saying here is your host at the Stutabaker Theatre in downtown Chicago.\n",
            "[00:42.000 --> 00:43.440]  Peter, say go.\n",
            "[00:43.440 --> 00:44.560]  Thank you, Bill.\n",
            "[00:44.560 --> 00:45.680]  Thank you, everybody.\n",
            "[00:46.640 --> 00:48.160]  Thank you all so much.\n",
            "[00:48.160 --> 00:56.320]  This fourth of July, we are celebrating the 249th anniversary of the Declaration of Independence.\n",
            "[00:56.320 --> 01:01.200]  Meaning this is the last year the country can say it's in its 240s.\n",
            "[01:02.880 --> 01:08.240]  And let me tell you, once you get to your 250s, it's all downhill baby.\n",
            "[01:09.440 --> 01:14.160]  But before that happens, we party by revisiting some great times with our favorite guest,\n",
            "[01:14.160 --> 01:17.840]  starting with actor Lauren Graham, who is most famous for playing\n",
            "[01:17.840 --> 01:22.080]  Laura Lye Gilmore in the beloved TV show, The Gilmore Girls.\n",
            "[01:22.160 --> 01:26.480]  When she joined us in March, Peter started by asking her about her new show,\n",
            "[01:26.480 --> 01:32.880]  in which she plays a middle-age professional, forced to work with kids these days.\n",
            "[01:32.880 --> 01:38.400]  I have to ask, I watched the first episode of the Z Suite, and are there actually young people\n",
            "[01:38.400 --> 01:44.160]  writing this show? Because I have to say, not being a young person myself, the young people seem like\n",
            "[01:44.160 --> 01:45.200]  lunatics to me.\n",
            "[01:46.080 --> 01:56.080]  There are. We consulted with actual young advertising people, and obviously it is making fun of all of us.\n",
            "[01:56.080 --> 02:01.920]  So it's not being overly critical of anyone because it's overly critical of everyone.\n",
            "[02:01.920 --> 02:06.080]  It's fine because of the show, I've heard even worse stories.\n",
            "[02:06.960 --> 02:13.840]  I have a friend who's young employee called in sick because his eyes were baggy.\n",
            "[02:13.840 --> 02:15.440]  He had under eye bags.\n",
            "[02:18.160 --> 02:21.440]  Any more time for them to settle before he felt it.\n",
            "[02:23.680 --> 02:25.760]  That's true. That is a thing that happened.\n",
            "[02:27.840 --> 02:32.000]  I want to talk about the fans of the show, but I have to engage in just a little speculation.\n",
            "[02:32.000 --> 02:36.960]  One of the things that I have learned about the Gilmore Girls is that the fame for its references,\n",
            "[02:36.960 --> 02:41.200]  constantly there are web pages giving the explanation of every reference in every episode of the\n",
            "[02:41.280 --> 02:45.840]  Gilmore Girls. In the very first scene of the first episode of the Gilmore Girls,\n",
            "[02:45.840 --> 02:51.760]  your character, Laura Lye, offers some flavored lip gloss to your daughter, Rory.\n",
            "[02:51.760 --> 02:58.320]  In one of the very first scenes of Z Suite, your character describes one of the colleagues as so\n",
            "[02:58.320 --> 03:01.680]  young she still uses flavored lip gloss.\n",
            "[03:03.600 --> 03:05.840]  Was that most NPR?\n",
            "[03:06.320 --> 03:12.560]  I think this is God of the Asuttle Cobb Act, right?\n",
            "[03:12.560 --> 03:19.920]  On somebody's. No? No? No? I mean, I love. I don't think so. No one, no one, I did not make that\n",
            "[03:19.920 --> 03:24.160]  association and no one said, hey, that's a little Easter egg for you.\n",
            "[03:24.960 --> 03:26.880]  I think it's just your very smart man.\n",
            "[03:28.240 --> 03:30.560]  You are a very lovely woman, but we do that.\n",
            "[03:31.280 --> 03:33.920]  Let's talk about the Gilmore Girls.\n",
            "[03:35.040 --> 03:41.680]  Gilmore Girls is so beloved that there are two fan conventions that this year in Connecticut alone.\n",
            "[03:42.640 --> 03:51.600]  Have you ever gone to one? No. I haven't.\n",
            "[03:53.760 --> 04:00.480]  It is the 25th anniversary this year and we are in discussions by myself and Amy Sherman\n",
            "[04:00.480 --> 04:06.800]  Paladino, the creator of the show, to say what can we do? What can we do to give people\n",
            "[04:07.840 --> 04:12.560]  the experience they seem to crave of community around the show?\n",
            "[04:13.840 --> 04:18.960]  Maybe getting all of us together in some way so we're working on it.\n",
            "[04:18.960 --> 04:21.280]  You're working on it. So there might be something like...\n",
            "[04:21.280 --> 04:26.480]  I don't see it. There will be something what could it be at an in-in- Connecticut?\n",
            "[04:26.480 --> 04:30.560]  I don't know. It will be great to have all the people wear plaid. That will be exciting.\n",
            "[04:31.680 --> 04:35.920]  Many years ago I had the privilege of interviewing Leonard Neymoy and he had a thing\n",
            "[04:36.720 --> 04:41.760]  earlier in his career where he got very upset that people thought he was Mr. Spock.\n",
            "[04:41.760 --> 04:49.200]  He later embraced it and I wonder you played a similarly iconic character to people think you\n",
            "[04:49.200 --> 04:55.200]  are you, Lauren Graham, actual human being, are Laura like Gilmore, fictional creation?\n",
            "[04:56.080 --> 05:00.480]  Yes. And I don't think I've worked hard enough to just wait them for believe in that.\n",
            "[05:02.000 --> 05:09.760]  I think the show, as any long-running TV show, you become it and it becomes you and sort of the\n",
            "[05:09.760 --> 05:16.400]  reason I gravitated toward this way back when I first read the pilot, it felt like familiar somehow.\n",
            "[05:16.400 --> 05:23.840]  It felt like the way I speak or think already. So it was kind of meant to be in that way.\n",
            "[05:23.840 --> 05:31.600]  And yes, in general it's really positive people view me as their cool mom and\n",
            "[05:32.400 --> 05:36.640]  I don't, you know, that's not bad. No, it has it ever gotten awkward as anybody\n",
            "[05:36.640 --> 05:40.160]  like come and like laid out their troubles and ask for Laura's advice?\n",
            "[05:40.720 --> 05:48.160]  Yeah, I mean, it's not even awkward so much as it's and this is just being on TV and playing\n",
            "[05:48.240 --> 05:53.040]  someone who's like not Walter White, you know, like if you play a friendly kind of\n",
            "[05:54.080 --> 06:01.760]  warm person, like people just feel that they know you and you know, people cry sometimes and\n",
            "[06:02.960 --> 06:08.240]  you know, it gets awkward like if I'm in the bathroom and like coming out of a stall like that's\n",
            "[06:08.240 --> 06:13.600]  not my favorite like oh my god, can I? And I'm like let me just leave this room and just\n",
            "[06:14.480 --> 06:21.840]  somewhere else. Lauren going back in your life way before Gilmore girls you were in your college\n",
            "[06:21.840 --> 06:29.280]  occupella group is that right? Yes, yes, and in your on Broadway in guys and dolls. How often do you\n",
            "[06:29.280 --> 06:37.040]  sing now in your life? Not often, but I do, I am still in a text thread with the Columbia\n",
            "[06:37.040 --> 06:44.400]  Metrotones, my group of songs we think would make good aquapellas songs. They're not going to\n",
            "[06:44.400 --> 06:47.600]  perform and we're not going to arrange them, we're not going to get together and even like we're\n",
            "[06:47.600 --> 06:52.640]  rehearsed but what there is kind of a it's always there once once you're an aquapella you never leave\n",
            "[06:54.640 --> 07:01.600]  I know in that group we had because we traveled other colleges on the weekends and we\n",
            "[07:01.600 --> 07:08.720]  had to have and perhaps you can employ this if it's a problem in your life. I know harmony in the car\n",
            "[07:08.720 --> 07:18.240]  rule because I'm singing along to the like it songs in you know third it was really airtating.\n",
            "[07:18.240 --> 07:25.040]  Wow, well Lauren Graham it is a joy to talk to you and we have asked you to play a game and we are\n",
            "[07:25.040 --> 07:35.040]  calling it Gilmore girl meet girls with Gil. I know all right work with me here work with me\n",
            "[07:35.040 --> 07:40.560]  you play to Gilmore girls we're going to ask you three questions about Gil girls that is\n",
            "[07:40.560 --> 07:45.600]  mermaids okay makes sense answer two to three questions correctly you'll win a prize whenever\n",
            "[07:45.600 --> 07:51.040]  listeners the voice of anyone they choose from our show on their voice mail perhaps mothering them\n",
            "[07:51.040 --> 07:55.440]  well that's a great gift it is I think I think it's the only one we could possibly afford so\n",
            "[07:55.440 --> 08:01.040]  it better bill who is Lauren Graham playing for e-foot Murray of oak park Illinois\n",
            "[08:01.040 --> 08:03.040]  ah\n",
            "[08:04.560 --> 08:07.040]  place I know wow here we go Lauren you ready to play\n",
            "[08:07.760 --> 08:12.640]  I am all right here's your first question the old 20,000 leagues under the C ride at Disney land\n",
            "[08:12.640 --> 08:19.040]  now gone well for brief period in the 1960s had actresses dressed up as mermaids\n",
            "[08:19.120 --> 08:26.400]  lounging on rocks in the lagoon and waving to the visitors they had to end that part of the attraction\n",
            "[08:26.400 --> 08:32.000]  just after a few years why a one of the mermaids got a tail caught in the submarine and got dragged\n",
            "[08:32.000 --> 08:36.800]  through the lagoon b visitors kept jumping in the water and trying to hit on the mermaids\n",
            "[08:37.440 --> 08:43.040]  or see somebody who said they represented the real mermaid community said it was offensive stereotyping\n",
            "[08:44.000 --> 08:55.840]  um I believe that people would get in the water to meet them yes you apparently have met some people\n",
            "[08:56.560 --> 08:59.040]  yes that's right\n",
            "[09:00.240 --> 09:05.200]  uh it was unsurprisingly mostly men who were jumping into the water to go talk to the mermaids I\n",
            "[09:05.200 --> 09:10.400]  don't know if the man had noticed that the mermaids are fish from the waist down all right\n",
            "[09:10.560 --> 09:16.960]  very good very perceptive here's your next question the most famous mermaid attraction in\n",
            "[09:16.960 --> 09:22.320]  America is of course the mermaids of wiki watchy springs also in Florida if you were to dive to the\n",
            "[09:22.320 --> 09:28.880]  bottom of the wiki watchy springs with the mermaids play 75 years after that show started what would\n",
            "[09:28.880 --> 09:36.480]  you find down there a about 10 metric tons of loose plastic mermaids scales b a carton of cigarettes\n",
            "[09:36.560 --> 09:41.840]  that was dropped by a mermaid in 1968 who actually thought she could have a smoke break down there\n",
            "[09:42.560 --> 09:47.120]  or see nobody has any idea because nobody's ever seen the bottom\n",
            "[09:49.120 --> 09:57.680]  well scales I guess scales no it's not scales it's that nobody knows wiki watchy springs is the\n",
            "[09:57.680 --> 10:02.800]  deepest natural springs in the world and nobody's gotten down there all right you have one more chance\n",
            "[10:02.880 --> 10:09.360]  if you get this right you win an aquarium in china also offers a mermaid show with performers dressed as\n",
            "[10:09.360 --> 10:15.840]  mermaids performing this time in a giant fish tank but they were recently accused of covering up an\n",
            "[10:15.840 --> 10:22.160]  incident in which what happened a the tail fell off a particular mermaid revealing it to be a merman\n",
            "[10:24.080 --> 10:27.920]  b the head fell off a mermaid revealing it to be a giant surgeon\n",
            "[10:28.240 --> 10:39.600]  or see a giant surgeon tried to eat a mermaids head there the audience is yelling c no no they're just\n",
            "[10:39.600 --> 10:47.440]  an acopelic room yeah now that's either yelling c in c so I being booed no no you're not being\n",
            "[10:47.520 --> 10:52.960]  booed you're being helped you're being helped by the by the c c it's c it is c yes\n",
            "[10:56.400 --> 11:02.720]  the the giant surgeon if he's in the tank just swam and over and took just tried to swallow\n",
            "[11:02.720 --> 11:07.280]  that mermaids head and I have to say having seen the video it is horrifying but in a good way\n",
            "[11:08.400 --> 11:15.600]  the mermaid was fine she's okay in my own defense I believe that you could have a shell bruiser\n",
            "[11:16.000 --> 11:20.320]  that was deceptively inhabited\n",
            "[11:21.360 --> 11:26.000]  I think I think you're right I think with mermaids I think that would be possible\n",
            "[11:26.000 --> 11:32.000]  I'm going to grant you that bill how did Lauren Graham do in our quiz? Lauren got two out of three\n",
            "[11:32.000 --> 11:41.120]  and that is who wins Lauren you did that like Laura Lai you were thoughtful you struggled a bit\n",
            "[11:41.200 --> 11:48.160]  but you want to need you came through Lauren Graham is now starring on the z suite you can find it on\n",
            "[11:48.160 --> 11:52.400]  to be Lauren Graham thank you so much for joining us on wait wait wait wait wait wait\n",
            "[11:52.400 --> 11:56.160]  touch on that was in the light thank you Lauren thank you bye bye\n",
            "[12:00.560 --> 12:07.280]  when we come back comedian Roy Wood Jr Denys stealing our stick and Amanda Cypher teaches us how to dance\n",
            "[12:07.280 --> 12:12.560]  like you know someone's watching that's when we return with more way way don't tell me from N.P.\n",
            "[12:18.000 --> 12:23.120]  this message comes from N.P. our sponsor Rosetta Stone an expert in language learning for\n",
            "[12:23.120 --> 12:29.600]  30 years right now N.P. our listeners can get Rosetta stones lifetime membership to 25 different\n",
            "[12:29.600 --> 12:35.600]  languages for 50% off learn more at Rosetta Stone dot com slash N.P. R.\n",
            "[12:59.600 --> 13:05.680]  generation just better customer experiences built on Sierra visit Sierra dot AI to learn more\n",
            "[13:29.600 --> 13:34.960]  instrument he our sponsor HP easily search through personal files gang valuable insights and make\n",
            "[13:34.960 --> 13:41.680]  smarter more informed business decisions unlock the future of work today with the HP AI PC with the\n",
            "[13:41.680 --> 13:47.920]  right tools work doesn't have to feel like work to learn more go to hp dot com slash AI PC\n",
            "[13:48.880 --> 13:59.120]  come in PR and WDV easy Chicago this is wait wait don't tell me the N.P. our news quiz\n",
            "[13:59.120 --> 14:05.040]  I'm Bill Curtis and here is your host at the student maker it's either in downtown Chicago\n",
            "[14:05.040 --> 14:12.160]  Peter Segel thank you Bill so we are having a blowout celebration for our nation's\n",
            "[14:12.240 --> 14:18.080]  249th birthday because this is the last year we can credibly claim to be young\n",
            "[14:18.080 --> 14:22.080]  once you hit 250 here flags starts to say\n",
            "[14:24.400 --> 14:29.440]  before our crow's feet land we're revisiting some of our favorite moments from the last year\n",
            "[14:29.440 --> 14:36.640]  including this visit with comedian Roy Wood Jr. from March Peter ask him where he got the idea\n",
            "[14:36.640 --> 14:43.520]  for his latest gig hosting a comedy quiz show about the week's news I have another show\n",
            "[14:43.520 --> 14:49.440]  that I'm getting ready to host called fortune of the wheel\n",
            "[14:50.400 --> 14:55.200]  letters yeah very smart yes smart I want to talk about your new special uh\n",
            "[14:55.200 --> 15:00.640]  lonely flowers which is truly great uh on hulu and I found out some things about you including\n",
            "[15:00.640 --> 15:05.440]  that you started doing stand up and you were 19 years old yeah which is a\n",
            "[15:05.440 --> 15:10.080]  I was still in school at Florida and M.M. and and and what inspired you to pursue that\n",
            "[15:10.080 --> 15:16.560]  difficult life I was born at school for journalism and I would get laughs and so I was like all right\n",
            "[15:16.560 --> 15:22.000]  well this feels like comedy I'm gonna go do that and I would just sleep in bus stations and do\n",
            "[15:22.000 --> 15:27.200]  stand up get back to Tallahassee on Monday and go to Golden Corral that night work\n",
            "[15:27.760 --> 15:32.640]  and just go to class the next three days yeah that was my life I there are a couple things about\n",
            "[15:32.960 --> 15:36.560]  that that I want to ask you about one of which is that you've said that that job at Golden\n",
            "[15:36.560 --> 15:42.080]  Corral which is a buffet was like one of the most important formative experiences of your life yeah\n",
            "[15:42.800 --> 15:50.400]  I think that every American should either serve in the military a year or the food service industry\n",
            "[15:50.400 --> 15:54.960]  for three years because when you work in a restaurant especially it midsize like that when a staff\n",
            "[15:54.960 --> 16:01.280]  was about 40 to 50 back in front of house that job your first job is a teenager that's the first time\n",
            "[16:01.280 --> 16:07.920]  you encounter adults who don't give a f*** about you most adults I'm serious most adults in your\n",
            "[16:07.920 --> 16:13.200]  life up into that point have a vested interest in you being okay right but I work with the dude\n",
            "[16:13.200 --> 16:22.400]  we literally called cocaine mine this is a man who's 39 and doesn't care what 18 year old Roy and\n",
            "[16:22.400 --> 16:30.640]  he's gonna talk to you about life and I feel like it also introduces you to every type of American\n",
            "[16:30.640 --> 16:37.600]  I've worked in North Florida so everything from white supremacists to nons to pastors to gang\n",
            "[16:37.600 --> 16:43.920]  bangers to cut you meet literally every type of person and you have to figure out a way to connect with them\n",
            "[16:43.920 --> 16:51.120]  it's hands down the best life school I ever got was 2013 in our in Tallahassee Florida\n",
            "[16:51.120 --> 16:58.160]  wow just in a curiosity this seems like the best commercial from Golden Corral I ever heard\n",
            "[16:58.160 --> 17:02.480]  but here's the question you've been pretty famous for at least a decade on tv the daily show a lot of\n",
            "[17:02.480 --> 17:08.560]  other things has anybody who like knew you back then reached out and said well I was the white supremacist\n",
            "[17:08.560 --> 17:16.480]  remember me I was the Nazi tattoo I'm cocaine mine that's for example I don't know where cocaine\n",
            "[17:16.480 --> 17:25.680]  like is but I sure hope that prison has in piaor there's another story you tell in the special\n",
            "[17:25.680 --> 17:29.680]  which I actually when I was unexpected because it's extremely funny and I didn't expect to be moved\n",
            "[17:30.960 --> 17:36.160]  you start back when you were staying in bus stations because you couldn't afford hotel and the story is\n",
            "[17:36.160 --> 17:41.120]  is that your mother found out somebody you're somebody ratted you out to your mom and yeah and she didn't\n",
            "[17:41.120 --> 17:48.000]  know you were out doing comedy right she had a student that was a bag of Chandler at the bus station and\n",
            "[17:48.000 --> 17:53.120]  he went to her class and then said Dr Wood I saw your son sleeping in a bus station you ain't seen none of my\n",
            "[17:53.120 --> 18:03.200]  damn sons my baby and tell I have no he's not Jewish down town he's sleeping at the bus station\n",
            "[18:03.200 --> 18:12.080]  and so my mom never agreed or understood why comedy was what I wanted to do but she was the one who\n",
            "[18:12.080 --> 18:17.680]  put down for what ended at being my first road car and I think my mom's objective was to get me the\n",
            "[18:17.680 --> 18:23.040]  car so that I could drive back to Tallahassee after the show but instead I would now just travel\n",
            "[18:23.040 --> 18:34.960]  twice as far and sleep in the car right in bus station parking lots and well Roy it is so great to\n",
            "[18:34.960 --> 18:40.800]  talk to you and we have asked you here to play a little game with us this time we're calling the game\n",
            "[18:41.360 --> 18:48.240]  have we got booze for you so you host the nns have I got news for you we're gonna ask you three\n",
            "[18:48.240 --> 18:54.320]  questions about ghosts and hauntings booze I believe in ghosts by the way you do do you have any\n",
            "[18:54.320 --> 18:59.920]  reason to leave in ghosts yeah I was I was dating a wood or were and we were trying to have six\n",
            "[18:59.920 --> 19:08.320]  and I kept getting the Charlie horse not feel like it was a bit of well all right knowing\n",
            "[19:08.320 --> 19:16.080]  both your belief in the supernatural and the reasons therefore I will still proceed Bill who is Roy\n",
            "[19:16.160 --> 19:22.080]  would you're playing for Peter grieving of a collect civil Georgia all right here's your first question\n",
            "[19:23.440 --> 19:29.360]  one of the most famous hauntings in u.s history was the red ghost the spirit that haunted\n",
            "[19:30.080 --> 19:37.200]  rural Arizona in the late 1800s people were quite relieved though when the red ghost turned out to be\n",
            "[19:37.200 --> 19:42.960]  what was it a a vaudeville comedian who was trying to promote himself as being quote dead funny\n",
            "[19:43.600 --> 19:49.680]  be a basset hound which no one in Arizona had ever seen before or see a funeral camel\n",
            "[19:49.680 --> 20:01.200]  that had been a part of a failed camel cavalry in the u.s army oh I that feels like a sea\n",
            "[20:01.760 --> 20:07.760]  give me give me see give me the camel cavalry you got it and you that's correct nice it was a camel\n",
            "[20:08.720 --> 20:13.280]  it had run away from the camel cavalry it was out enjoying itself people would see it and get\n",
            "[20:13.280 --> 20:20.240]  scared of the army camel corps by the way was created by Jefferson Davis one of his many many good\n",
            "[20:20.240 --> 20:28.560]  ideas all right all right second question every country has their own legends of ghosts\n",
            "[20:28.560 --> 20:33.440]  their own versions in Japan for example you could be visited in the middle of the night by a\n",
            "[20:33.520 --> 20:43.440]  camel carry a ghost that does what a gives you a really really bad haircut be just sits looks at you\n",
            "[20:43.440 --> 20:51.040]  shakes it's head size and leaves or see raids a refrigerator and invariably steals what you were\n",
            "[20:51.040 --> 20:59.440]  saving for lunch the next day I don't I don't Japan has a lot of customs around food so I don't\n",
            "[20:59.440 --> 21:05.120]  think a ghost will be disrespectful on the food solid. Not even a ghost yeah I can see that\n",
            "[21:05.120 --> 21:11.200]  yeah logic you'd be bad haircut I've seen some bad haircut in Asia I've been over a couple of times\n",
            "[21:11.200 --> 21:16.560]  maybe it was a ghost admitted so your choice is A the haircut Roy is right he picked correctly\n",
            "[21:17.600 --> 21:25.040]  it is okay stories spread back in olden days about people walking down the streets of Japan and\n",
            "[21:25.120 --> 21:29.200]  all of a sudden their hair would fall to the ground without them noticing it was the coming carry\n",
            "[21:29.760 --> 21:35.360]  you're doing very well Roy one more question for you last question a lot of people believe ghosts are real\n",
            "[21:35.360 --> 21:40.960]  in fact so many people believe in ghosts which of these is true a new Mexico you can drive in the\n",
            "[21:40.960 --> 21:48.000]  carpool lane if you have a ghost in the car be Vermont taxpayers are allowed to claim a ghost as a\n",
            "[21:48.080 --> 21:54.960]  dependent or see if you are selling a home in New York you have to disclose if it is haunted\n",
            "[21:54.960 --> 22:00.720]  Vermont seems like a nice fun happy go lucky type of place give me claim an a ghost on the\n",
            "[22:00.720 --> 22:06.960]  taxes no it wasn't fact if you sell a house in New York you have to tell people if you believe\n",
            "[22:06.960 --> 22:12.640]  the house is haunted Bill how did Roy would you and you're doing our quiz two out of three gives you\n",
            "[22:12.720 --> 22:20.320]  bragging rights for your panel congratulations Roy you won yeah Roy would Jr is a\n",
            "[22:20.320 --> 22:25.440]  comedian in the host of CNN's have I got news for you his new stand-up special lonely flowers\n",
            "[22:25.440 --> 22:31.680]  which is both funny and a little heartbreaking is streaming on Hulu Roy would Jr what a joy to talk to you\n",
            "[22:32.400 --> 22:37.680]  thank you so much for your hard work great pleasure to talk to you a brother and quiz take care bye bye\n",
            "[22:43.280 --> 22:45.280]  you\n",
            "[22:49.360 --> 22:55.440]  also in March we spoke to the actor Amanda Cyfred famous for her roles in Mean Girls Mama Mia and the\n",
            "[22:55.440 --> 23:01.200]  dropout her latest project has her playing a cop which turned out to be a childhood dream of hers\n",
            "[23:01.200 --> 23:06.560]  I had this weird obsession with the first 48 I would watch like two episodes before bed every night\n",
            "[23:07.520 --> 23:16.320]  I think that's probably why I was so anxious but yeah sure I mean I just think it's cool and I\n",
            "[23:16.320 --> 23:20.880]  never wanted to play detective don't get me wrong those are their cool but they're everywhere\n",
            "[23:21.760 --> 23:26.960]  beat cops is where it's at yeah really so I just feel so slight that like I know and would\n",
            "[23:26.960 --> 23:32.400]  believe me with that kind of authority so I just wanted to prove myself and everybody else wrong\n",
            "[23:32.400 --> 23:38.800]  wow so your model for the copy wanted to be was not like say cojack but like the little\n",
            "[23:38.800 --> 23:47.440]  bunny in Zootopia that's exactly who I modeled wow yeah I can see that yeah in preparation for this\n",
            "[23:47.440 --> 23:52.720]  role you did something I am told again that I know a lot of actors do which he did a ride along\n",
            "[23:52.720 --> 24:00.720]  with real Philadelphia police is that true yeah I had no business being there but I went and it was\n",
            "[24:00.800 --> 24:10.400]  something yeah yeah let's go give a while let's break up the fight they like they let me choose\n",
            "[24:10.400 --> 24:15.120]  which place we were gonna go next I do they really well there's a wellness check you want to do that\n",
            "[24:15.120 --> 24:22.400]  I'm like that could be anything we should go and it turned out to be a dead person but\n",
            "[24:23.680 --> 24:29.680]  so the wellness was pretty low yeah not a lot of wellness there yeah I can't we do it again actually\n",
            "[24:30.320 --> 24:35.280]  sure it occurs to me though I mean well the cops who have you that you could come in handy like\n",
            "[24:35.280 --> 24:40.080]  if a gunfight got for a bit of breakout they could shout put down your weapons and man to\n",
            "[24:40.080 --> 24:48.720]  sigh Fred is here there's like a third mama Mia movie on the line and if that ever happened listen\n",
            "[24:48.720 --> 24:53.760]  I'm for it I'll do anything to save a life right all right\n",
            "[24:53.920 --> 25:03.440]  uh speaking of mama Mia we have read this might be urban legend it might be true we have read that\n",
            "[25:03.440 --> 25:07.840]  like when you were making that movie and it's sequel on these beautiful places that the entire\n",
            "[25:07.840 --> 25:15.760]  cast was drunk the entire time yeah yeah\n",
            "[25:16.400 --> 25:28.400]  once again yeah yeah yeah yeah no it really was um debockery yeah yeah it it it it it it it\n",
            "[25:28.400 --> 25:33.680]  look like I mean it's it seemed like part of the appeal of the movie was just imagining\n",
            "[25:33.680 --> 25:38.160]  being able to make it with you guys because a boy it looked like fun they really was actually\n",
            "[25:38.160 --> 25:44.560]  those those images that came out a while back of us in our most drunken state\n",
            "[25:45.440 --> 25:53.200]  at some party where we did karaoke on the on in scopolis it just looks it looks like the most fun\n",
            "[25:53.200 --> 25:59.280]  anyone could ever have especially because mariles dreamers at the center of every photo and and\n",
            "[25:59.280 --> 26:04.080]  like I wish I could say I wasn't that fun but my grandmother got drunk that night\n",
            "[26:05.760 --> 26:12.160]  and it was a memory that I will never it I just I hold it so close and and and I really look forward to\n",
            "[26:12.160 --> 26:19.920]  a third so just so we could keep getting drunk together right so you want an Emmy for playing\n",
            "[26:19.920 --> 26:24.480]  Elizabeth Holmes and the dropout the story of her and Theranos and perhaps the single most iconic\n",
            "[26:24.480 --> 26:31.440]  moment in the show is when you as a Elizabeth kind of dances into your boy friend's office to\n",
            "[26:31.440 --> 26:37.440]  little Wayne either seduce him or cheer him up or both and it is somehow the most awkward thing I have\n",
            "[26:37.440 --> 26:43.760]  ever seen and my question is how does someone who knows how to dance dance badly?\n",
            "[26:46.400 --> 26:52.240]  I'm going to be honest I'm not a good dancer really I really am not I'm not a kid or she thought that was\n",
            "[26:52.240 --> 26:58.720]  really good really oh god I was like the best dancing she could do I don't know I was wearing a\n",
            "[26:59.760 --> 27:06.160]  dance well or even so seriously when they're wearing that puffy vest yeah what when that really happened\n",
            "[27:06.160 --> 27:11.200]  presumably it did little Wayne felt a horrible twin somewhere he just knew something was wrong he was\n",
            "[27:11.200 --> 27:19.920]  more uncomfortable than you were and as far as I know I tried to find this out you have never\n",
            "[27:19.920 --> 27:24.720]  met Elizabeth Holmes and she has never reached out to meet you is that right correct yeah so it's\n",
            "[27:24.720 --> 27:28.640]  it was better than I didn't because then I wouldn't have been able to make fun of her too because\n",
            "[27:28.640 --> 27:33.760]  part of the show is getting on the inside and trying to you know breed some kind of compassionate and\n",
            "[27:33.760 --> 27:40.400]  show it three-dimensional person but the other part is like making fun of her like with the\n",
            "[27:40.400 --> 27:46.000]  like with the scene that was you did both exceptionally well well Amanda's I've read this is a\n",
            "[27:46.000 --> 27:52.320]  joy to talk to you and we have asked you here to play a game we're calling you me and girl need\n",
            "[27:52.320 --> 28:00.000]  nice guy so you began your career by starring in the classic movie comedy Mean Girls so in honor of\n",
            "[28:00.000 --> 28:05.600]  that we found three questions about some guys who are actually really really nice answer just\n",
            "[28:05.600 --> 28:08.560]  two of them correctly you'll win our prize for one of our listeners bill who is Amanda's\n",
            "[28:08.560 --> 28:21.280]  Cyphered playing for Michelle Lucira of Cleveland Ohio all right you same you seem a little\n",
            "[28:21.280 --> 28:25.680]  good tips I hope you were warned that this would be happening get some wine girl right\n",
            "[28:26.000 --> 28:32.960]  all right here we go here's the first question Mr Rogers was possibly the nicest person of all\n",
            "[28:32.960 --> 28:38.400]  time after Mr Rogers filed a police report that his car had been stolen what happened two days later\n",
            "[28:38.400 --> 28:44.160]  a PBS pledged money to him to buy a new car being neighbors complained about all the people\n",
            "[28:44.160 --> 28:49.120]  clogging up their street hoping to give them a ride somewhere or see the thieves return the car with\n",
            "[28:49.120 --> 28:56.400]  a note that said if we'd known it was yours we never would have taken it see yes that's what happened\n",
            "[28:56.400 --> 29:04.240]  yeah he was so nice he could turn other people into nice guys through osmosis he was amazing all right\n",
            "[29:04.240 --> 29:08.720]  that's very well done here's your next question during will board two candid of famously treated\n",
            "[29:08.720 --> 29:15.040]  their POW so well that some of them didn't want to go back to Germany when the war was over according\n",
            "[29:15.040 --> 29:21.040]  to one capture German corporal that great treatment at the POW camp included which of these a the\n",
            "[29:21.040 --> 29:26.960]  government brought in a famous chef to make authentic schnitzel for them b the guards would\n",
            "[29:26.960 --> 29:33.040]  regularly lend the prisoners their rifles so they could go hunting or see upon request candidate\n",
            "[29:33.040 --> 29:39.920]  would fly in a soldier's wife and kids so they could all be POWs together nass a i'm afraid it was\n",
            "[29:39.920 --> 29:45.680]  actually be they gave them rifles to go hunting oh I can't believe it's a boring one really can't be\n",
            "[29:45.680 --> 29:53.920]  be no there version of a trust fall they hand the rifle close the rise turn around\n",
            "[29:55.520 --> 30:00.720]  alright anyway here's your last question sometimes it's hard to tell whether or not someone is nice\n",
            "[30:01.520 --> 30:06.160]  like the man who helped out a woman in Wales one day by hanging up her laundry to dry\n",
            "[30:06.160 --> 30:10.720]  washing her floor putting her groceries away and taking out the recycling but there was one\n",
            "[30:10.720 --> 30:15.840]  catch what was it a he had broken in her house to do these things while she was away at work\n",
            "[30:16.480 --> 30:20.720]  be the whole time he was working he told her how bad her clothing and food choices were\n",
            "[30:20.720 --> 30:28.320]  or see after he finished he told her now you have to come do my house um yeah i mean a seems like\n",
            "[30:28.320 --> 30:38.480]  the obvious choice yes you're right an animal he broke into her house and he did all those things\n",
            "[30:38.480 --> 30:44.480]  for her and then she came home and found him doing them now they're married she likes a bad boy\n",
            "[30:44.480 --> 30:53.200]  yes he built how did Amanda Cyford do in our quiz mom Mia two out of three is a win\n",
            "[30:53.360 --> 31:01.840]  and that's it well done Amanda Cyford is an Emmy winning actor who you can see right now\n",
            "[31:01.840 --> 31:10.080]  in long bright river all episodes are streaming on peacock now go watch it Amanda thank you so much\n",
            "[31:10.080 --> 31:17.120]  for being with us what a pleasure to talk to you as soon as I'm late my honey you guys we will\n",
            "[31:17.280 --> 31:28.160]  come on coming up Jim Gaffigan the world's funniest dad an mxm2 the world's most popular\n",
            "[31:28.160 --> 31:36.240]  ukulele is is ukulele you could the world's popular player of the ukulele that's when we come back\n",
            "[31:36.240 --> 31:38.960]  with more of wait wait don't tell me from nbr\n",
            "[31:47.440 --> 31:53.280]  support for npr and the following message come from ixl learning ixl learning uses advanced\n",
            "[31:53.280 --> 31:58.400]  algorithms to give the right help to each kid no matter the age or personality get an exclusive\n",
            "[31:58.400 --> 32:07.360]  20% off ixl membership when you sign up today at ixl.com slash npr this message comes from amazon\n",
            "[32:07.360 --> 32:12.720]  when you're on hold with a doctor's officer pharmacy and the operator says your call is very important\n",
            "[32:13.600 --> 32:19.120]  after the tenth time well suddenly your call doesn't seem very important to them at all amazon one\n",
            "[32:19.120 --> 32:25.440]  medical has 24 seven virtual care and amazon pharmacy delivers meds right to you fast without any\n",
            "[32:25.440 --> 32:32.080]  horrible hold music thanks to amazon health care just got less painful this message comes from mint\n",
            "[32:32.080 --> 32:37.200]  mobile mint mobile took what's wrong with wireless and made it right they offer premium wireless\n",
            "[32:37.200 --> 32:42.400]  plans for less and all plans include high speed data unlimited talk and text and nationwide coverage\n",
            "[32:42.720 --> 32:50.720]  see for yourself at mint mobile dot com slash switch\n",
            "[33:12.720 --> 33:20.320]  vibrant it's still youthful it's still hip remember age ain't nothing but a three digit number\n",
            "[33:21.600 --> 33:27.360]  to prove that our country still has it at least the very tiny part of our country that is our show\n",
            "[33:27.360 --> 33:31.920]  we're going over some of the best conversations we've had in the last year or so last December\n",
            "[33:31.920 --> 33:38.640]  Jim Gephigan joined us to talk about his new comedy special titled this skidney because well\n",
            "[33:39.040 --> 33:45.200]  that's what he is no i asked him about whether he enjoyed people congratulating him on his weight loss\n",
            "[33:45.200 --> 33:52.320]  well i feel there's there's a certain imposter syndrome because i you know i use an appetite\n",
            "[33:52.320 --> 33:59.520]  suppressant so it's not like i put any effort or change any behavior right you know i joke in\n",
            "[33:59.520 --> 34:05.440]  this special that you know i used to be a fact guy and now i'm just i'm thin therefore arrogant\n",
            "[34:05.440 --> 34:12.560]  because i always viewed thin people as arrogant but i do feel like i mean i love it my knees don't\n",
            "[34:12.560 --> 34:19.040]  hurt it's you know with the appetites press it i'm just kind of it's not like i don't eat i just eat\n",
            "[34:19.040 --> 34:27.200]  like a normal human i'm less consuming like a dog so the the special comes out of the end of what i\n",
            "[34:27.200 --> 34:33.600]  understand has been a pretty remarkable year for you for example earlier you went with about 200 other\n",
            "[34:33.680 --> 34:40.000]  comedians to the Vatican to meet the pope is that right yeah i mean that shows you the position\n",
            "[34:40.000 --> 34:49.760]  that the Catholic church is in right now they're like okay i'm calling the committee why i mean\n",
            "[34:49.760 --> 34:58.080]  why in the world did Pope Francis why did he want to have 200 comedians come to the Vatican\n",
            "[34:58.880 --> 35:07.680]  well there was a really uh intellectually sound reason which he believes that humor is a really\n",
            "[35:08.240 --> 35:15.760]  important part of dealing with uh everyday life and so he wanted to articulate that but the reality\n",
            "[35:15.760 --> 35:25.520]  of sitting in a room in the Vatican with you know Jimmy Fallon Chris rock you know Seth\n",
            "[35:25.600 --> 35:34.560]  you feel like it was just a gathering of every kid who couldn't behave in her right i don't know\n",
            "[35:34.560 --> 35:39.040]  i don't know if the none can do it for these guys we better go to the pope um you said and\n",
            "[35:39.040 --> 35:43.680]  your Instagram post about it that the pope told you Pope Francis told you Jim Gaffigan that you\n",
            "[35:43.680 --> 35:49.840]  were his favorite comedian what is that true that is not true at all that\n",
            "[35:50.800 --> 35:57.200]  that was me trying to be funny thank you one of your little jokes but i posted and i was\n",
            "[35:57.200 --> 36:01.680]  like you know what our people gonna think that i'm serious would have been funnier from a fat guy yeah\n",
            "[36:04.320 --> 36:10.000]  so another accomplishment that happened this year you got the chance to play Tim Walsh on Saturday\n",
            "[36:10.000 --> 36:14.320]  night live now when you saw the announcement that he was gonna be the vice president that's what\n",
            "[36:14.320 --> 36:18.320]  candidate did you just start hovering by the phone waiting for Lauren Michaels to call\n",
            "[36:20.720 --> 36:27.920]  maybe i've just been kicking around long enough where i had you know i've been burned so many times\n",
            "[36:27.920 --> 36:37.120]  that i didn't want to emotionally invest in it and so when you know the internet kind of after Steve\n",
            "[36:37.120 --> 36:47.200]  Martin turned it down they kind of identified every midwestern do we guys i was like i was yeah\n",
            "[36:47.280 --> 36:51.920]  i mean i definitely wanted to do it but uh i'm wouldn't with the irony would have been\n",
            "[36:51.920 --> 36:57.840]  oh Jim we wanted you to play uh Tim Walsh but you've lost too much weight right you're not doing\n",
            "[36:57.840 --> 37:03.920]  enough it's a shame well that good thing about being a midwestern uh do we guys is like you\n",
            "[37:03.920 --> 37:14.000]  can lose the weight but you still look out of shape well Jim Gaffigan is great to talk to you again\n",
            "[37:14.080 --> 37:19.600]  in this time we have invited you here to play a game we're calling your weight weight gift guy\n",
            "[37:19.600 --> 37:23.360]  now the holidays are right around the corner so we're gonna ask you three questions but gifts you can\n",
            "[37:23.360 --> 37:27.920]  buy for your loved ones answer two questions correctly and you want to present for one of our listeners\n",
            "[37:27.920 --> 37:31.520]  the voice of anyone from our show they might like bill who is Jim Gaffigan playing for\n",
            "[37:31.520 --> 37:38.720]  lives wilder of phoenix cerosona all right first question uh there are lots of high tech products you\n",
            "[37:38.800 --> 37:46.480]  can buy including a whole category just meant to improve your sleep including which of these a a smart pillow\n",
            "[37:46.480 --> 37:53.120]  which uses a i and motors to nudge you when you start snoring be a smart mattress that flings you\n",
            "[37:53.120 --> 37:58.880]  out of bed if you hit snooze one to many times or see a smart fitted sheet with a speaker that\n",
            "[37:58.880 --> 38:07.600]  tells you step by step how to fold it correctly i feel like it's gotta be the smart pillow it is it's\n",
            "[38:07.680 --> 38:15.280]  the smart pillow the delucci smart pillow can sense it says if you're snoring and then uses these\n",
            "[38:15.280 --> 38:20.720]  motors in the pillow to nudge your head which will either make you stop snoring because you've moved\n",
            "[38:20.720 --> 38:26.880]  or you'll just learn not to snore to avoid that punishment all right second question it wouldn't be\n",
            "[38:26.880 --> 38:35.760]  Christmas without the goop gift guide and this year in the sexy holiday section of the gift guide\n",
            "[38:35.760 --> 38:40.640]  winner th pau chose to just that what might be just the thing to spice up your love life\n",
            "[38:41.360 --> 38:49.040]  a pet parrot so they can repeat your pillow talk back to you be a replica of the 1995 batman\n",
            "[38:49.040 --> 38:56.160]  costume you know the one with the nipples or see a printed photograph of a classic 1951\n",
            "[38:56.240 --> 39:08.720]  Ferrari 212 sports car wow no no they're also hot it's hard to choose something i i think it's the third one\n",
            "[39:08.720 --> 39:17.040]  it's the photo it's the picture of the Ferrari you're right why did you why did you why did you\n",
            "[39:17.040 --> 39:24.960]  you think it was that one not that i understand goop logic but i think there's the nostalgia of\n",
            "[39:24.960 --> 39:31.680]  the beauty of the past that is timeless right yeah that would be my reason all right here's\n",
            "[39:31.680 --> 39:35.680]  the third question see if you can be perfect of course if you want to get for the person who has\n",
            "[39:35.680 --> 39:41.360]  everything you always turn to neem and marcus in this year in their holiday gift guide they are offering\n",
            "[39:41.360 --> 39:50.160]  a 48 thousand dollar moway shundal vending machine which lets you have 35 bottles of champagne\n",
            "[39:50.240 --> 39:55.600]  available to your friends and family at the touch of a button there's a catch though and what is\n",
            "[39:55.600 --> 40:03.200]  it a the 48 thousand dollar price does not include the champagne be the machine only holds\n",
            "[40:03.200 --> 40:08.480]  those single serving mini bottles of champagne or see it'll cost you an extra thousand dollars to\n",
            "[40:08.480 --> 40:15.920]  have it delivered oh i think it's i think it's the thousand dollars delivered it is it's the\n",
            "[40:15.920 --> 40:22.240]  first one it is both the first one in the last one they're all true so for 48 thousand dollars\n",
            "[40:23.520 --> 40:29.680]  you get basically an empty vending machine that says moway shand on in it you wish i kind of want\n",
            "[40:31.520 --> 40:35.760]  but there's nothing worse than when like the champagne gets jammed and then the next person comes\n",
            "[40:35.760 --> 40:42.080]  along gets two bottles of champagne you put the machine in no not frustrating you're trying to\n",
            "[40:42.080 --> 40:45.840]  get your champagne and you keep trying to get your hundred dollar bill now it gives you a jack thing\n",
            "[40:46.080 --> 40:51.680]  let's just the worst bill how did Jim gaffing and doing our quiz pretty it a row perfect excellent\n",
            "[40:52.400 --> 40:59.520]  Jim could got your lady thank you so much Jim gaffing is a comedian and actor who's latest\n",
            "[40:59.520 --> 41:04.480]  special the skinny is on who loon now it's fabulous check it out Jim gaff again thank you so much for\n",
            "[41:04.480 --> 41:07.120]  joining us again see you next time my hope take care\n",
            "[41:12.400 --> 41:19.360]  support for n p r in the following message come from good arrex good arrex can help you save money\n",
            "[41:19.360 --> 41:24.240]  and better manage your health this summer good arrex lets you compare prescription prices at over\n",
            "[41:24.240 --> 41:29.760]  seventy thousand pharmacies and instantly find free coupons you can find big savings at the pharmacy\n",
            "[41:29.760 --> 41:36.240]  for the whole family pets too good arrex is not insurance but may beat your copay if you do have\n",
            "[41:36.240 --> 41:43.360]  insurance save at the pharmacy this summer go to good arrex dot com slash wait this message comes from\n",
            "[41:43.360 --> 41:49.200]  mint mobile if you're tired of spending hundreds on big wireless bills bogus fees and free perks\n",
            "[41:49.200 --> 41:53.840]  mint mobile might be right for you with plan starting from fifteen bucks a month shop plans today\n",
            "[41:53.840 --> 41:58.880]  at mint mobile dot com slash switch up front payment of forty five dollars for three month five\n",
            "[41:58.880 --> 42:04.400]  gigabyte plan required new customer offer for first three months only then full price plan options available\n",
            "[42:04.480 --> 42:10.640]  taxes and fees extra see mint mobile for details this message comes from wise the app for doing\n",
            "[42:10.640 --> 42:16.720]  things and other currencies with wise you can send spend or receive money across borders all at a\n",
            "[42:16.720 --> 42:23.200]  fair exchange rate no markups or hidden fees join millions of customers and visit wise dot com\n",
            "[42:23.200 --> 42:30.320]  t's and c's apply finally in January we talked with musician and youtube superstar mxm tune\n",
            "[42:30.640 --> 42:36.560]  first on fame through her viral videos playing the ukulele suddenly being catapulted into internet\n",
            "[42:36.560 --> 42:42.160]  start-up she was able to take and stride you know but appearing on our show that was a little weird\n",
            "[42:42.720 --> 42:46.320]  this is the most of your experience ever had\n",
            "[42:48.320 --> 42:52.720]  if i understand correctly your stage name your your online a mix at mxm tune\n",
            "[42:53.360 --> 42:57.280]  began because you were a cartoonist as a very young person and you were like posting cartoons right\n",
            "[42:57.840 --> 43:01.680]  i was my dad is actually the person you created the handle so i'll have to hand it to him\n",
            "[43:01.680 --> 43:05.840]  all right but yet he created it when i was eleven years old and was sharing things on\n",
            "[43:05.840 --> 43:10.160]  like my cartoons on the internet and thought that would be my claim to fame right it was not yeah\n",
            "[43:10.960 --> 43:16.880]  but something was and i'm told it was the ukulele yeah so you were you were posting your videos\n",
            "[43:16.880 --> 43:22.880]  of yourself playing the ukulele how did how did you know they were getting popular let me tell you\n",
            "[43:23.200 --> 43:27.760]  there's a thing called a few count and a light count and i saw that number kind of slowly\n",
            "[43:27.760 --> 43:34.320]  creep up and then get exponentially bigger and then suddenly i didn't go to college and i was a full\n",
            "[43:34.320 --> 43:41.920]  time musician so wow is where i met yeah wow you know there's a man who gave himself a\n",
            "[43:41.920 --> 43:51.520]  vasectomy and filmed it and it got four million views how about you um i'm definitely not\n",
            "[43:51.520 --> 43:57.600]  beating out that vasectomy video so yeah say that that's where i'm at well if you can find a man who will\n",
            "[43:57.600 --> 44:06.560]  do that well you play the ukulele i'm curious you just did you like walk into your parents one\n",
            "[44:06.560 --> 44:12.720]  day and say guess what everybody i'm skipping college and i'm gonna go just be a musician on youtube\n",
            "[44:12.720 --> 44:17.600]  essentially there was two coming out so came out as bisexual in 2017 and then the far scarier one\n",
            "[44:17.680 --> 44:22.640]  was coming out as a musician who didn't want to pursue higher education which was\n",
            "[44:22.640 --> 44:27.280]  mortifying to both of my parents who are both educators but they've been nothing but supportive\n",
            "[44:27.280 --> 44:31.920]  really i was about like clear enemies of you i'm guessing the first one was easier\n",
            "[44:33.360 --> 44:38.960]  i want to talk a little bit about your music because you've progressed far beyond merely playing the ukulele\n",
            "[44:38.960 --> 44:44.560]  you are writing and performing these beautiful heartfelt songs did you have a particular inspiration\n",
            "[44:44.640 --> 44:48.560]  did you have a sound or a person you were trying to emulate or reach when you started\n",
            "[44:48.560 --> 44:53.680]  singing and writing and singing your own songs maybe kermit the frog i think is the only person that\n",
            "[44:53.680 --> 44:58.960]  comes to my music i'm really i'm the best i like what's the matter the matter you listen to\n",
            "[44:58.960 --> 45:05.760]  kermit and you you know what else is not easy being me it's not easy i want to sing about that\n",
            "[45:05.760 --> 45:12.320]  what i love about your music is it's it's timeless but it seems very much for and buy your\n",
            "[45:12.400 --> 45:17.360]  generation which i guess technically is jenzie am i right about that like you have one lyric and one\n",
            "[45:17.360 --> 45:22.960]  of your song when you love songs that i love where you talk about the singer talks about her relationship\n",
            "[45:22.960 --> 45:29.520]  with this other person we snap together like legos and i was like that is perfect\n",
            "[45:31.120 --> 45:36.800]  it is except that the plural of lego is just lego and i found that out way too late there's\n",
            "[45:36.800 --> 45:41.280]  not for the country it doesn't match lately the government section made that abundantly clear to me\n",
            "[45:41.520 --> 45:46.800]  oh oh that's another question but it's still doesn't matter it's the passion of what you're doing\n",
            "[45:46.800 --> 45:53.200]  and and i wouldn't get tripped up by that if i were you you know you should write a song about\n",
            "[45:53.200 --> 45:59.280]  reading the comments oh i should that's a good idea i'll do that after i collab with the guy who\n",
            "[45:59.280 --> 46:09.360]  did the self-insectomy so that's pretty good well my it is it is an omnisfund to talk to you\n",
            "[46:09.440 --> 46:16.960]  and we have invited you here to play a game we're calling in mexham tun meet tun m&m's by which we mean\n",
            "[46:17.760 --> 46:23.600]  those charming animated mascots that help sell m&m candies or and ask you three questions about\n",
            "[46:23.600 --> 46:26.800]  those cartoon candies if you get too right you win our prize one of our listeners the\n",
            "[46:26.800 --> 46:31.840]  voice of their choice in their instrument she'll ki who is my a playing for malery cali of peoria\n",
            "[46:31.840 --> 46:38.800]  elenoi all right you ready to do this i think so malery i'm gonna try so my very best for you all\n",
            "[46:38.960 --> 46:44.160]  so here's your first question for m&m's 75th anniversary they released a video showing\n",
            "[46:44.160 --> 46:52.640]  360 degree views inside the m&m mascots homes right one feature of the orange m&m's house\n",
            "[46:52.640 --> 47:00.240]  surprise some people what was it a six locks on the front door be a tanning bed or see a\n",
            "[47:00.240 --> 47:08.160]  Robert maple thorp print first of all i have not seen this advertisement i'm delighted to know that\n",
            "[47:08.160 --> 47:15.440]  the m&m's are homeowners congratulations to them yeah i i'm using tanning bed\n",
            "[47:15.440 --> 47:22.800]  they were tanning that i feel like the locks thing is a little too ominous yeah well it might be ominous\n",
            "[47:22.800 --> 47:28.720]  it is true apparently orange is little quirk is that he's paranoid about being eaten\n",
            "[47:28.800 --> 47:36.080]  he's not a can imagine why so his apartment has six locks and a monitor showing feeds from nine\n",
            "[47:36.080 --> 47:43.680]  security cameras okay so here's your next question you have two more you can do it before her\n",
            "[47:43.680 --> 47:50.400]  redesign in 2022 the green m&m was a female with big eyelashes and go go boots relatively sexy for a\n",
            "[47:50.400 --> 47:57.680]  candy why was she designed to be sexy was it a because research showed that people get hungrier when they\n",
            "[47:57.760 --> 48:05.440]  are feeling romantic because of the widely held belief that green m&m's were an afro-dezeac\n",
            "[48:05.440 --> 48:10.640]  or sea because of a planned but abandoned ad campaign featuring a passion that love a fair\n",
            "[48:10.640 --> 48:18.720]  between her and the jolly green giant wow okay you know i've listened to the show for years and\n",
            "[48:18.720 --> 48:22.960]  i've always thought maybe i'd be good at this and i think i'm just learning rapidly that this is not\n",
            "[48:23.040 --> 48:27.520]  my skill set and that's okay that's all right the audience is trying to help you by\n",
            "[48:27.520 --> 48:32.240]  they are helping me i've been able to hear them a lot throughout the season but i'm going to be\n",
            "[48:32.240 --> 48:36.480]  thankful when i answer i believe that it's the second one it is in fact\n",
            "[48:37.600 --> 48:43.440]  because apparently certain members of the audience i'm not saying they're old enough\n",
            "[48:43.440 --> 48:48.160]  personally but they might have heard that back in the seventies that was a widespread rumor that green\n",
            "[48:48.240 --> 48:54.160]  m&m's were an afro-dezeac it was the thing all right that's good you got one right with one to go\n",
            "[48:54.160 --> 49:00.560]  if you get this you win here's your last question m&m's almost had a live mascot they asked\n",
            "[49:00.560 --> 49:05.200]  Kevin Bacon the actor to do a commercial where he would dance to the song footlose from his\n",
            "[49:05.200 --> 49:11.200]  famous movie in a yellow m&m costume but he turned them down why did he turn them down was it a his\n",
            "[49:11.200 --> 49:16.400]  agent told him your Kevin freaking bacon you don't play the yellow m&m you play the blue m&m\n",
            "[49:17.280 --> 49:22.560]  be because he was doing ads for hormonal bacon and his deal banned him from representing any other food\n",
            "[49:22.560 --> 49:27.920]  or see because his wife he said gets too creeped out by the concept of talking food\n",
            "[49:29.360 --> 49:36.240]  you know marial problems present themselves in all sorts of colors and sometimes in the format\n",
            "[49:36.240 --> 49:40.960]  of people you know revealing their deepest arc of secrets like talking food being a real fear\n",
            "[49:41.040 --> 49:45.120]  so you're picking sea I think so and you're right yeah yeah yeah yeah yeah\n",
            "[49:47.360 --> 49:54.640]  his wife Kira said wrote doesn't like it when food talks and put her foot down about it\n",
            "[49:55.920 --> 50:03.440]  chiokey how did my a do in our quiz it makes him tune got two right which means she asked to come out\n",
            "[50:03.440 --> 50:07.680]  to her parents as a winner on weight weight don't tell well done\n",
            "[50:07.760 --> 50:15.840]  and next time tune my thank you so much for joining us thanks for listening thanks for playing\n",
            "[50:15.840 --> 50:20.080]  and we'll see you around take care bye bye\n",
            "[50:21.600 --> 50:27.760]  that's it for our happy 249th birthday america hopefully you'll still be around for the 250th\n",
            "[50:27.760 --> 50:32.960]  addition weight weight don't tell me is a production of npr and wbe's ishicago an association\n",
            "[50:32.960 --> 50:37.440]  with urgent haircut productions dot berman benevolent overlord the leptyca rets are\n",
            "[50:37.440 --> 50:42.000]  lemrics are public address announcers Paul Friedman our tour manager is shaina doll\n",
            "[50:42.000 --> 50:46.640]  BJ leader been composed our theme our program is produced by Jennifer Mills miles drawn boss\n",
            "[50:46.640 --> 50:52.320]  and lilyan king special thanks to monica hikki Peter Gwyn is our illegal fireworks\n",
            "[50:52.320 --> 50:57.360]  smuggler our vibe curator is amachoy technical direction to some more on a wider cFO is\n",
            "[50:57.360 --> 51:01.760]  colonel are production manager as Robert newhouse our senior producers the enchilogue and the\n",
            "[51:01.840 --> 51:06.480]  executive producer of weight weight don't tell me is Mike Danforth thanks to everybody you heard\n",
            "[51:06.480 --> 51:10.160]  all our panelists our guests and of course bill Curtis and thanks to all of you for listening\n",
            "[51:10.160 --> 51:14.800]  on Peter's Segal and we'll be back next week hopefully with all 10 of our fingers\n",
            "[51:23.600 --> 51:26.000]  this is npr\n",
            "[51:27.360 --> 51:31.680]  this message comes from homes.com you don't just live in your home you live\n",
            "[51:31.680 --> 51:36.160]  in your neighborhood as well so when you're shopping for a home you want to know as much about\n",
            "[51:36.160 --> 51:41.760]  the area around it as possible luckily homes.com has got you covered each listing features a\n",
            "[51:41.760 --> 51:46.240]  comprehensive neighborhood guide from local experts everything you'd ever want to know about a\n",
            "[51:46.240 --> 51:51.120]  neighborhood including the number of homes for sale local amenities and even things like median\n",
            "[51:51.120 --> 51:58.640]  lot size and a noise score homes.com we've done your homework this message comes from in PR sponsor\n",
            "[51:58.720 --> 52:03.440]  Capella University sometimes it takes a different approach to pursue your goals\n",
            "[52:03.440 --> 52:08.720]  Capella is an online university accredited by the higher learning commission that means you can\n",
            "[52:08.720 --> 52:13.440]  earn your degree from wherever you are and be confident your education is relevant\n",
            "[52:13.440 --> 52:19.040]  recognized and respected a different future is closer than you think with Capella University\n",
            "[52:19.040 --> 52:22.880]  learn more about earning a relevant degree at capella.edu\n",
            "[52:23.760 --> 52:28.960]  support for NPR comes from the Wallace Foundation and independent nonpartisan research foundation\n",
            "[52:28.960 --> 52:33.440]  collaborating with grantees and partners in the arts school leadership and youth development\n",
            "[52:33.440 --> 52:35.440]  more at Wallacefoundation.org\n",
            "✅ Transcription complete! Characters: 43748\n",
            "📝 Processed text: 1 segments, 43747 characters\n",
            "🌐 Detected language: en\n",
            "\n",
            "=== Analyzing with GPT ===\n",
            "✅ GPT analysis complete! Tokens: 2568\n",
            "Meeting title: Weight Weight Don't Tell Me The NPR News Quiz\n",
            "Participants: 3\n",
            "Meeting type: Interview\n",
            "Action items: 0\n",
            "⚠️ No action items detected\n",
            "\n",
            "=== Syncing to Notion ===\n",
            "✅ Notion entry created! ID: 2305fee1-8e37-81e5-a819-d3163af77b90\n",
            "\n",
            "✅ Process complete! Logs saved\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.28.1 python-docx notion-client langdetect pydub\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg  # 确保安装必要的依赖\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import openai\n",
        "import whisper\n",
        "from docx import Document\n",
        "from google.colab import files, userdata\n",
        "from notion_client import Client\n",
        "from langdetect import detect, LangDetectException\n",
        "import datetime\n",
        "import tempfile\n",
        "import torch\n",
        "from pydub import AudioSegment\n",
        "import subprocess\n",
        "\n",
        "try:\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    NOTION_TOKEN = userdata.get('NOTION_TOKEN')\n",
        "    NOTION_DB_ID = userdata.get('NOTION_DB_ID')\n",
        "\n",
        "    if not OPENAI_API_KEY:\n",
        "        raise ValueError(\"OPENAI_API_KEY not set\")\n",
        "    if not NOTION_TOKEN:\n",
        "        print(\"⚠️ Notion token missing - feature disabled\")\n",
        "    if not NOTION_DB_ID:\n",
        "        print(\"⚠️ Notion DB ID missing - feature disabled\")\n",
        "\n",
        "    openai.api_key = OPENAI_API_KEY\n",
        "    print(\"✅ OpenAI API key set\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Key retrieval failed: {str(e)}\")\n",
        "\n",
        "def get_audio_duration(audio_path):\n",
        "    \"\"\"使用ffmpeg获取精确音频时长（秒）\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"format=duration\",\n",
        "             \"-of\", \"default=noprint_wrappers=1:nokey=1\", audio_path],\n",
        "            capture_output=True, text=True\n",
        "        )\n",
        "        return float(result.stdout)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ 无法获取精确时长，使用估算值: {e}\")\n",
        "        # 估算：8KB/s 是常见音频比特率\n",
        "        return max(30, os.path.getsize(audio_path) // 8000)\n",
        "\n",
        "def transcribe_audio(audio_path, model_size=\"base\"):\n",
        "    \"\"\"使用Whisper转录音频文件\"\"\"\n",
        "    print(f\"🔊 Starting transcription with Whisper ({model_size} model)...\")\n",
        "\n",
        "    try:\n",
        "        # 检查GPU加速\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"💻 Using device: {device.upper()}\")\n",
        "\n",
        "        # 加载模型\n",
        "        model = whisper.load_model(model_size, device=device)\n",
        "        print(f\"✅ Loaded Whisper {model_size} model\")\n",
        "\n",
        "        # 转录音频\n",
        "        result = model.transcribe(\n",
        "            audio_path,\n",
        "            fp16=(device == \"cuda\"),\n",
        "            verbose=True,\n",
        "            task=\"transcribe\"\n",
        "        )\n",
        "\n",
        "        transcription = result[\"text\"]\n",
        "        print(f\"✅ Transcription complete! Characters: {len(transcription)}\")\n",
        "        return transcription\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Transcription failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def test_notion_connection():\n",
        "    \"\"\"测试Notion连接是否有效\"\"\"\n",
        "    try:\n",
        "        notion = Client(auth=NOTION_TOKEN)\n",
        "        notion.databases.retrieve(database_id=NOTION_DB_ID)\n",
        "        print(\"✅ Notion connection verified\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Notion connection failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def clean_transcript(text):\n",
        "    \"\"\"Cleans raw transcript text\"\"\"\n",
        "    text = re.sub(r'\\d{1,2}:\\d{2}:\\d{2}', '', text)\n",
        "    text = re.sub(r'Speaker\\s*\\d+:?', '', text)\n",
        "    return re.sub(r'\\n\\s*\\n', '\\n\\n', text).strip()\n",
        "\n",
        "def segment_text(text):\n",
        "    \"\"\"Segments text into paragraphs\"\"\"\n",
        "    return [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "\n",
        "def handle_transcript_input():\n",
        "    \"\"\"Handles transcript input methods\"\"\"\n",
        "    print(\"\\n=== Handling Transcript Input ===\")\n",
        "    print(\"Choose input method:\")\n",
        "    print(\"1 - Upload text file (.txt or .docx)\")\n",
        "    print(\"2 - Paste text directly\")\n",
        "    print(\"3 - Upload audio file (transcribe with Whisper)\")\n",
        "\n",
        "    input_method = input(\"Your choice (1/2/3): \")\n",
        "    transcript_text = \"\"\n",
        "\n",
        "    # 文本文件上传\n",
        "    if input_method == \"1\":\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            print(\"⚠️ No files uploaded, switching to paste\")\n",
        "            transcript_text = input(\"Paste meeting transcript: \")\n",
        "        else:\n",
        "            filename = list(uploaded.keys())[0]\n",
        "            print(f\"✅ Uploaded: {filename}\")\n",
        "\n",
        "            # 文本文件处理\n",
        "            if filename.endswith('.txt'):\n",
        "                transcript_text = uploaded[filename].decode('utf-8')\n",
        "\n",
        "            # DOCX处理\n",
        "            elif filename.endswith('.docx'):\n",
        "                with tempfile.NamedTemporaryFile(delete=False, suffix='.docx') as tmp:\n",
        "                    tmp.write(uploaded[filename])\n",
        "                    tmp_path = tmp.name\n",
        "\n",
        "                doc = Document(tmp_path)\n",
        "                transcript_text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "                os.unlink(tmp_path)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "    # 文本粘贴\n",
        "    elif input_method == \"2\":\n",
        "        transcript_text = input(\"Paste meeting transcript: \")\n",
        "\n",
        "    # 音频文件处理\n",
        "    elif input_method == \"3\":\n",
        "        uploaded_audio = files.upload()\n",
        "        if not uploaded_audio:\n",
        "            print(\"⚠️ No audio files uploaded, switching to text paste\")\n",
        "            transcript_text = input(\"Paste meeting transcript: \")\n",
        "        else:\n",
        "            filename = list(uploaded_audio.keys())[0]\n",
        "            print(f\"✅ Uploaded audio: {filename}\")\n",
        "\n",
        "            # 创建临时文件\n",
        "            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(filename)[1]) as tmp:\n",
        "                tmp.write(uploaded_audio[filename])\n",
        "                audio_path = tmp.name\n",
        "\n",
        "            print(\"\\n⚡ Select transcription speed:\")\n",
        "            print(\"1 - Fast (tiny model, fastest, lower accuracy)\")\n",
        "            print(\"2 - Balanced (base model, recommended)\")\n",
        "            print(\"3 - High Quality (small model, slower)\")\n",
        "\n",
        "            speed_choice = input(\"Your choice (1/2/3): \") or \"2\"\n",
        "            model_map = {\"1\": \"tiny\", \"2\": \"base\", \"3\": \"small\"}\n",
        "            model_size = model_map.get(speed_choice, \"base\")\n",
        "\n",
        "            # 获取音频时长\n",
        "            try:\n",
        "                duration = get_audio_duration(audio_path)\n",
        "                print(f\"⏱ Audio duration: {duration//60:.0f}m {duration%60:.0f}s\")\n",
        "\n",
        "                # 时间估算\n",
        "                time_estimates = {\"tiny\": 0.3, \"base\": 0.8, \"small\": 2.0}\n",
        "                est_sec = duration * time_estimates[model_size]\n",
        "                print(f\"⏳ Estimated processing time: ~{est_sec//60:.0f}m {est_sec%60:.0f}s\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Duration estimation failed: {e}\")\n",
        "\n",
        "            # 转录音频\n",
        "            transcript_text = transcribe_audio(audio_path, model_size)\n",
        "\n",
        "            # 清理临时文件\n",
        "            os.unlink(audio_path)\n",
        "\n",
        "    else:\n",
        "        print(\"⚠️ Invalid option, defaulting to text paste\")\n",
        "        transcript_text = input(\"Paste meeting transcript: \")\n",
        "\n",
        "    cleaned_text = clean_transcript(transcript_text)\n",
        "    segments = segment_text(cleaned_text)\n",
        "\n",
        "    print(f\"📝 Processed text: {len(segments)} segments, {len(cleaned_text)} characters\")\n",
        "    return cleaned_text, segments\n",
        "\n",
        "def analyze_with_gpt(text, language='en'):\n",
        "    \"\"\"Analyzes text with GPT API\"\"\"\n",
        "    print(\"\\n=== Analyzing with GPT ===\")\n",
        "\n",
        "    if not openai.api_key:\n",
        "        print(\"❌ OpenAI API key missing\")\n",
        "        return {\"error\": \"OpenAI API key not set\", \"fallback_used\": True}, 0\n",
        "\n",
        "    # Language mapping\n",
        "    lang_map = {'zh': 'Chinese', 'es': 'Spanish', 'fr': 'French', 'en': 'English'}\n",
        "    lang_name = lang_map.get(language[:2], 'English')\n",
        "\n",
        "    # System prompt setup\n",
        "    system_prompt = f\"\"\"\n",
        "    You are a professional meeting analyst. Extract key information:\n",
        "    - Respond in {lang_name}\n",
        "    - Use this JSON format:\n",
        "    {{\n",
        "        \"meeting_title\": \"Meeting Title\",\n",
        "        \"participants\": [\"Attendee1\", \"Attendee2\"],\n",
        "        \"summary\": \"Meeting summary\",\n",
        "        \"action_items\": [{{\"task\": \"Task\", \"assignee\": \"Owner\"}}],\n",
        "        \"key_points\": {{\n",
        "            \"concerns\": [],\n",
        "            \"decisions\": [],\n",
        "            \"deadlines\": [],\n",
        "            \"updates\": []\n",
        "        }},\n",
        "        \"meeting_type\": \"Meeting type\",\n",
        "        \"platform\": \"Platform\",\n",
        "        \"fallback_used\": false\n",
        "    }}\n",
        "\n",
        "    Extraction rules:\n",
        "    1. meeting_title: Extract from start/end or generate\n",
        "    2. participants: Extract all attendees\n",
        "    3. Focus on meeting start/end sections\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt = f\"Meeting transcript:\\n{text[:10000]}\"\n",
        "\n",
        "    try:\n",
        "        # GPT API call\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message['content']\n",
        "        result = json.loads(content)\n",
        "        tokens_used = response.usage['total_tokens']\n",
        "\n",
        "        print(f\"✅ GPT analysis complete! Tokens: {tokens_used}\")\n",
        "        print(f\"Meeting title: {result.get('meeting_title', 'N/A')}\")\n",
        "        print(f\"Participants: {len(result.get('participants', []))}\")\n",
        "        print(f\"Meeting type: {result.get('meeting_type', 'N/A')}\")\n",
        "        print(f\"Action items: {len(result.get('action_items', []))}\")\n",
        "\n",
        "        # Fallback for action items\n",
        "        if not result.get('action_items'):\n",
        "            result['fallback_used'] = True\n",
        "            print(\"⚠️ No action items detected\")\n",
        "\n",
        "        return result, tokens_used\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ GPT analysis failed: {str(e)}\")\n",
        "        return {\n",
        "            \"error\": str(e),\n",
        "            \"fallback_used\": True\n",
        "        }, 0\n",
        "\n",
        "def create_notion_entry(meeting_data):\n",
        "    \"\"\"Creates Notion database entry\"\"\"\n",
        "    if not NOTION_TOKEN or not NOTION_DB_ID:\n",
        "        print(\"⚠️ Notion config incomplete - skipping\")\n",
        "        return False\n",
        "\n",
        "    print(\"\\n=== Syncing to Notion ===\")\n",
        "\n",
        "    try:\n",
        "        notion = Client(auth=NOTION_TOKEN)\n",
        "\n",
        "        # Prepare properties\n",
        "        properties = {\n",
        "            \"Meeting Title\": {\"title\": [{\"text\": {\"content\": meeting_data.get(\"meeting_title\", \"Untitled\")}}]},\n",
        "            \"Participant\": {\"rich_text\": [{\"text\": {\"content\": \", \".join(meeting_data.get(\"participants\", [\"Unknown\"]))}}]},\n",
        "            \"Date & Duration\": {\"date\": {\"start\": meeting_data.get(\"date\", datetime.datetime.now().isoformat())}},\n",
        "            \"Meeting Type\": {\"rich_text\": [{\"text\": {\"content\": meeting_data.get(\"meeting_type\", \"Other\")}}]},\n",
        "            \"Platform\": {\"select\": {\"name\": meeting_data.get(\"platform\", \"Unknown\")}},\n",
        "            \"Summary\": {\"rich_text\": [{\"text\": {\"content\": meeting_data.get(\"summary\", \"\")}}]},\n",
        "            \"Key Points\": {\"rich_text\": [{\"text\": {\"content\": format_key_points(meeting_data)}}]},\n",
        "            \"Action Items\": {\"rich_text\": [{\"text\": {\"content\": format_action_items(meeting_data)}}]},\n",
        "        }\n",
        "\n",
        "        # Create entry\n",
        "        new_page = notion.pages.create(\n",
        "            parent={\"database_id\": NOTION_DB_ID},\n",
        "            properties=properties\n",
        "        )\n",
        "\n",
        "        print(f\"✅ Notion entry created! ID: {new_page['id']}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Notion sync failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def format_key_points(data):\n",
        "    \"\"\"Formats key points for Notion\"\"\"\n",
        "    points = []\n",
        "    key_points = data.get(\"key_points\", {})\n",
        "    for category, items in key_points.items():\n",
        "        if items and isinstance(items, list):\n",
        "            points.append(f\"{category.upper()}:\")\n",
        "            points.extend([f\"- {item}\" for item in items])\n",
        "    return \"\\n\".join(points)\n",
        "\n",
        "def format_action_items(data):\n",
        "    \"\"\"Formats action items for Notion\"\"\"\n",
        "    action_items = data.get(\"action_items\", [])\n",
        "    if not action_items or not isinstance(action_items, list):\n",
        "        return \"No action items\"\n",
        "\n",
        "    formatted = []\n",
        "    for item in action_items:\n",
        "        if isinstance(item, dict):\n",
        "            task = item.get('task', 'Unknown task')\n",
        "            assignee = item.get('assignee', 'Unassigned')\n",
        "            formatted.append(f\"- {task} (Owner: {assignee})\")\n",
        "        else:\n",
        "            formatted.append(f\"- {str(item)}\")\n",
        "    return \"\\n\".join(formatted)\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main workflow execution\"\"\"\n",
        "    if not openai.api_key:\n",
        "        print(\"❌ OpenAI API key missing\")\n",
        "        return\n",
        "\n",
        "    logs = {\"steps\": [], \"errors\": []}\n",
        "\n",
        "    # Test Notion connection\n",
        "    if NOTION_TOKEN and NOTION_DB_ID:\n",
        "        if not test_notion_connection():\n",
        "            print(\"⚠️ Notion connection failed\")\n",
        "\n",
        "    try:\n",
        "        # Process input\n",
        "        cleaned_text, segments = handle_transcript_input()\n",
        "        logs[\"steps\"].append({\n",
        "            \"step\": \"Text input\",\n",
        "            \"segment_count\": len(segments),\n",
        "            \"status\": \"success\"\n",
        "        })\n",
        "\n",
        "        # Detect language\n",
        "        try:\n",
        "            language = detect(cleaned_text[:500]) if cleaned_text else 'en'\n",
        "        except LangDetectException:\n",
        "            language = 'en'\n",
        "        print(f\"🌐 Detected language: {language}\")\n",
        "\n",
        "        # GPT analysis\n",
        "        gpt_results, tokens_used = analyze_with_gpt(cleaned_text, language)\n",
        "\n",
        "        if \"error\" in gpt_results:\n",
        "            logs[\"steps\"].append({\n",
        "                \"step\": \"GPT analysis\",\n",
        "                \"status\": \"failed\",\n",
        "                \"error\": gpt_results[\"error\"]\n",
        "            })\n",
        "            print(f\"❌ GPT failed: {gpt_results['error']}\")\n",
        "            return\n",
        "        else:\n",
        "            logs[\"steps\"].append({\n",
        "                \"step\": \"GPT analysis\",\n",
        "                \"tokens_used\": tokens_used,\n",
        "                \"meeting_title\": gpt_results.get(\"meeting_title\"),\n",
        "                \"participants_count\": len(gpt_results.get(\"participants\", [])),\n",
        "                \"meeting_type\": gpt_results.get(\"meeting_type\"),\n",
        "                \"action_items_count\": len(gpt_results.get(\"action_items\", [])),\n",
        "                \"status\": \"success\"\n",
        "            })\n",
        "\n",
        "        # Add date and sync to Notion\n",
        "        gpt_results[\"date\"] = datetime.datetime.now().isoformat()\n",
        "        notion_success = create_notion_entry(gpt_results)\n",
        "        logs[\"steps\"].append({\n",
        "            \"step\": \"Notion sync\",\n",
        "            \"status\": \"success\" if notion_success else \"failed\"\n",
        "        })\n",
        "\n",
        "        # Save logs\n",
        "        with open(\"meeting_logs.json\", \"w\") as f:\n",
        "            json.dump(logs, f, indent=2)\n",
        "\n",
        "        print(\"\\n✅ Process complete! Logs saved\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logs[\"errors\"].append(str(e))\n",
        "        print(f\"\\n❌ Process error: {str(e)}\")\n",
        "        with open(\"error_log.json\", \"w\") as f:\n",
        "            json.dump(logs, f, indent=2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RsNbU1oC8N9I",
        "outputId": "8ae12fb7-7362-47c8-8d49-85395e777345"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: langchain 0.2.0\n",
            "Uninstalling langchain-0.2.0:\n",
            "  Successfully uninstalled langchain-0.2.0\n",
            "Found existing installation: langchain-core 0.2.43\n",
            "Uninstalling langchain-core-0.2.43:\n",
            "  Successfully uninstalled langchain-core-0.2.43\n",
            "\u001b[33mWARNING: Skipping langchain-community as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping langchain-openai as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping openai as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting langchain==0.2.0\n",
            "  Using cached langchain-0.2.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.0) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.0) (2.0.41)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.0) (3.11.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.0) (0.6.7)\n",
            "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain==0.2.0)\n",
            "  Using cached langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.0) (0.2.4)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.0) (0.1.147)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.0) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.0) (2.11.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.0) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.0) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.2.0) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.2.0) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain==0.2.0) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain==0.2.0) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain==0.2.0) (4.14.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.2.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.2.0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.2.0) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.2.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.2.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.2.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.2.0) (2025.7.9)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.0) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain==0.2.0) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.2.0) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (1.3.1)\n",
            "Using cached langchain-0.2.0-py3-none-any.whl (973 kB)\n",
            "Using cached langchain_core-0.2.43-py3-none-any.whl (397 kB)\n",
            "Installing collected packages: langchain-core, langchain\n",
            "Successfully installed langchain-0.2.0 langchain-core-0.2.43\n",
            "Collecting langchain-core==0.2.1\n",
            "  Using cached langchain_core-0.2.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.2.1) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.2.1) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.2.1) (0.1.147)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.2.1) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.2.1) (2.11.7)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.2.1) (8.5.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.2.1) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core==0.2.1) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core==0.2.1) (3.10.18)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core==0.2.1) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core==0.2.1) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core==0.2.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core==0.2.1) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core==0.2.1) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core==0.2.1) (0.4.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core==0.2.1) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core==0.2.1) (2025.7.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core==0.2.1) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core==0.2.1) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core==0.2.1) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core==0.2.1) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core==0.2.1) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core==0.2.1) (1.3.1)\n",
            "Using cached langchain_core-0.2.1-py3-none-any.whl (308 kB)\n",
            "Installing collected packages: langchain-core\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.2.43\n",
            "    Uninstalling langchain-core-0.2.43:\n",
            "      Successfully uninstalled langchain-core-0.2.43\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-text-splitters 0.2.4 requires langchain-core<0.3.0,>=0.2.38, but you have langchain-core 0.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-core-0.2.1\n",
            "Collecting langchain-community==0.2.0\n",
            "  Using cached langchain_community-0.2.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (2.0.41)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (3.11.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (0.2.0)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (0.2.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (0.1.147)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.0) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.0) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community==0.2.0) (0.2.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community==0.2.0) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.0) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.0) (23.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.2.0) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.2.0) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.2.0) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.2.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.2.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.2.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.2.0) (2025.7.9)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.2.0) (3.2.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.2.0) (4.14.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.0) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.0) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.0) (3.0.0)\n",
            "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain-community==0.2.0)\n",
            "  Using cached langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community==0.2.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community==0.2.0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community==0.2.0) (0.4.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.0) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.0) (1.3.1)\n",
            "Using cached langchain_community-0.2.0-py3-none-any.whl (2.1 MB)\n",
            "Using cached langchain_core-0.2.43-py3-none-any.whl (397 kB)\n",
            "Installing collected packages: langchain-core, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.2.1\n",
            "    Uninstalling langchain-core-0.2.1:\n",
            "      Successfully uninstalled langchain-core-0.2.1\n",
            "Successfully installed langchain-community-0.2.0 langchain-core-0.2.43\n",
            "Collecting langchain-openai==0.1.7\n",
            "  Downloading langchain_openai-0.1.7-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.1.46 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.1.7) (0.2.43)\n",
            "Collecting openai<2.0.0,>=1.24.0 (from langchain-openai==0.1.7)\n",
            "  Using cached openai-1.96.1-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.1.7) (0.9.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (0.1.147)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (2.11.7)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (4.14.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.1.7) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.1.7) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (2025.7.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.1.7) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.1.7) (2.4.0)\n",
            "Downloading langchain_openai-0.1.7-py3-none-any.whl (34 kB)\n",
            "Using cached openai-1.96.1-py3-none-any.whl (757 kB)\n",
            "Installing collected packages: openai, langchain-openai\n",
            "Successfully installed langchain-openai-0.1.7 openai-1.96.1\n",
            "Collecting openai==1.30.1\n",
            "  Using cached openai-1.30.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.30.1) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.30.1) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.30.1) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.30.1) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai==1.30.1) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai==1.30.1) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.11/dist-packages (from openai==1.30.1) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai==1.30.1) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai==1.30.1) (2025.7.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai==1.30.1) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.30.1) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai==1.30.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai==1.30.1) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai==1.30.1) (0.4.1)\n",
            "Using cached openai-1.30.1-py3-none-any.whl (320 kB)\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.96.1\n",
            "    Uninstalling openai-1.96.1:\n",
            "      Successfully uninstalled openai-1.96.1\n",
            "Successfully installed openai-1.30.1\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: notion-client in /usr/local/lib/python3.11/dist-packages (2.4.0)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from notion-client) (0.28.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->notion-client) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->notion-client) (2025.7.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->notion-client) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->notion-client) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->notion-client) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.0->notion-client) (1.3.1)\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-stogccvw\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-stogccvw\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (1.26.4)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20250625) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2025.7.9)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.2)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "36 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n",
            "✅ All credentials set\n",
            "🔗 Connected to Notion as: AI Meeting Assistant Access\n",
            "\n",
            "=== Input Method ===\n",
            "1: Upload audio file (.mp3/.wav/.m4a/.opus)\n",
            "2: Upload text file (.txt/.docx)\n",
            "3: Paste text directly\n",
            "Select input method (1/2/3): 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2665366e-61d1-4922-a1f5-25e1fbe07f74\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2665366e-61d1-4922-a1f5-25e1fbe07f74\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test.mp3 to test.mp3\n",
            "\n",
            "⚡ Transcription Model:\n",
            "1: Fast (tiny, low accuracy)\n",
            "2: Balanced (base, recommended)\n",
            "3: High Quality (small, slow)\n",
            "Select model (1/2/3): 1\n",
            "Detected language: English\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 315616/315616 [04:22<00:00, 1200.10frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "❌ PROCESS FAILED! Error logs saved to error_logs.json\n",
            "Error details: Analysis failed: 1 validation error for ChatOpenAI\n",
            "__root__\n",
            "  Client.__init__() got an unexpected keyword argument 'proxies' (type=type_error)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 更新安装命令部分\n",
        "!pip uninstall -y langchain langchain-core langchain-community langchain-openai openai\n",
        "\n",
        "# 安装兼容版本的核心包\n",
        "!pip install langchain==0.2.0\n",
        "!pip install langchain-core==0.2.1\n",
        "!pip install langchain-community==0.2.0\n",
        "!pip install langchain-openai==0.1.7  # 兼容版本\n",
        "!pip install openai==0.28.1  # 保持此版本\n",
        "\n",
        "# 其他依赖保持不变\n",
        "!pip install tqdm python-docx notion-client langdetect pydub\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg -y\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import openai\n",
        "import whisper\n",
        "from docx import Document\n",
        "from google.colab import files, userdata\n",
        "from notion_client import Client\n",
        "from langdetect import detect, LangDetectException\n",
        "import datetime\n",
        "import tempfile\n",
        "import torch\n",
        "from pydub import AudioSegment\n",
        "import subprocess\n",
        "from tqdm import tqdm\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain\n",
        "from langchain_community.llms import OpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "# ======================\n",
        "# 初始化设置（增强错误处理）\n",
        "# ======================\n",
        "try:\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    NOTION_TOKEN = userdata.get('NOTION_TOKEN')\n",
        "    NOTION_DB_ID = userdata.get('NOTION_DB_ID')\n",
        "    NOTION_PAGE_ID = userdata.get('NOTION_PAGE_ID')\n",
        "\n",
        "    # 更严格的凭证检查\n",
        "    missing_creds = []\n",
        "    if not OPENAI_API_KEY:\n",
        "        missing_creds.append(\"OPENAI_API_KEY\")\n",
        "    if not NOTION_TOKEN:\n",
        "        missing_creds.append(\"NOTION_TOKEN\")\n",
        "    if not NOTION_DB_ID:\n",
        "        missing_creds.append(\"NOTION_DB_ID\")\n",
        "    if not NOTION_PAGE_ID:\n",
        "        missing_creds.append(\"NOTION_PAGE_ID\")\n",
        "\n",
        "    if missing_creds:\n",
        "        raise ValueError(f\"Missing credentials: {', '.join(missing_creds)}\")\n",
        "\n",
        "    openai.api_key = OPENAI_API_KEY\n",
        "    print(\"✅ All credentials set\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Key retrieval failed: {str(e)}\")\n",
        "    print(\"\\n🔧 Setup Instructions:\")\n",
        "    print(\"1. Click the key icon on the left sidebar (Colab secrets)\")\n",
        "    print(\"2. Add the following secrets:\")\n",
        "    print(\"   - OPENAI_API_KEY: Your OpenAI API key\")\n",
        "    print(\"   - NOTION_TOKEN: Your Notion integration token\")\n",
        "    print(\"   - NOTION_DB_ID: ID of your Notion database\")\n",
        "    print(\"   - NOTION_PAGE_ID: ID of the parent page for reports\")\n",
        "    print(\"3. Rerun this cell after adding secrets\")\n",
        "    raise\n",
        "\n",
        "# ======================\n",
        "# 日志系统增强\n",
        "# ======================\n",
        "class MeetingLogger:\n",
        "    def __init__(self):\n",
        "        self.logs = {\n",
        "            \"start_time\": datetime.datetime.now().isoformat(),\n",
        "            \"steps\": [],\n",
        "            \"errors\": [],\n",
        "            \"metrics\": {}\n",
        "        }\n",
        "\n",
        "    def log_step(self, step_name, status, details=None, error=None):\n",
        "        \"\"\"记录处理步骤\"\"\"\n",
        "        entry = {\n",
        "            \"step\": step_name,\n",
        "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "            \"status\": status\n",
        "        }\n",
        "        if details:\n",
        "            entry[\"details\"] = details\n",
        "        if error:\n",
        "            entry[\"error\"] = str(error)\n",
        "        self.logs[\"steps\"].append(entry)\n",
        "\n",
        "    def log_metric(self, name, value):\n",
        "        \"\"\"记录性能指标\"\"\"\n",
        "        self.logs[\"metrics\"][name] = value\n",
        "\n",
        "    def save_logs(self, filename=\"meeting_logs.json\"):\n",
        "        \"\"\"保存日志到文件\"\"\"\n",
        "        with open(filename, \"w\") as f:\n",
        "            json.dump(self.logs, f, indent=2)\n",
        "        return filename\n",
        "\n",
        "    def get_console_log(self):\n",
        "        \"\"\"生成控制台友好的日志摘要\"\"\"\n",
        "        log_str = f\"=== Meeting Processing Log ===\\n\"\n",
        "        log_str += f\"Start Time: {self.logs['start_time']}\\n\"\n",
        "\n",
        "        for step in self.logs[\"steps\"]:\n",
        "            status_icon = \"✅\" if step[\"status\"] == \"success\" else \"❌\"\n",
        "            log_str += f\"{status_icon} [{step['timestamp']}] {step['step']}\"\n",
        "            if \"details\" in step:\n",
        "                log_str += f\" - {step['details']}\"\n",
        "            if step[\"status\"] == \"failed\":\n",
        "                log_str += f\" - ERROR: {step.get('error', 'Unknown')}\"\n",
        "            log_str += \"\\n\"\n",
        "\n",
        "        if self.logs[\"metrics\"]:\n",
        "            log_str += \"\\n=== Metrics ===\\n\"\n",
        "            for metric, value in self.logs[\"metrics\"].items():\n",
        "                log_str += f\"- {metric}: {value}\\n\"\n",
        "\n",
        "        return log_str\n",
        "\n",
        "# 初始化全局日志器\n",
        "logger = MeetingLogger()\n",
        "\n",
        "# ======================\n",
        "# 音频处理增强\n",
        "# ======================\n",
        "def get_audio_duration(audio_path):\n",
        "    \"\"\"获取音频时长并记录指标\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"format=duration\",\n",
        "             \"-of\", \"default=noprint_wrappers=1:nokey=1\", audio_path],\n",
        "            capture_output=True, text=True\n",
        "        )\n",
        "        duration = float(result.stdout)\n",
        "        logger.log_metric(\"audio_duration_sec\", duration)\n",
        "        return duration\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"Audio Duration\", \"warning\", error=e)\n",
        "        return max(30, os.path.getsize(audio_path) // 8000)\n",
        "\n",
        "def transcribe_audio(audio_path, model_size=\"base\"):\n",
        "    \"\"\"使用Whisper转录音频并返回语言信息\"\"\"\n",
        "    logger.log_step(\"Whisper Transcription\", \"started\",\n",
        "                   {\"model_size\": model_size, \"audio_path\": audio_path})\n",
        "\n",
        "    try:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        logger.log_step(\"Hardware Check\", \"success\", {\"device\": device})\n",
        "\n",
        "        model = whisper.load_model(model_size, device=device)\n",
        "        logger.log_step(\"Model Loading\", \"success\")\n",
        "\n",
        "        result = model.transcribe(\n",
        "            audio_path,\n",
        "            fp16=(device == \"cuda\"),\n",
        "            verbose=False,\n",
        "            task=\"transcribe\"\n",
        "        )\n",
        "\n",
        "        transcription = result[\"text\"]\n",
        "        detected_lang = result[\"language\"]\n",
        "        logger.log_step(\"Whisper Transcription\", \"success\", {\n",
        "            \"characters\": len(transcription),\n",
        "            \"detected_language\": detected_lang\n",
        "        })\n",
        "\n",
        "        return transcription, detected_lang\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"Whisper Transcription\", \"failed\", error=e)\n",
        "        raise\n",
        "\n",
        "# ======================\n",
        "# LangChain模块化 (更新为使用Pydantic模型)\n",
        "# ======================\n",
        "class MeetingAnalysis(BaseModel):\n",
        "    meeting_title: str = Field(description=\"Meeting title extracted from context\")\n",
        "    participants: list[str] = Field(description=\"List of participant names\")\n",
        "    summary: str = Field(description=\"Comprehensive meeting summary in 3-5 paragraphs\")\n",
        "    key_points: dict = Field(description=\"Key points organized by category: concerns, decisions, updates, risks\")\n",
        "    action_items: list[dict] = Field(description=\"List of action items with task, assignee and due date\")\n",
        "    meeting_type: str = Field(description=\"Type of meeting e.g., project update, client call, team sync\")\n",
        "    platform: str = Field(description=\"Platform where meeting occurred e.g., Zoom, Teams, WhatsApp\")\n",
        "\n",
        "def setup_langchain_chains(language='en'):\n",
        "    \"\"\"创建LangChain处理链\"\"\"\n",
        "    # 多语言提示模板\n",
        "    lang_map = {\n",
        "        'en': \"Analyze this meeting transcript in English\",\n",
        "        'zh': \"用中文分析此会议记录\",\n",
        "        'es': \"Analiza esta transcripción de reunión en español\",\n",
        "        'fr': \"Analysez cette transcription de réunion en français\"\n",
        "    }\n",
        "    lang_instruction = lang_map.get(language[:2], lang_map['en'])\n",
        "\n",
        "    # 设置输出解析器\n",
        "    parser = JsonOutputParser(pydantic_object=MeetingAnalysis)\n",
        "\n",
        "    # 创建提示模板\n",
        "    prompt_template = PromptTemplate(\n",
        "        template=\"\"\"\n",
        "        {lang_instruction}:\n",
        "\n",
        "        {format_instructions}\n",
        "\n",
        "        ### Meeting Transcript:\n",
        "        {transcript}\n",
        "\n",
        "        ### Analysis Guidelines:\n",
        "        1. Identify all participants mentioned\n",
        "        2. Extract meeting title from context or create descriptive one\n",
        "        3. Group key points by category:\n",
        "           - Concerns: Any risks or worries expressed\n",
        "           - Decisions: Formal agreements made\n",
        "           - Updates: Project or task progress reports\n",
        "           - Risks: Potential future problems identified\n",
        "        4. Action items must include: task description, assignee, and due date (if mentioned)\n",
        "        5. Determine meeting type and platform from context clues\n",
        "        \"\"\",\n",
        "        input_variables=[\"transcript\"],\n",
        "        partial_variables={\n",
        "            \"lang_instruction\": lang_instruction,\n",
        "            \"format_instructions\": parser.get_format_instructions()\n",
        "        }\n",
        "    )\n",
        "\n",
        "      # 创建处理链 - 使用兼容的初始化方式\n",
        "    llm = ChatOpenAI(\n",
        "        openai_api_key=OPENAI_API_KEY,  # 直接传递api_key\n",
        "        temperature=0.3,\n",
        "        model=\"gpt-3.5-turbo\"    # 使用model参数\n",
        "    )\n",
        "\n",
        "    analysis_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=\"analysis\")\n",
        "\n",
        "    return analysis_chain, parser\n",
        "\n",
        "# ======================\n",
        "# Notion报告增强\n",
        "# ======================\n",
        "def create_notion_report_page(meeting_data, transcript, logs):\n",
        "    \"\"\"创建详细的Notion报告页面\"\"\"\n",
        "    logger.log_step(\"Notion Report\", \"started\")\n",
        "\n",
        "    try:\n",
        "        notion = Client(auth=NOTION_TOKEN)\n",
        "\n",
        "        # 验证父页面是否存在\n",
        "        try:\n",
        "            parent_page = notion.pages.retrieve(NOTION_PAGE_ID)\n",
        "            logger.log_step(\"Parent Page Check\", \"success\",\n",
        "                          {\"title\": parent_page['properties']['title']['title'][0]['plain_text']})\n",
        "        except Exception as e:\n",
        "            logger.log_step(\"Parent Page Check\", \"failed\", error=e)\n",
        "            raise ValueError(f\"Parent page {NOTION_PAGE_ID} not found or inaccessible\")\n",
        "\n",
        "        # 创建子页面\n",
        "        new_page = notion.pages.create(\n",
        "            parent={\"page_id\": NOTION_PAGE_ID},\n",
        "            properties={\n",
        "                \"title\": {\n",
        "                    \"title\": [\n",
        "                        {\n",
        "                            \"text\": {\n",
        "                                \"content\": meeting_data.get(\"meeting_title\", \"Meeting Report\")[:200]\n",
        "                            }\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "        page_id = new_page[\"id\"]\n",
        "        logger.log_step(\"Page Created\", \"success\", {\"page_id\": page_id})\n",
        "\n",
        "        # 添加报告内容块\n",
        "        children_blocks = []\n",
        "\n",
        "        # 1. 元数据部分\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"Meeting Details\"}}]}\n",
        "        })\n",
        "\n",
        "        details_text = f\"\"\"\n",
        "        **Date**: {meeting_data.get('date', 'N/A')}\n",
        "        **Participants**: {', '.join(meeting_data.get('participants', []))}\n",
        "        **Language**: {meeting_data.get('language', 'Unknown')}\n",
        "        **Platform**: {meeting_data.get('platform', 'Unknown')}\n",
        "        **Meeting Type**: {meeting_data.get('meeting_type', 'N/A')}\n",
        "        \"\"\"\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"paragraph\",\n",
        "            \"paragraph\": {\"rich_text\": [{\"text\": {\"content\": details_text.strip()}}]}\n",
        "        })\n",
        "\n",
        "        # 2. 摘要部分\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"Summary\"}}]}\n",
        "        })\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"paragraph\",\n",
        "            \"paragraph\": {\"rich_text\": [{\"text\": {\"content\": meeting_data.get('summary', '')}}]}\n",
        "        })\n",
        "\n",
        "        # 3. 关键点部分\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"Key Points\"}}]}\n",
        "        })\n",
        "\n",
        "        key_points = meeting_data.get('key_points', {})\n",
        "        for category, items in key_points.items():\n",
        "            children_blocks.append({\n",
        "                \"object\": \"block\",\n",
        "                \"type\": \"heading_3\",\n",
        "                \"heading_3\": {\"rich_text\": [{\"text\": {\"content\": category.capitalize()}}]}\n",
        "            })\n",
        "\n",
        "            if items:\n",
        "                for item in items:\n",
        "                    children_blocks.append({\n",
        "                        \"object\": \"block\",\n",
        "                        \"type\": \"bulleted_list_item\",\n",
        "                        \"bulleted_list_item\": {\"rich_text\": [{\"text\": {\"content\": item}}]}\n",
        "                    })\n",
        "\n",
        "        # 4. 行动项表格\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"Action Items\"}}]}\n",
        "        })\n",
        "\n",
        "        # 创建表格\n",
        "        table_rows = []\n",
        "        for idx, item in enumerate(meeting_data.get('action_items', [])):\n",
        "            task = item.get('task', '')\n",
        "            assignee = item.get('assignee', 'Unassigned')\n",
        "            due_date = item.get('due_date', 'N/A')\n",
        "\n",
        "            table_rows.append([\n",
        "                [{\"text\": {\"content\": str(idx+1)}}],\n",
        "                [{\"text\": {\"content\": task}}],\n",
        "                [{\"text\": {\"content\": assignee}}],\n",
        "                [{\"text\": {\"content\": due_date}}]\n",
        "            ])\n",
        "\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"table\",\n",
        "            \"table\": {\n",
        "                \"table_width\": 4,\n",
        "                \"has_column_header\": True,\n",
        "                \"has_row_header\": False,\n",
        "                \"children\": [\n",
        "                    {\n",
        "                        \"object\": \"block\",\n",
        "                        \"type\": \"table_row\",\n",
        "                        \"table_row\": {\n",
        "                            \"cells\": [\n",
        "                                [{\"text\": {\"content\": \"#\"}}],\n",
        "                                [{\"text\": {\"content\": \"Task\"}}],\n",
        "                                [{\"text\": {\"content\": \"Assignee\"}}],\n",
        "                                [{\"text\": {\"content\": \"Due Date\"}}]\n",
        "                            ]\n",
        "                        }\n",
        "                    },\n",
        "                    *[{\n",
        "                        \"object\": \"block\",\n",
        "                        \"type\": \"table_row\",\n",
        "                        \"table_row\": {\"cells\": cells}\n",
        "                    } for cells in table_rows]\n",
        "                ]\n",
        "            }\n",
        "        })\n",
        "\n",
        "        # 5. 日志部分（可选）\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"heading_2\",\n",
        "            \"heading_2\": {\"rich_text\": [{\"text\": {\"content\": \"Processing Logs\"}}]}\n",
        "        })\n",
        "        children_blocks.append({\n",
        "            \"object\": \"block\",\n",
        "            \"type\": \"code\",\n",
        "            \"code\": {\"rich_text\": [{\"text\": {\"content\": logger.get_console_log()}}]}\n",
        "        })\n",
        "\n",
        "        # 添加所有块到页面\n",
        "        notion.blocks.children.append(\n",
        "            block_id=page_id,\n",
        "            children=children_blocks\n",
        "        )\n",
        "\n",
        "        # 返回页面URL\n",
        "        report_url = new_page.get(\"url\", \"\")\n",
        "        logger.log_step(\"Notion Report\", \"success\", {\"url\": report_url})\n",
        "\n",
        "        # 添加数据库属性\n",
        "        if NOTION_DB_ID:\n",
        "            notion.pages.update(\n",
        "                page_id=page_id,\n",
        "                properties={\n",
        "                    \"Database Relation\": {\n",
        "                        \"relation\": [{\"id\": NOTION_DB_ID}]\n",
        "                    }\n",
        "                }\n",
        "            )\n",
        "\n",
        "        return report_url\n",
        "\n",
        "    except Exception as e:\n",
        "        # 更详细的错误日志\n",
        "        error_details = f\"Notion API error: {str(e)}\"\n",
        "        if hasattr(e, 'response') and hasattr(e.response, 'content'):\n",
        "            error_details += f\"\\nResponse: {e.response.content.decode('utf-8')}\"\n",
        "\n",
        "        logger.log_step(\"Notion Report\", \"failed\", error=error_details)\n",
        "        print(f\"❌ Notion failed: {error_details}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 主流程函数修改\n",
        "# ======================\n",
        "def handle_transcript_input():\n",
        "    \"\"\"处理转录输入（增强版）\"\"\"\n",
        "    logger.log_step(\"Input Handling\", \"started\")\n",
        "\n",
        "    print(\"\\n=== Input Method ===\")\n",
        "    print(\"1: Upload audio file (.mp3/.wav/.m4a/.opus)\")\n",
        "    print(\"2: Upload text file (.txt/.docx)\")\n",
        "    print(\"3: Paste text directly\")\n",
        "\n",
        "    choice = input(\"Select input method (1/2/3): \").strip()\n",
        "\n",
        "    if choice == \"1\":\n",
        "        # 音频处理流程\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            logger.log_step(\"Audio Upload\", \"failed\", \"No files uploaded\")\n",
        "            print(\"No files uploaded, switching to text input\")\n",
        "            return handle_transcript_input()\n",
        "\n",
        "        filename = next(iter(uploaded.keys()))\n",
        "        logger.log_step(\"Audio Upload\", \"success\", {\"filename\": filename, \"size\": len(uploaded[filename])})\n",
        "\n",
        "        # 保存临时文件\n",
        "        ext = os.path.splitext(filename)[1].lower()\n",
        "        supported_audio = ['.mp3', '.wav', '.m4a', '.opus']\n",
        "        if ext not in supported_audio:\n",
        "            logger.log_step(\"Audio Processing\", \"failed\", f\"Unsupported format: {ext}\")\n",
        "            raise ValueError(f\"Unsupported audio format: {ext}\")\n",
        "\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=ext) as tmp:\n",
        "            tmp.write(uploaded[filename])\n",
        "            audio_path = tmp.name\n",
        "\n",
        "        # 模型选择\n",
        "        print(\"\\n⚡ Transcription Model:\")\n",
        "        print(\"1: Fast (tiny, low accuracy)\")\n",
        "        print(\"2: Balanced (base, recommended)\")\n",
        "        print(\"3: High Quality (small, slow)\")\n",
        "        model_choice = input(\"Select model (1/2/3): \").strip() or \"2\"\n",
        "        model_map = {\"1\": \"tiny\", \"2\": \"base\", \"3\": \"small\"}\n",
        "        model_size = model_map.get(model_choice, \"base\")\n",
        "\n",
        "        # 转录音频\n",
        "        duration = get_audio_duration(audio_path)\n",
        "        logger.log_metric(\"transcription_model\", model_size)\n",
        "        transcript, detected_lang = transcribe_audio(audio_path, model_size)\n",
        "        os.unlink(audio_path)\n",
        "\n",
        "        return transcript, detected_lang\n",
        "\n",
        "    elif choice == \"2\":\n",
        "        # 文本文件处理\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            logger.log_step(\"Text Upload\", \"failed\", \"No files uploaded\")\n",
        "            print(\"No files uploaded, switching to paste\")\n",
        "            return handle_transcript_input()\n",
        "\n",
        "        filename = next(iter(uploaded.keys()))\n",
        "        logger.log_step(\"Text Upload\", \"success\", {\"filename\": filename})\n",
        "\n",
        "        if filename.endswith('.txt'):\n",
        "            transcript = uploaded[filename].decode('utf-8')\n",
        "        elif filename.endswith('.docx'):\n",
        "            with tempfile.NamedTemporaryFile(delete=False, suffix='.docx') as tmp:\n",
        "                tmp.write(uploaded[filename])\n",
        "                doc = Document(tmp.name)\n",
        "                transcript = \"\\n\".join(p.text for p in doc.paragraphs)\n",
        "                os.unlink(tmp.name)\n",
        "        else:\n",
        "            logger.log_step(\"Text Processing\", \"failed\", f\"Unsupported format: {filename}\")\n",
        "            raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "        # 检测语言\n",
        "        try:\n",
        "            lang = detect(transcript[:500])\n",
        "            logger.log_step(\"Language Detection\", \"success\", {\"language\": lang})\n",
        "            return transcript, lang\n",
        "        except LangDetectException as e:\n",
        "            logger.log_step(\"Language Detection\", \"warning\", \"Using default English\")\n",
        "            return transcript, 'en'\n",
        "\n",
        "    elif choice == \"3\":\n",
        "        # 直接粘贴文本\n",
        "        transcript = input(\"Paste meeting transcript: \")\n",
        "        logger.log_step(\"Text Input\", \"success\", {\"length\": len(transcript)})\n",
        "\n",
        "        try:\n",
        "            lang = detect(transcript[:500])\n",
        "            logger.log_step(\"Language Detection\", \"success\", {\"language\": lang})\n",
        "            return transcript, lang\n",
        "        except LangDetectException as e:\n",
        "            logger.log_step(\"Language Detection\", \"warning\", \"Using default English\")\n",
        "            return transcript, 'en'\n",
        "\n",
        "    else:\n",
        "        logger.log_step(\"Input Handling\", \"failed\", \"Invalid choice\")\n",
        "        print(\"Invalid choice, defaulting to audio upload\")\n",
        "        return handle_transcript_input()\n",
        "\n",
        "def analyze_meeting(transcript, language='en'):\n",
        "    \"\"\"使用LangChain分析会议内容\"\"\"\n",
        "    logger.log_step(\"Meeting Analysis\", \"started\", {\"language\": language})\n",
        "\n",
        "    try:\n",
        "        # 获取链和解析器\n",
        "        analysis_chain, parser = setup_langchain_chains(language)\n",
        "        raw_output = analysis_chain.run(transcript[:15000])  # 限制长度\n",
        "\n",
        "        # 解析结构化输出\n",
        "        parsed = parser.parse(raw_output)\n",
        "        parsed[\"language\"] = language\n",
        "        parsed[\"date\"] = datetime.datetime.now().isoformat()\n",
        "\n",
        "        # 验证行动项\n",
        "        if not parsed.get(\"action_items\"):\n",
        "            logger.log_step(\"Action Items\", \"warning\", \"No action items detected\")\n",
        "            parsed[\"fallback_used\"] = True\n",
        "        else:\n",
        "            parsed[\"fallback_used\"] = False\n",
        "\n",
        "        logger.log_step(\"Meeting Analysis\", \"success\", {\n",
        "            \"title\": parsed[\"meeting_title\"],\n",
        "            \"participants\": len(parsed[\"participants\"]),\n",
        "            \"action_items\": len(parsed[\"action_items\"])\n",
        "        })\n",
        "\n",
        "        return parsed\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"Meeting Analysis\", \"failed\", error=e)\n",
        "        return {\n",
        "            \"error\": str(e),\n",
        "            \"fallback_used\": True\n",
        "        }\n",
        "\n",
        "def create_notion_database_entry(meeting_data, report_url=None):\n",
        "    \"\"\"创建Notion数据库条目\"\"\"\n",
        "    if not NOTION_TOKEN or not NOTION_DB_ID:\n",
        "        logger.log_step(\"Notion DB Entry\", \"skipped\", \"Missing credentials\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        notion = Client(auth=NOTION_TOKEN)\n",
        "\n",
        "        properties = {\n",
        "            \"Meeting Title\": {\"title\": [{\"text\": {\"content\": meeting_data.get(\"meeting_title\", \"Untitled\")}}]},\n",
        "            \"Participants\": {\"rich_text\": [{\"text\": {\"content\": \", \".join(meeting_data.get(\"participants\", []))}}]},\n",
        "            \"Date\": {\"date\": {\"start\": meeting_data.get(\"date\", datetime.datetime.now().isoformat())}},\n",
        "            \"Meeting Type\": {\"select\": {\"name\": meeting_data.get(\"meeting_type\", \"Unknown\")}},\n",
        "            \"Platform\": {\"select\": {\"name\": meeting_data.get(\"platform\", \"Unknown\")}},\n",
        "            \"Language\": {\"select\": {\"name\": meeting_data.get(\"language\", \"Unknown\")}},\n",
        "            \"Status\": {\"status\": {\"name\": \"Processed\"}},\n",
        "        }\n",
        "\n",
        "        if report_url:\n",
        "            properties[\"Report\"] = {\"url\": report_url}\n",
        "\n",
        "        new_entry = notion.pages.create(\n",
        "            parent={\"database_id\": NOTION_DB_ID},\n",
        "            properties=properties\n",
        "        )\n",
        "\n",
        "        entry_id = new_entry[\"id\"]\n",
        "        logger.log_step(\"Notion DB Entry\", \"success\", {\"entry_id\": entry_id})\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"Notion DB Entry\", \"failed\", error=e)\n",
        "        return False\n",
        "\n",
        "# ======================\n",
        "# 主函数重构\n",
        "# ======================\n",
        "def main():\n",
        "    \"\"\"主工作流\"\"\"\n",
        "    logger.log_step(\"Workflow\", \"started\")\n",
        "\n",
        "    try:\n",
        "        # 检查Notion连接性\n",
        "        try:\n",
        "            notion = Client(auth=NOTION_TOKEN)\n",
        "            user_info = notion.users.me()\n",
        "            logger.log_step(\"Notion Connection\", \"success\",\n",
        "                          {\"user\": user_info['name'], \"email\": user_info.get('person', {}).get('email')})\n",
        "            print(f\"🔗 Connected to Notion as: {user_info['name']}\")\n",
        "        except Exception as e:\n",
        "            logger.log_step(\"Notion Connection\", \"failed\", error=e)\n",
        "            print(f\"❌ Notion connection failed: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "        # 处理输入\n",
        "        transcript, language = handle_transcript_input()\n",
        "        logger.log_metric(\"transcript_length\", len(transcript))\n",
        "\n",
        "        # 分析会议内容\n",
        "        meeting_data = analyze_meeting(transcript, language)\n",
        "        if \"error\" in meeting_data:\n",
        "            raise RuntimeError(f\"Analysis failed: {meeting_data['error']}\")\n",
        "\n",
        "        # 创建Notion报告\n",
        "        report_url = create_notion_report_page(\n",
        "            meeting_data,\n",
        "            transcript,\n",
        "            logger.logs\n",
        "        )\n",
        "\n",
        "        if not report_url:\n",
        "            raise RuntimeError(\"Failed to create Notion report\")\n",
        "\n",
        "        # 创建数据库条目\n",
        "        db_success = create_notion_database_entry(meeting_data, report_url)\n",
        "\n",
        "        if not db_success:\n",
        "            print(\"⚠️ Failed to create Notion database entry, but report page was created\")\n",
        "\n",
        "        # 保存日志\n",
        "        log_file = logger.save_logs()\n",
        "        print(f\"\\n✅ PROCESS COMPLETE! Logs saved to {log_file}\")\n",
        "        print(f\"📄 Report Page: {report_url}\")\n",
        "\n",
        "        # 在Colab中提供直接链接\n",
        "        from IPython.display import HTML\n",
        "        display(HTML(f'<a href=\"{report_url}\" target=\"_blank\">Open Notion Report</a>'))\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_step(\"Workflow\", \"failed\", error=e)\n",
        "        log_file = logger.save_logs(\"error_logs.json\")\n",
        "        print(f\"\\n❌ PROCESS FAILED! Error logs saved to {log_file}\")\n",
        "        print(f\"Error details: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTunablvaxA62uPuiDuu8d",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}